{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13192b60-b5f1-443d-a672-f857e0541af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('/gpfs/helios/home/ploter/projects/MultiSensorDropout/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac28c36-f0e5-4227-a054-72da44cd974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    # Basic training parameters\n",
    "    seed=42,\n",
    "    batch_size=1,\n",
    "    epochs=18,\n",
    "    learning_rate=1e-3,\n",
    "    learning_rate_backbone=1e-4,\n",
    "    learning_rate_backbone_names=[\"backbone\"],\n",
    "    weight_decay=0.01,\n",
    "    scheduler_step_size=12,\n",
    "    eval_interval=1,\n",
    "    patience=5,\n",
    "    model='perceiver',\n",
    "    backbone='cnn',  # Set specific value for evaluation\n",
    "    eval=True,\n",
    "    weight_loss_center_point=5,\n",
    "    weight_loss_bce=1,\n",
    "    shuffle_views=False,\n",
    "    object_detection=True,  # Set to True for object detection\n",
    "    resize_frame = None,\n",
    "    \n",
    "    # Matcher parameters\n",
    "    set_cost_class=1,\n",
    "    set_cost_bbox=5,\n",
    "    set_cost_giou=2,\n",
    "    focal_alpha=0.25,\n",
    "    focal_gamma=2,\n",
    "    \n",
    "    # Loss coefficients\n",
    "    bbox_loss_coef=5,\n",
    "    giou_loss_coef=2,\n",
    "    eos_coef=0.1,\n",
    "    \n",
    "    # Checkpoint parameters\n",
    "    resume='checkpoint_epoch_17.pth',  # Replace with actual checkpoint path\n",
    "    output_dir=\"./output\",  # Provide a default output directory\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # Dataset parameters\n",
    "    dataset='moving-mnist',\n",
    "    dataset_path='Max-Ploter/detection-moving-mnist-easy',\n",
    "    generate_dataset_runtime=False,\n",
    "    num_workers=4,\n",
    "    num_frames=20,\n",
    "    train_dataset_fraction=1.0,\n",
    "    train_dataset_size=1.0,  # Additional parameter from notebook\n",
    "    test_dataset_fraction=1.0,\n",
    "    frame_dropout_pattern=None,\n",
    "    view_dropout_probs=[],#[0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85],\n",
    "    sampler_steps=[], #[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
    "    sequential_sampler=False,\n",
    "    grid_size=(1, 1),\n",
    "    tile_overlap=0.0,\n",
    "    \n",
    "    # Wandb parameters\n",
    "    wandb_project='multi-sensor-dropout',\n",
    "    wandb_id=None,\n",
    "    \n",
    "    # Perceiver model parameters\n",
    "    num_freq_bands=6,  # Set to 4 as in the notebook\n",
    "    max_freq=10, \n",
    "    enc_layers=1,\n",
    "    num_queries=16,  # Set to 16 as in the notebook\n",
    "    hidden_dim=128,\n",
    "    enc_nheads_cross=1,\n",
    "    nheads=1,\n",
    "    dropout=0.0,\n",
    "    self_per_cross_attn=1,\n",
    "    multi_classification_heads=False,\n",
    "    \n",
    "    # LSTM model parameters\n",
    "    lstm_hidden_size=128,\n",
    "    \n",
    "    # Additional parameters for complete compatibility\n",
    "    focal_loss=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2185130c-1eff-4867-8213-fefb17f6e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyCriterion:\n",
    "    def __init__(self, device):\n",
    "        # Initialize with the necessary attributes\n",
    "        self.device = device\n",
    "        self.weight_dict = {'loss_ce': 1}\n",
    "        \n",
    "    def __call__(self, outputs, targets, *args, **kwargs):\n",
    "        # Return a dictionary with zero losses to maintain the expected interface\n",
    "        return {\n",
    "            'loss_ce': torch.tensor(0.0, device=self.device),\n",
    "            'loss_bbox': torch.tensor(0.0, device=self.device),\n",
    "            'loss_giou': torch.tensor(0.0, device=self.device),\n",
    "            'loss': torch.tensor(0.0, device=self.device)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45f8ec69-480d-4b0c-b8ec-c263ea52cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class PostProcessTopK(nn.Module):\n",
    "    \"\"\" Wrapper that applies a post-processor and keeps only the top-k predictions by score and a score threshold \"\"\"\n",
    "\n",
    "    def __init__(self, post_processor, top_k=10, score_threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.post_processor = post_processor\n",
    "        self.top_k = top_k\n",
    "        self.score_threshold = score_threshold\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, target_sizes):\n",
    "        # Get results from the original post processor\n",
    "        results = self.post_processor(outputs, target_sizes)\n",
    "\n",
    "        # Filter to keep only top-k results and results above the score threshold\n",
    "        filtered_results = []\n",
    "        for result in results:\n",
    "            scores, labels, boxes = result['scores'], result['labels'], result['boxes']\n",
    "\n",
    "            # Get top-k indices by score\n",
    "            top_k = min(self.top_k, len(scores))\n",
    "            if top_k > 0:  # Check if there are any predictions\n",
    "                top_indices = torch.topk(scores, top_k).indices\n",
    "\n",
    "                # Filter by top-k indices\n",
    "                top_scores = scores[top_indices]\n",
    "                top_labels = labels[top_indices]\n",
    "                top_boxes = boxes[top_indices]\n",
    "\n",
    "                # Filter by score threshold\n",
    "                threshold_indices = top_scores >= self.score_threshold\n",
    "                final_scores = top_scores[threshold_indices]\n",
    "                final_labels = top_labels[threshold_indices]\n",
    "                final_boxes = top_boxes[threshold_indices]\n",
    "\n",
    "                filtered_results.append({'scores': final_scores, 'labels': final_labels, 'boxes': final_boxes})\n",
    "            else:\n",
    "                # Keep empty result if no predictions\n",
    "                filtered_results.append({'scores': torch.empty(0), 'labels': torch.empty(0, dtype=torch.int64), 'boxes': torch.empty(0, 4)})\n",
    "\n",
    "        return filtered_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a11936a-3054-47ad-9310-dda772cf7490",
   "metadata": {},
   "source": [
    "### Seq NMS post processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bbf31d4-b1d2-44a3-b1c9-846264341b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Try to import seq_nms, handle import error\n",
    "try:\n",
    "    from pt_seq_nms import seq_nms_from_list\n",
    "except ImportError:\n",
    "    print(\"Warning: 'pt_seq_nms' library not found. SeqNMSEvaluationPostprocessor will not function.\")\n",
    "    print(\"Install using: pip install git+https://github.com/MrParosk/seq_nms.git\")\n",
    "    seq_nms_from_list = None\n",
    "\n",
    "\n",
    "class SeqNMSEvaluationPostprocessor(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies standard post-processing (using the *original* PostProcess logic)\n",
    "    followed by Sequence Non-Maximum Suppression (Seq-NMS) to the outputs\n",
    "    of a video object detector (like RecurrentVideoObjectModule).\n",
    "\n",
    "    It first converts raw outputs frame-by-frame, filters low-confidence\n",
    "    detections, applies Seq-NMS, and formats the final results.\n",
    "\n",
    "    Assumes the original PostProcess class (as provided in the initial prompt)\n",
    "    is available and handles the conversion from raw model output (logits, rel boxes)\n",
    "    to scores, labels, and absolute boxes for foreground classes.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 postprocessor: nn.Module, # Expects an instance of the original PostProcess\n",
    "                 min_score_threshold: float = 0.01, # Threshold BEFORE SeqNMS\n",
    "                 linkage_threshold: float = 0.5,\n",
    "                 iou_threshold: float = 0.5):\n",
    "        \"\"\"\n",
    "        Initializes the SeqNMSEvaluationPostprocessor.\n",
    "\n",
    "        Args:\n",
    "            postprocessor (nn.Module): An instance of the *original* PostProcess\n",
    "                                       module definition.\n",
    "            min_score_threshold (float): Minimum confidence score required for a\n",
    "                                         detection to be considered *before* Seq-NMS.\n",
    "                                         Helps filter out noise from padding/background.\n",
    "                                         Defaults to 0.01.\n",
    "            linkage_threshold (float): The linkage threshold used by Seq-NMS. Defaults to 0.5.\n",
    "            iou_threshold (float): The IoU threshold used by Seq-NMS. Defaults to 0.5.\n",
    "\n",
    "        Raises:\n",
    "            ImportError: If the 'pt_seq_nms' library is not installed.\n",
    "            TypeError: If the provided 'postprocessor' is not an instance of nn.Module.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if seq_nms_from_list is None:\n",
    "            # Error message printed during import attempt\n",
    "            raise ImportError(\"'pt_seq_nms' library is required but could not be imported.\")\n",
    "        if not isinstance(postprocessor, nn.Module):\n",
    "             # Basic check, could be more specific if PostProcess class name is known\n",
    "             raise TypeError(f\"Argument 'postprocessor' must be an instance of nn.Module (expecting original PostProcess), but got {type(postprocessor)}\")\n",
    "\n",
    "        self.postprocess = postprocessor\n",
    "        self.min_score_threshold = min_score_threshold\n",
    "        self.linkage_threshold = linkage_threshold\n",
    "        self.iou_threshold = iou_threshold\n",
    "        print(f\"Initialized SeqNMSEvaluationPostprocessor with min_score_thresh={min_score_threshold}, \"\n",
    "              f\"linkage_thresh={linkage_threshold}, iou_thresh={iou_threshold}\")\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs: dict, target_sizes: torch.Tensor) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Processes the model output by applying PostProcess, pre-filtering, and then Seq-NMS.\n",
    "\n",
    "        Args:\n",
    "            outputs (dict): Output dictionary from the video object detector model.\n",
    "                            Expected keys and tensor shapes (assuming batch size B=1):\n",
    "                             - 'pred_logits': Tensor [T, NumQueries, NumClasses]\n",
    "                             - 'pred_boxes': Tensor [T, NumQueries, 4] (relative cxcywh)\n",
    "                             Where T is the number of frames.\n",
    "            target_sizes (torch.Tensor): Tensor of shape [T, 2] containing the original\n",
    "                                         (height, width) for each of the T frames.\n",
    "\n",
    "        Returns:\n",
    "            list[dict]: A list containing T dictionaries, one for each frame.\n",
    "                        Each dictionary represents the detections kept after Seq-NMS:\n",
    "                        - 'scores': Tensor [num_kept_detections], original scores of kept detections.\n",
    "                        - 'labels': Tensor [num_kept_detections], original labels of kept detections.\n",
    "                        - 'boxes': Tensor [num_kept_detections, 4] (absolute xyxy format).\n",
    "                        If no detections are kept for a frame, the tensors in the dict will be empty.\n",
    "        \"\"\"\n",
    "        pred_logits = outputs.get('pred_logits')\n",
    "        pred_boxes = outputs.get('pred_boxes')\n",
    "\n",
    "        # --- Input Validation ---\n",
    "        if pred_logits is None or pred_boxes is None:\n",
    "            raise KeyError(\"Input 'outputs' dictionary must contain 'pred_logits' and 'pred_boxes'.\")\n",
    "        if not isinstance(pred_logits, torch.Tensor) or not isinstance(pred_boxes, torch.Tensor):\n",
    "             raise TypeError(\"'pred_logits' and 'pred_boxes' must be torch tensors.\")\n",
    "        if pred_logits.dim() != 3 or pred_boxes.dim() != 3:\n",
    "             raise ValueError(f\"Expected 3D tensors for logits and boxes [T, NumQueries, Dim], got {pred_logits.shape} and {pred_boxes.shape}\")\n",
    "        if pred_logits.shape[0] != pred_boxes.shape[0] or pred_logits.shape[1] != pred_boxes.shape[1]:\n",
    "             raise ValueError(f\"Shape mismatch between 'pred_logits' {pred_logits.shape} and 'pred_boxes' {pred_boxes.shape}\")\n",
    "\n",
    "        num_frames = pred_logits.shape[0]\n",
    "        if not isinstance(target_sizes, torch.Tensor):\n",
    "             raise TypeError(\"'target_sizes' must be a torch tensor.\")\n",
    "        if target_sizes.shape != (num_frames, 2):\n",
    "            raise ValueError(f\"Expected 'target_sizes' shape [{num_frames}, 2], got {target_sizes.shape}\")\n",
    "\n",
    "        # --- 1. Apply Original PostProcess & Filter Frame-by-Frame ---\n",
    "        boxes_list_filtered = []\n",
    "        scores_list_filtered = []\n",
    "        labels_list_filtered = []\n",
    "        print(f\"Processing {num_frames} frames with original PostProcess and pre-filtering (min_score={self.min_score_threshold})...\")\n",
    "        num_total_before_seqnms = 0\n",
    "\n",
    "        for t in range(num_frames):\n",
    "            # Prepare input for PostProcess (needs batch dimension)\n",
    "            frame_output = {\n",
    "                'pred_logits': pred_logits[t:t+1], # Shape [1, NumQueries, NumClasses]\n",
    "                'pred_boxes': pred_boxes[t:t+1]    # Shape [1, NumQueries, 4]\n",
    "            }\n",
    "            frame_target_size = target_sizes[t:t+1] # Shape [1, 2]\n",
    "\n",
    "            # Run *original* PostProcess for the single frame\n",
    "            # It returns a list containing one dictionary: [{'scores': [N], 'labels': [N], 'boxes': [N, 4]}]\n",
    "            # N = NumQueries. Scores/Labels are from max over foreground classes.\n",
    "            processed_result_list = self.postprocess(frame_output, frame_target_size)\n",
    "\n",
    "            if not processed_result_list:\n",
    "                 # Should not happen if PostProcess works correctly, but handle defensively\n",
    "                 print(f\"Warning: PostProcess returned empty list for frame {t}. Appending empty tensors.\")\n",
    "                 boxes_list_filtered.append(torch.empty((0, 4), device=pred_logits.device, dtype=torch.float))\n",
    "                 scores_list_filtered.append(torch.empty(0, device=pred_logits.device, dtype=torch.float))\n",
    "                 labels_list_filtered.append(torch.empty(0, device=pred_logits.device, dtype=torch.long))\n",
    "                 continue\n",
    "\n",
    "            # Extract the results dict for the frame\n",
    "            result_t = processed_result_list[0]\n",
    "            scores_t = result_t['scores']\n",
    "            labels_t = result_t['labels']\n",
    "            boxes_t = result_t['boxes']\n",
    "\n",
    "            # --- Pre-filtering before SeqNMS ---\n",
    "            # Keep only detections with score above the minimum threshold\n",
    "            keep_mask_pre = scores_t >= self.min_score_threshold\n",
    "            num_kept_frame_pre = keep_mask_pre.sum().item()\n",
    "            num_total_before_seqnms += num_kept_frame_pre\n",
    "\n",
    "            # Append filtered tensors to the lists for SeqNMS input\n",
    "            boxes_list_filtered.append(boxes_t[keep_mask_pre])\n",
    "            scores_list_filtered.append(scores_t[keep_mask_pre])\n",
    "            labels_list_filtered.append(labels_t[keep_mask_pre])\n",
    "\n",
    "        print(f\"Finished PostProcess & pre-filtering. Kept {num_total_before_seqnms} detections across {num_frames} frames.\")\n",
    "\n",
    "        # --- 2. Check if any detections remain for Seq-NMS ---\n",
    "        if num_total_before_seqnms == 0:\n",
    "            print(\"No detections passed the minimum score threshold. Skipping Seq-NMS.\")\n",
    "            # Return a list of empty dictionaries for each frame\n",
    "            empty_results = []\n",
    "            device = pred_logits.device # Get device for empty tensors\n",
    "            for _ in range(num_frames):\n",
    "                 empty_results.append({\n",
    "                     'scores': torch.empty(0, device=device, dtype=torch.float),\n",
    "                     'labels': torch.empty(0, device=device, dtype=torch.long),\n",
    "                     'boxes': torch.empty((0, 4), device=device, dtype=torch.float)\n",
    "                 })\n",
    "            return empty_results\n",
    "\n",
    "        # --- 3. Apply Seq-NMS ---\n",
    "        print(f\"Applying Seq-NMS with linkage_thresh={self.linkage_threshold}, iou_thresh={self.iou_threshold}...\")\n",
    "        try:\n",
    "            # seq_nms_from_list operates on the pre-filtered lists\n",
    "            # It returns a list of updated score tensors where suppressed detections have score 0.\n",
    "            updated_scores_list = seq_nms_from_list(\n",
    "                boxes_list=boxes_list_filtered,\n",
    "                scores_list=scores_list_filtered, # Pass scores after pre-filtering\n",
    "                classes_list=labels_list_filtered, # Pass labels after pre-filtering\n",
    "                linkage_threshold=self.linkage_threshold,\n",
    "                iou_threshold=self.iou_threshold\n",
    "            )\n",
    "            print(\"Seq-NMS applied successfully.\")\n",
    "        except Exception as e:\n",
    "             # Catch potential errors during the C++/CUDA call\n",
    "             print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "             print(f\"Error occurred during seq_nms_from_list execution: {e}\")\n",
    "             print(f\"Inputs shapes summary (post-filtering):\")\n",
    "             print(f\"  boxes_list_filtered: {len(boxes_list_filtered)} frames, shapes {[b.shape for b in boxes_list_filtered[:5]]}...\")\n",
    "             print(f\"  scores_list_filtered: {len(scores_list_filtered)} frames, shapes {[s.shape for s in scores_list_filtered[:5]]}...\")\n",
    "             print(f\"  labels_list_filtered: {len(labels_list_filtered)} frames, shapes {[l.shape for l in labels_list_filtered[:5]]}...\")\n",
    "             print(f\"Returning results *before* Seq-NMS due to the error.\")\n",
    "             print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "             # Return the results obtained just after PostProcess and pre-filtering\n",
    "             final_results_per_frame_error = []\n",
    "             for i in range(num_frames):\n",
    "                 final_results_per_frame_error.append({\n",
    "                    'scores': scores_list_filtered[i],\n",
    "                    'labels': labels_list_filtered[i],\n",
    "                    'boxes': boxes_list_filtered[i]\n",
    "                 })\n",
    "             return final_results_per_frame_error\n",
    "\n",
    "        # --- 4. Filter Results Based on Seq-NMS Output ---\n",
    "        final_results_per_frame = []\n",
    "        num_kept_total_post_seqnms = 0\n",
    "        for i in range(num_frames):\n",
    "            # Get the pre-filtered results for the frame (these were input to seq_nms)\n",
    "            original_boxes = boxes_list_filtered[i]\n",
    "            original_labels = labels_list_filtered[i]\n",
    "            original_scores = scores_list_filtered[i] # Scores before Seq-NMS\n",
    "\n",
    "            # Get the scores modified by Seq-NMS\n",
    "            updated_scores = updated_scores_list[i] # Scores after Seq-NMS (0 if suppressed)\n",
    "\n",
    "            # Create a mask to keep only detections whose score > 0 after Seq-NMS\n",
    "            keep_mask_post = updated_scores > 0.0\n",
    "            num_kept_frame_post = keep_mask_post.sum().item()\n",
    "            num_kept_total_post_seqnms += num_kept_frame_post\n",
    "\n",
    "            # Filter the *pre-filtered* data using the mask derived from updated scores\n",
    "            final_results_per_frame.append({\n",
    "                'scores': original_scores[keep_mask_post], # Report original score\n",
    "                'labels': original_labels[keep_mask_post],\n",
    "                'boxes': original_boxes[keep_mask_post]\n",
    "            })\n",
    "\n",
    "        print(f\"Seq-NMS finished. Kept {num_kept_total_post_seqnms} final detections across {num_frames} frames.\")\n",
    "        return final_results_per_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5143de3f-149c-43fb-908c-312d6dac58cb",
   "metadata": {},
   "source": [
    "## Recurrent module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6ceff5c-17f6-4a87-abd9-fb9a737c1d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # Needed for padding\n",
    "\n",
    "class RecurrentVideoObjectModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Refactored module using a YOLO detector, producing output compatible\n",
    "    with the original implementation's format {'pred_logits': Tensor, 'pred_boxes': Tensor}.\n",
    "    Detections from each frame are padded to a maximum number before stacking.\n",
    "    Includes fix for grayscale input.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 detector_module: nn.Module, # E.g., a loaded YOLO model\n",
    "                 num_classes: int = 10, # Needed for converting scores/classes to logits\n",
    "\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.detector_module = detector_module\n",
    "        self.num_classes = num_classes + 1\n",
    "        # Store the number of foreground classes if needed elsewhere\n",
    "        self.num_foreground_classes = num_classes\n",
    "\n",
    "    @torch.no_grad() # Crucial for evaluation/inference mode\n",
    "    def forward(self, samples, targets: list = None):\n",
    "        \"\"\"\n",
    "        Processes video frames sequentially, collects raw detections, pads them\n",
    "        to the maximum number of detections found in any frame, and stacks them.\n",
    "        Handles potential grayscale input.\n",
    "\n",
    "        Args:\n",
    "            samples (torch.Tensor): Input video tensor (B, T, C, H, W).\n",
    "                                    Module assumes B=1. Can handle C=1 or C=3.\n",
    "            targets (list): List containing target dictionaries for the video.\n",
    "                            Passed through unmodified.\n",
    "\n",
    "        Returns:\n",
    "            tuple[dict, list]:\n",
    "                - dict: {'pred_logits': Tensor [T, MaxDetsPerFrame, NumClasses],\n",
    "                         'pred_boxes': Tensor [T, MaxDetsPerFrame, 4]}\n",
    "                         Padded and stacked detections across all frames.\n",
    "                - list: The original targets list (passed through).\n",
    "        \"\"\"\n",
    "        if samples.dim() != 5:\n",
    "             raise ValueError(f\"Expected input tensor with 5 dimensions (B, T, C, H, W), but got {samples.dim()}\")\n",
    "\n",
    "        if samples.shape[0] != 1:\n",
    "            print(f\"Warning: RecurrentVideoObjectModule expects batch size 1, but got {samples.shape[0]}. Processing only the first item.\")\n",
    "            samples = samples[:1]\n",
    "\n",
    "        # samples shape: (B, T, C, H, W), B=1 assumed\n",
    "        num_frames = samples.shape[1]\n",
    "        src = samples.permute(1, 0, 2, 3, 4).squeeze(1) # Shape: (T, C, H, W)\n",
    "        device = samples.device\n",
    "        input_channels = src.shape[1] # Get the number of channels C\n",
    "\n",
    "        # Check if input is grayscale (C=1) or color (C=3)\n",
    "        if input_channels not in [1, 3]:\n",
    "            raise ValueError(f\"Expected input frames to have 1 or 3 channels, but got {input_channels}\")\n",
    "\n",
    "        logits_accumulator = []\n",
    "        boxes_accumulator = []\n",
    "        max_detections = 0 # Keep track of the maximum number of detections in any frame\n",
    "\n",
    "        if isinstance(self.detector_module, nn.Module):\n",
    "              self.detector_module.eval()\n",
    "\n",
    "        for timestamp, frame in enumerate(src): # Iterate through frames (C, H, W)\n",
    "\n",
    "            # --- Ensure 3 Channels for YOLO ---\n",
    "            if input_channels == 1:\n",
    "                # Repeat the grayscale channel 3 times\n",
    "                frame = frame.repeat(3, 1, 1) # Shape becomes (3, H, W)\n",
    "\n",
    "            frame_batch = frame.unsqueeze(0) # Add batch dimension (1, 3, H, W)\n",
    "\n",
    "            # --- Run Detector (YOLO) ---\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    # Use model.predict for easier handling of results and NMS\n",
    "                    # Adjust conf/iou as needed for your specific detector/task\n",
    "                    detector_preds = self.detector_module.predict(frame_batch, conf=0.001, iou=0.5, device=device, verbose=False)\n",
    "                    # 'preds' is typically a list of Results objects, one per image\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during detector_module.predict on frame {timestamp}: {e}\")\n",
    "                print(\"Skipping frame due to prediction error.\")\n",
    "                # Decide how to handle: continue, raise, or append empty tensors?\n",
    "                # Appending empty tensors might be safer for padding later.\n",
    "                # logits_accumulator.append(torch.empty((0, self.num_classes), device=device))\n",
    "                # boxes_accumulator.append(torch.empty((0, 4), device=device))\n",
    "                raise e\n",
    "\n",
    "            # --- Process Detections ---\n",
    "            results_obj = detector_preds[0] # Assuming batch size 1 for predict\n",
    "\n",
    "\n",
    "            # Use normalized xywh format directly if available\n",
    "            frame_boxes_cxcywh = results_obj.boxes.xywhn.to(device)\n",
    "\n",
    "            # print(f\"frame_boxes_cxcywh: {frame_boxes_cxcywh.shape}\")\n",
    "            \n",
    "            frame_scores = results_obj.boxes.conf.to(device)\n",
    "\n",
    "            # print(f\"frame_scores: {frame_scores.shape}\")\n",
    "            # print(frame_scores)\n",
    "            \n",
    "            frame_classes = results_obj.boxes.cls.to(device).long()\n",
    "\n",
    "            # print(f\"frame_classes: {frame_classes.shape}\")\n",
    "\n",
    "            num_detections = frame_classes.shape[0]\n",
    "\n",
    "            # print(f\"num_detections: {num_detections}\")\n",
    "\n",
    "            # --- Convert scores/classes to approximate logits ---\n",
    "            frame_logits = torch.zeros((num_detections, self.num_classes), device=device)\n",
    "            \n",
    "            # Place the score logits at the correct class index\n",
    "            frame_logits[torch.arange(num_detections, device=device), frame_classes] = frame_scores\n",
    "            \n",
    "            logits_accumulator.append(frame_logits)\n",
    "            boxes_accumulator.append(frame_boxes_cxcywh)\n",
    "\n",
    "            # Update max detections found so far\n",
    "            if num_detections > max_detections:\n",
    "                max_detections = num_detections\n",
    "\n",
    "        # --- Pad tensors to max_detections ---\n",
    "        padded_logits = []\n",
    "        padded_boxes = []\n",
    "        pad_val_boxes = 0.0\n",
    "        background_class_index = self.num_classes - 1 # Last index is background\n",
    "        for frame_logits, frame_boxes in zip(logits_accumulator, boxes_accumulator):\n",
    "            num_dets = frame_logits.shape[0]\n",
    "            pad_size = max_detections - num_dets\n",
    "\n",
    "            if pad_size > 0:\n",
    "                # --- Pad logits ---\n",
    "                # Create padding with 0.0 everywhere first\n",
    "                logit_padding = torch.zeros((pad_size, self.num_classes), device=device, dtype=frame_logits.dtype)\n",
    "                # Set the background class logit to +inf for padded entries\n",
    "                logit_padding[:, background_class_index] = float('inf')\n",
    "                # Concatenate original logits with the padding\n",
    "                padded_frame_logits = torch.cat((frame_logits, logit_padding), dim=0)\n",
    "\n",
    "                # --- Pad boxes ---\n",
    "                # Pad boxes with 0.0\n",
    "                box_padding = torch.full((pad_size, 4), pad_val_boxes, device=device, dtype=frame_boxes.dtype)\n",
    "                # Concatenate original boxes with the padding\n",
    "                padded_frame_boxes = torch.cat((frame_boxes, box_padding), dim=0)\n",
    "            else:\n",
    "                # No padding needed if already max_detections\n",
    "                padded_frame_logits = frame_logits\n",
    "                padded_frame_boxes = frame_boxes\n",
    "\n",
    "            padded_logits.append(padded_frame_logits)\n",
    "            padded_boxes.append(padded_frame_boxes)\n",
    "\n",
    "\n",
    "        final_logits = torch.stack(padded_logits) # Shape: [T, max_detections, num_classes]\n",
    "        final_boxes = torch.stack(padded_boxes)   # Shape: [T, max_detections, 4]\n",
    "\n",
    "        result = {\n",
    "            'pred_logits': final_logits,\n",
    "            'pred_boxes': final_boxes\n",
    "        }\n",
    "\n",
    "        return result, targets[0]\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"Sets the detector module to evaluation mode.\"\"\"\n",
    "        if isinstance(self.detector_module, nn.Module):\n",
    "            self.detector_module.eval()\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6075d841-970f-40ae-b52d-033347d01a6f",
   "metadata": {},
   "source": [
    "## MAP Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3df20ff-2fea-4e25-a9e3-626f746a4be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "# Assuming box_ops contains the necessary bounding box utility functions like box_cxcywh_to_xyxy\n",
    "# Import the actual box_ops module\n",
    "from util import box_ops\n",
    "# Assuming PostProcess is importable, e.g.:\n",
    "# from models.perceiver import PostProcess # Adjust import path as needed\n",
    "# Assuming box_iou is available from torchvision.ops\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "\n",
    "class MeanAveragePrecisionEvaluator:\n",
    "    \"\"\"\n",
    "    Computes Mean Average Precision (mAP) for object detection tasks using a provided postprocessor.\n",
    "    Also computes separate mAP scores for:\n",
    "    1. Predictions matched to overlapping ground truth objects.\n",
    "    2. Predictions matched to ground truth objects near the frame boundary.\n",
    "\n",
    "    Processes model outputs assuming shape [sequence_length, num_queries, ...] and targets_flat\n",
    "    as a list of dictionaries of length sequence_length.\n",
    "\n",
    "    Standard mAP is updated per timestamp. Overlap and Boundary mAP use results\n",
    "    from a single matcher call performed once per sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, device, postprocessor, matcher, box_format='xyxy', iou_thresholds=None, overlap_iou_threshold=0.01, boundary_pixel_tolerance=0):\n",
    "        \"\"\"\n",
    "        Initializes the mAP evaluator.\n",
    "\n",
    "        Parameters:\n",
    "        device (torch.device): The device to run computations on (e.g., 'cuda', 'cpu').\n",
    "        postprocessor (torch.nn.Module): Postprocessing module (e.g., PostProcess).\n",
    "        matcher (torch.nn.Module): Matcher instance (e.g., HungarianMatcher). Assumed to handle\n",
    "                                   sequence dim as batch dim or be adapted accordingly.\n",
    "        box_format (str): Box format for torchmetrics ('xyxy').\n",
    "        iou_thresholds (list, optional): IoU thresholds for mAP calculation.\n",
    "        overlap_iou_threshold (float): Min IoU between two GT boxes to be considered overlapping.\n",
    "        boundary_pixel_tolerance (int): Pixel distance from edge to consider a GT box as 'on boundary'.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.postprocessor = postprocessor.to(self.device)\n",
    "        self.matcher = matcher # Store matcher instance\n",
    "        self.overlap_iou_threshold = overlap_iou_threshold\n",
    "        self.boundary_pixel_tolerance = boundary_pixel_tolerance\n",
    "\n",
    "        # --- Initialize Metrics ---\n",
    "        self.map_metric = torchmetrics.detection.MeanAveragePrecision(\n",
    "            box_format='xyxy', iou_type='bbox', iou_thresholds=iou_thresholds\n",
    "        ).to(self.device)\n",
    "        self.map_results = None\n",
    "\n",
    "        self.map_metric_overlap = torchmetrics.detection.MeanAveragePrecision(\n",
    "            box_format='xyxy', iou_type='bbox', iou_thresholds=iou_thresholds\n",
    "        ).to(self.device)\n",
    "        self.map_results_overlap = None\n",
    "\n",
    "        self.map_metric_boundary = torchmetrics.detection.MeanAveragePrecision(\n",
    "            box_format='xyxy', iou_type='bbox', iou_thresholds=iou_thresholds\n",
    "        ).to(self.device)\n",
    "        self.map_results_boundary = None\n",
    "\n",
    "\n",
    "    # Helper function provided by user (might need adjustment based on actual matcher output format)\n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "      # This helper might not be directly usable if indices is now a list over sequence length\n",
    "      # The logic using matcher results below accesses indices[t] directly.\n",
    "      print(\"Warning: _get_src_permutation_idx might not be suitable for the new matcher output format.\")\n",
    "      # Placeholder return if called unexpectedly\n",
    "      return torch.tensor([], dtype=torch.long), torch.tensor([], dtype=torch.long)\n",
    "\n",
    "\n",
    "    def update(self, outputs, targets_flat):\n",
    "        \"\"\"\n",
    "        Updates the evaluator state. Standard mAP is updated per timestamp.\n",
    "        Matcher runs once per sequence. Overlap/Boundary metrics are updated\n",
    "        per timestamp using the single matcher result.\n",
    "\n",
    "        Parameters:\n",
    "        outputs (dict): Raw outputs. Keys: 'pred_logits', 'pred_boxes'. Shape [seq_len, num_queries, ...].\n",
    "        targets_flat (list[dict]): GT dicts. Len=seq_len. Keys: 'orig_size', 'boxes', 'labels'.\n",
    "        \"\"\"\n",
    "        # --- Input Validation ---\n",
    "        if not isinstance(targets_flat, list): return\n",
    "        if 'pred_logits' not in outputs or 'pred_boxes' not in outputs: return\n",
    "\n",
    "        pred_logits_seq = outputs['pred_logits'].to(self.device) # Shape [seq_len, num_queries, ...]\n",
    "        pred_boxes_seq = outputs['pred_boxes'].to(self.device)   # Shape [seq_len, num_queries, ...]\n",
    "        seq_len = pred_logits_seq.shape[0]\n",
    "\n",
    "        if pred_logits_seq.dim() < 2 or seq_len != len(targets_flat):\n",
    "             print(f\"Error: Mismatch/Shape issue between outputs ({pred_logits_seq.shape}) and targets_flat ({len(targets_flat)}). Skipping.\")\n",
    "             return\n",
    "\n",
    "        # --- Store per-timestamp processed data ---\n",
    "        all_postprocessed_preds = []\n",
    "        all_processed_targets = []\n",
    "        all_orig_sizes = [] # Store original sizes (tensor[2]) for boundary check\n",
    "\n",
    "        # --- First Pass: Update Standard mAP & Prepare Data ---\n",
    "        for t in range(seq_len):\n",
    "            target_dict = targets_flat[t]\n",
    "            if not isinstance(target_dict, dict) or not all(k in target_dict for k in ['orig_size', 'boxes', 'labels']):\n",
    "                 print(f\"Error: Invalid target dict at timestamp {t}. Skipping timestamp.\")\n",
    "                 all_postprocessed_preds.append(None)\n",
    "                 all_processed_targets.append(None)\n",
    "                 all_orig_sizes.append(None)\n",
    "                 continue\n",
    "\n",
    "            # Prepare Predictions (Post-processed)\n",
    "            orig_size_tensor_t = None\n",
    "            prediction_dict_postprocessed_t = None\n",
    "            try:\n",
    "                orig_size_tensor_t = target_dict['orig_size'].to(self.device)\n",
    "                if orig_size_tensor_t.shape != (2,): raise ValueError(\"Incorrect orig_size shape\")\n",
    "                orig_target_size_tensor_batch = orig_size_tensor_t.unsqueeze(0) # Shape [1, 2]\n",
    "\n",
    "                outputs_raw_t_batch = { # Need batch dim for postprocessor\n",
    "                    'pred_logits': pred_logits_seq[t].unsqueeze(0),\n",
    "                    'pred_boxes': pred_boxes_seq[t].unsqueeze(0)\n",
    "                }\n",
    "                with torch.no_grad():\n",
    "                    predictions_postprocessed_t_list = self.postprocessor(outputs_raw_t_batch, orig_target_size_tensor_batch)\n",
    "                if len(predictions_postprocessed_t_list) != 1: raise RuntimeError(\"Postprocessor failed\")\n",
    "                prediction_dict_postprocessed_t = predictions_postprocessed_t_list[0]\n",
    "                all_postprocessed_preds.append(prediction_dict_postprocessed_t)\n",
    "                all_orig_sizes.append(orig_size_tensor_t) # Store shape [2] tensor\n",
    "\n",
    "            except Exception as e:\n",
    "                 print(f\"Error processing predictions for timestamp {t}: {e}. Skipping.\")\n",
    "                 all_postprocessed_preds.append(None)\n",
    "                 all_processed_targets.append(None)\n",
    "                 all_orig_sizes.append(None)\n",
    "                 continue\n",
    "\n",
    "            # Prepare Targets (Absolute xyxy)\n",
    "            target_dict_processed_t = None\n",
    "            try:\n",
    "                target_dict_device = {k: v.to(self.device) for k, v in target_dict.items() if isinstance(v, torch.Tensor)}\n",
    "                img_h_target_tensor = orig_size_tensor_t[0]\n",
    "                img_w_target_tensor = orig_size_tensor_t[1]\n",
    "                scale_fct_target = torch.stack([img_w_target_tensor, img_h_target_tensor, img_w_target_tensor, img_h_target_tensor], dim=0)\n",
    "\n",
    "                boxes_target = target_dict_device['boxes'] # Assumed relative cxcywh\n",
    "                labels_target = target_dict_device['labels'] # Original labels\n",
    "\n",
    "                gt_boxes_xyxy = torch.empty((0,4), device=self.device) # Default empty boxes\n",
    "                labels_final = torch.empty(0, dtype=torch.long, device=self.device) # Default empty labels\n",
    "\n",
    "                if boxes_target.numel() > 0:\n",
    "                     # Only process boxes and use original labels if boxes exist\n",
    "                     gt_boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes_target) * scale_fct_target\n",
    "                     # Ensure labels correspond to the boxes processed\n",
    "                     if labels_target.shape[0] == boxes_target.shape[0]:\n",
    "                          labels_final = labels_target\n",
    "                     else:\n",
    "                          print(f\"Warning: Mismatch between number of boxes ({boxes_target.shape[0]}) and labels ({labels_target.shape[0]}) at timestamp {t}. Using empty labels.\")\n",
    "                          # Keep labels_final empty if mismatch occurs\n",
    "                # else: gt_boxes_xyxy and labels_final remain empty\n",
    "\n",
    "                target_dict_processed_t = {\n",
    "                    'boxes': gt_boxes_xyxy, # Shape [N, 4] or [0, 4]\n",
    "                    'labels': labels_final   # Shape [N] or [0]\n",
    "                }\n",
    "                all_processed_targets.append(target_dict_processed_t)\n",
    "            except Exception as e:\n",
    "                 print(f\"Error preparing targets for timestamp {t}: {e}. Skipping.\")\n",
    "                 all_processed_targets.append(None)\n",
    "                 if len(all_postprocessed_preds) > len(all_processed_targets):\n",
    "                      all_postprocessed_preds.pop()\n",
    "                      all_orig_sizes.pop()\n",
    "                 continue\n",
    "\n",
    "            # Update Standard mAP Metric for this timestamp\n",
    "            # Ensure both prediction and target dicts are valid before updating\n",
    "            if prediction_dict_postprocessed_t is not None and target_dict_processed_t is not None:\n",
    "                try:\n",
    "                    self.map_metric.update([prediction_dict_postprocessed_t], [target_dict_processed_t])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error updating standard mAP metric for timestamp {t}: {e}\")\n",
    "            else:\n",
    "                 print(f\"Skipping standard mAP update for timestamp {t} due to previous errors.\")\n",
    "\n",
    "\n",
    "        # --- Run Matcher ONCE for the whole sequence ---\n",
    "        indices_seq = None # Will store list of tuples: [(pred_idx_t, gt_idx_t)] * seq_len\n",
    "        try:\n",
    "            # Prepare inputs for matcher (treating sequence as batch)\n",
    "            outputs_raw_seq = {'pred_logits': pred_logits_seq, 'pred_boxes': pred_boxes_seq}\n",
    "            # Ensure targets are on device and have required keys\n",
    "            targets_flat_device = []\n",
    "            valid_targets_exist = False\n",
    "            for t_idx, td in enumerate(targets_flat):\n",
    "                 # Use the already processed targets if available and valid\n",
    "                 processed_target = all_processed_targets[t_idx]\n",
    "                 if processed_target is not None and isinstance(processed_target.get('boxes'), torch.Tensor) and isinstance(processed_target.get('labels'), torch.Tensor):\n",
    "                      # Matcher might expect original format, re-prepare if necessary\n",
    "                      # For simplicity, let's assume matcher works with processed targets (abs xyxy)\n",
    "                      # If matcher needs relative cxcywh, re-fetch from targets_flat\n",
    "                      original_target = targets_flat[t_idx]\n",
    "                      if isinstance(original_target, dict) and all(k in original_target for k in ['boxes', 'labels']):\n",
    "                           targets_flat_device.append({k: v.to(self.device) for k, v in original_target.items() if k in ['boxes', 'labels']})\n",
    "                           valid_targets_exist = True\n",
    "                      else:\n",
    "                           # Add placeholder if original target was invalid\n",
    "                           targets_flat_device.append({'boxes': torch.empty((0,4), device=self.device), 'labels': torch.empty(0, dtype=torch.long, device=self.device)})\n",
    "\n",
    "                 else:\n",
    "                      # Add placeholder if processing failed\n",
    "                      targets_flat_device.append({'boxes': torch.empty((0,4), device=self.device), 'labels': torch.empty(0, dtype=torch.long, device=self.device)})\n",
    "\n",
    "\n",
    "            if valid_targets_exist: # Only run matcher if there's something to match\n",
    "                 with torch.no_grad():\n",
    "                      # Assumes matcher takes [Seq, N, C] outputs and List[Dict] targets (len=Seq)\n",
    "                      indices_seq = self.matcher(outputs_raw_seq, targets_flat_device)\n",
    "            else:\n",
    "                 print(\"Skipping matcher run as no valid targets were found in the sequence.\")\n",
    "\n",
    "        except Exception as e:\n",
    "             print(f\"Error running matcher for the sequence: {e}\")\n",
    "             indices_seq = None # Ensure indices_seq is None if matcher fails\n",
    "\n",
    "        # --- Second Pass: Update Overlap and Boundary Metrics ---\n",
    "        if indices_seq is not None and len(indices_seq) == seq_len: # Check if matcher ran successfully and returned expected length\n",
    "            for t in range(seq_len):\n",
    "                # Retrieve pre-processed data for timestamp t\n",
    "                prediction_dict_postprocessed_t = all_postprocessed_preds[t]\n",
    "                target_dict_processed_t = all_processed_targets[t]\n",
    "                orig_size_tensor_t = all_orig_sizes[t]\n",
    "\n",
    "                # Skip if data preparation failed in the first pass for this timestamp\n",
    "                if prediction_dict_postprocessed_t is None or target_dict_processed_t is None or orig_size_tensor_t is None:\n",
    "                    continue\n",
    "\n",
    "                # Get matcher results for this specific timestamp\n",
    "                # Add check for empty matcher results for this timestamp\n",
    "                if t >= len(indices_seq) or not indices_seq[t] or len(indices_seq[t]) != 2:\n",
    "                     # print(f\"Matcher results missing or invalid for timestamp {t}. Skipping overlap/boundary.\")\n",
    "                     continue # Skip if no valid match for this frame\n",
    "                #matched_pred_indices_t, matched_gt_indices_t = indices_seq[t]\n",
    "                matched_pred_indices_t = indices_seq[t][0].to(self.device)\n",
    "                matched_gt_indices_t = indices_seq[t][1].to(self.device)\n",
    "\n",
    "                gt_boxes_xyxy = target_dict_processed_t['boxes']\n",
    "                num_gt = gt_boxes_xyxy.shape[0]\n",
    "\n",
    "                # --- Overlap Calculation ---\n",
    "                if num_gt > 1: # Overlap requires at least 2 GT boxes\n",
    "                    try:\n",
    "                        gt_iou_matrix = box_iou(gt_boxes_xyxy, gt_boxes_xyxy)\n",
    "                        gt_iou_matrix.fill_diagonal_(0)\n",
    "                        overlaps_exist = (gt_iou_matrix > self.overlap_iou_threshold).any(dim=1)\n",
    "                        overlap_gt_indices = torch.where(overlaps_exist)[0].to(self.device)\n",
    "\n",
    "                        if overlap_gt_indices.numel() > 0:\n",
    "                            targets_overlap = {\n",
    "                                'boxes': gt_boxes_xyxy[overlap_gt_indices],\n",
    "                                'labels': target_dict_processed_t['labels'][overlap_gt_indices]\n",
    "                            }\n",
    "                            if matched_pred_indices_t.numel() > 0: # Check if matcher found matches for this frame\n",
    "                                is_overlap_match = torch.isin(matched_gt_indices_t, overlap_gt_indices)\n",
    "                                pred_indices_for_overlap = matched_pred_indices_t[is_overlap_match]\n",
    "                                if pred_indices_for_overlap.numel() > 0:\n",
    "                                    predictions_overlap = {\n",
    "                                        'scores': prediction_dict_postprocessed_t['scores'][pred_indices_for_overlap],\n",
    "                                        'labels': prediction_dict_postprocessed_t['labels'][pred_indices_for_overlap],\n",
    "                                        'boxes': prediction_dict_postprocessed_t['boxes'][pred_indices_for_overlap],\n",
    "                                    }\n",
    "                                    self.map_metric_overlap.update([predictions_overlap], [targets_overlap])\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error calculating/updating overlap mAP for timestamp {t}: {e}\")\n",
    "\n",
    "                # --- Boundary Calculation ---\n",
    "                if num_gt > 0: # Boundary check needs at least 1 GT box\n",
    "                     try:\n",
    "                        img_h_target_tensor = orig_size_tensor_t[0]\n",
    "                        img_w_target_tensor = orig_size_tensor_t[1]\n",
    "                        xmin, ymin, xmax, ymax = gt_boxes_xyxy.unbind(-1)\n",
    "                        tol = self.boundary_pixel_tolerance\n",
    "                        is_on_boundary = (xmin <= tol) | (ymin <= tol) | \\\n",
    "                                         (xmax >= img_w_target_tensor - tol) | (ymax >= img_h_target_tensor - tol)\n",
    "                        boundary_gt_indices = torch.where(is_on_boundary)[0].to(self.device)\n",
    "\n",
    "                        if boundary_gt_indices.numel() > 0:\n",
    "                            targets_boundary = {\n",
    "                                'boxes': gt_boxes_xyxy[boundary_gt_indices],\n",
    "                                'labels': target_dict_processed_t['labels'][boundary_gt_indices]\n",
    "                            }\n",
    "                            if matched_pred_indices_t.numel() > 0: # Check if matcher found matches for this frame\n",
    "                                is_boundary_match = torch.isin(matched_gt_indices_t, boundary_gt_indices)\n",
    "                                pred_indices_for_boundary = matched_pred_indices_t[is_boundary_match]\n",
    "                                if pred_indices_for_boundary.numel() > 0:\n",
    "                                    predictions_boundary = {\n",
    "                                        'scores': prediction_dict_postprocessed_t['scores'][pred_indices_for_boundary],\n",
    "                                        'labels': prediction_dict_postprocessed_t['labels'][pred_indices_for_boundary],\n",
    "                                        'boxes': prediction_dict_postprocessed_t['boxes'][pred_indices_for_boundary],\n",
    "                                    }\n",
    "                                    self.map_metric_boundary.update([predictions_boundary], [targets_boundary])\n",
    "                     except Exception as e:\n",
    "                         print(f\"Error calculating/updating boundary mAP for timestamp {t}: {e}\")\n",
    "        else:\n",
    "             print(\"Matcher did not run successfully or returned unexpected format. Skipping overlap/boundary metric updates.\")\n",
    "\n",
    "\n",
    "    def accumulate(self):\n",
    "        \"\"\"\n",
    "        Computes the final mAP results across all updated timestamps from all sequences.\n",
    "        Stores the results internally.\n",
    "        \"\"\"\n",
    "        # Accumulate standard mAP\n",
    "        try:\n",
    "            self.map_results = self.map_metric.compute()\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing standard mAP metric: {e}\")\n",
    "            self.map_results = None\n",
    "\n",
    "        # Accumulate overlap mAP\n",
    "        try:\n",
    "            self.map_results_overlap = self.map_metric_overlap.compute()\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing overlap mAP metric: {e}\")\n",
    "            self.map_results_overlap = None\n",
    "\n",
    "        # Accumulate boundary mAP\n",
    "        try:\n",
    "            self.map_results_boundary = self.map_metric_boundary.compute()\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing boundary mAP metric: {e}\")\n",
    "            self.map_results_boundary = None\n",
    "\n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"\n",
    "        Processes and returns the computed mAP results in a dictionary, including overlap and boundary metrics.\n",
    "        NOTE: Metrics reflect performance averaged over *all timestamps* processed.\n",
    "\n",
    "        Returns:\n",
    "        dict: Dictionary containing scalar mAP results prefixed with 'mAP_', 'mAP_overlap_', and 'mAP_boundary_'.\n",
    "        \"\"\"\n",
    "        summary_dict = {}\n",
    "\n",
    "        # Process standard mAP results\n",
    "        if self.map_results is not None:\n",
    "            print(f\"\\nRaw mAP Results (Evaluator): {self.map_results}\\n\")\n",
    "            for k, v in self.map_results.items():\n",
    "                if isinstance(v, torch.Tensor) and v.numel() == 1:\n",
    "                    summary_dict[f'mAP_{k}'] = v.item() # Prefix standard keys\n",
    "            if not any(k.startswith('mAP_') for k in summary_dict) and self.map_results:\n",
    "                 if 'map' in self.map_results and self.map_results['map'] == 0: pass\n",
    "                 else: print(\"Warning (Evaluator): No scalar standard mAP metrics found.\")\n",
    "        else:\n",
    "             print(\"Warning (Evaluator): Standard mAP metric computation failed or produced no results.\")\n",
    "\n",
    "        # Process overlap mAP results\n",
    "        if self.map_results_overlap is not None:\n",
    "            print(f\"\\nRaw Overlap mAP Results (Evaluator): {self.map_results_overlap}\\n\")\n",
    "            for k, v in self.map_results_overlap.items():\n",
    "                if isinstance(v, torch.Tensor) and v.numel() == 1:\n",
    "                    summary_dict[f'mAP_overlap_{k}'] = v.item() # Prefix overlap keys\n",
    "            if not any(k.startswith('mAP_overlap_') for k in summary_dict) and self.map_results_overlap:\n",
    "                 if 'map' in self.map_results_overlap and self.map_results_overlap['map'] == 0: pass\n",
    "                 else: print(\"Warning (Evaluator): No scalar overlap mAP metrics found.\")\n",
    "        else:\n",
    "             print(\"Warning (Evaluator): Overlap mAP metric computation failed or produced no results.\")\n",
    "\n",
    "        # Process boundary mAP results\n",
    "        if self.map_results_boundary is not None:\n",
    "            print(f\"\\nRaw Boundary mAP Results (Evaluator): {self.map_results_boundary}\\n\")\n",
    "            for k, v in self.map_results_boundary.items():\n",
    "                if isinstance(v, torch.Tensor) and v.numel() == 1:\n",
    "                    summary_dict[f'mAP_boundary_{k}'] = v.item() # Prefix boundary keys\n",
    "            if not any(k.startswith('mAP_boundary_') for k in summary_dict) and self.map_results_boundary:\n",
    "                 if 'map' in self.map_results_boundary and self.map_results_boundary['map'] == 0: pass\n",
    "                 else: print(\"Warning (Evaluator): No scalar boundary mAP metrics found.\")\n",
    "        else:\n",
    "             print(\"Warning (Evaluator): Boundary mAP metric computation failed or produced no results.\")\n",
    "\n",
    "\n",
    "        return summary_dict\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the internal state of all metrics.\"\"\"\n",
    "        self.map_metric.reset()\n",
    "        self.map_results = None\n",
    "        self.map_metric_overlap.reset()\n",
    "        self.map_results_overlap = None\n",
    "        self.map_metric_boundary.reset()\n",
    "        self.map_results_boundary = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4e1bd5-97f5-4bb7-a2e8-53fb59a9cead",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "206fe301-b091-4e49-a2f6-e460bc7a05dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/helios/home/ploter/.conda/envs/sensor_dropout/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test huggingface MovingMNIST dataset...\n",
      "Using object detection mode\n",
      "Resizing frames to 320x320\n",
      "Transforms: Compose(\n",
      "    <dataset.transformations.RandomResize object at 0x14775cec5210>\n",
      "    <dataset.transformations.NormBoxesTransform object at 0x14775cec4950>\n",
      ")\n",
      "ds size: 10000\n",
      "Loading checkpoint from ../not_tracked_dir/output_perceiver_detection_2025-04-23_19-42-48/checkpoint_epoch_31.pth\n",
      "Using CNN v2 backbone with resized input size 320x320\n",
      "num_freq_bands: 6\n",
      "depth: 1\n",
      "max_freq: 10\n",
      "input_channels: 128\n",
      "input_axis: 2\n",
      "num_latents: 16\n",
      "latent_dim: 128\n",
      "cross_heads: 1\n",
      "latent_heads: 1\n",
      "cross_dim_head: 154\n",
      "latent_dim_head: 128\n",
      "num_classes: -1\n",
      "attn_dropout: 0.0\n",
      "ff_dropout: 0.0\n",
      "weight_tie_layers: False\n",
      "fourier_encode_data: True\n",
      "self_per_cross_attn: 1\n",
      "final_classifier_head: False\n",
      "num_sensors: 1\n",
      "__class__: <class 'models.perceiver.Perceiver'>\n",
      "Using HungarianMatcher for object detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval -1::   0%|          | 0/10000 [00:00<?, ?it/s]/gpfs/helios/home/ploter/.conda/envs/sensor_dropout/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "Eval -1:: 100%|| 10000/10000 [34:24<00:00,  4.84it/s, loss_running=0, class_error_running=nan, loss_center_point_running=nan, loss_ce_running=0, loss=0, loss_ce_unscaled=0, loss_bbox_unscaled=0, loss_giou_unscaled=0, loss_unscaled=0, loss_ce=0, view_dropout_prob=-1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw mAP Results (Evaluator): {'map': tensor(0.9025), 'map_50': tensor(0.9688), 'map_75': tensor(0.9442), 'map_small': tensor(0.9025), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.7367), 'mar_10': tensor(0.9240), 'mar_100': tensor(0.9240), 'mar_small': tensor(0.9240), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
      "\n",
      "\n",
      "Raw Overlap mAP Results (Evaluator): {'map': tensor(0.6940), 'map_50': tensor(0.8853), 'map_75': tensor(0.7777), 'map_small': tensor(0.6940), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.6686), 'mar_10': tensor(0.7389), 'mar_100': tensor(0.7389), 'mar_small': tensor(0.7389), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
      "\n",
      "\n",
      "Raw Boundary mAP Results (Evaluator): {'map': tensor(0.8470), 'map_50': tensor(0.9552), 'map_75': tensor(0.9101), 'map_small': tensor(0.8470), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.8512), 'mar_10': tensor(0.8739), 'mar_100': tensor(0.8739), 'mar_small': tensor(0.8739), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
      "\n",
      "Evaluation Results:\n",
      "loss_running: 0.0000\n",
      "class_error_running: nan\n",
      "loss_center_point_running: nan\n",
      "loss_ce_running: 0.0000\n",
      "loss: 0.0000\n",
      "loss_ce_unscaled: 0.0000\n",
      "loss_bbox_unscaled: 0.0000\n",
      "loss_giou_unscaled: 0.0000\n",
      "loss_unscaled: 0.0000\n",
      "loss_ce: 0.0000\n",
      "mAP_map: 0.9025\n",
      "mAP_map_50: 0.9688\n",
      "mAP_map_75: 0.9442\n",
      "mAP_map_small: 0.9025\n",
      "mAP_map_medium: -1.0000\n",
      "mAP_map_large: -1.0000\n",
      "mAP_mar_1: 0.7367\n",
      "mAP_mar_10: 0.9240\n",
      "mAP_mar_100: 0.9240\n",
      "mAP_mar_small: 0.9240\n",
      "mAP_mar_medium: -1.0000\n",
      "mAP_mar_large: -1.0000\n",
      "mAP_map_per_class: -1.0000\n",
      "mAP_mar_100_per_class: -1.0000\n",
      "mAP_overlap_map: 0.6940\n",
      "mAP_overlap_map_50: 0.8853\n",
      "mAP_overlap_map_75: 0.7777\n",
      "mAP_overlap_map_small: 0.6940\n",
      "mAP_overlap_map_medium: -1.0000\n",
      "mAP_overlap_map_large: -1.0000\n",
      "mAP_overlap_mar_1: 0.6686\n",
      "mAP_overlap_mar_10: 0.7389\n",
      "mAP_overlap_mar_100: 0.7389\n",
      "mAP_overlap_mar_small: 0.7389\n",
      "mAP_overlap_mar_medium: -1.0000\n",
      "mAP_overlap_mar_large: -1.0000\n",
      "mAP_overlap_map_per_class: -1.0000\n",
      "mAP_overlap_mar_100_per_class: -1.0000\n",
      "mAP_boundary_map: 0.8470\n",
      "mAP_boundary_map_50: 0.9552\n",
      "mAP_boundary_map_75: 0.9101\n",
      "mAP_boundary_map_small: 0.8470\n",
      "mAP_boundary_map_medium: -1.0000\n",
      "mAP_boundary_map_large: -1.0000\n",
      "mAP_boundary_mar_1: 0.8512\n",
      "mAP_boundary_mar_10: 0.8739\n",
      "mAP_boundary_mar_100: 0.8739\n",
      "mAP_boundary_mar_small: 0.8739\n",
      "mAP_boundary_mar_medium: -1.0000\n",
      "mAP_boundary_mar_large: -1.0000\n",
      "mAP_boundary_map_per_class: -1.0000\n",
      "mAP_boundary_mar_100_per_class: -1.0000\n"
     ]
    }
   ],
   "source": [
    "from engine import evaluate\n",
    "from dataset import build_dataset\n",
    "import torch\n",
    "from engine import evaluate\n",
    "from dataset import build_dataset\n",
    "from models import build_model\n",
    "from models.perceiver import PostProcess\n",
    "import os\n",
    "from pathlib import Path\n",
    "from util.misc import collate_fn, is_main_process, get_sha, get_rank\n",
    "from torch.utils.data import DataLoader\n",
    "from ultralytics import YOLO\n",
    "import torchvision.transforms as T\n",
    "from models.matcher import build_matcher\n",
    "\n",
    "args.model = 'perceiver' # 'perceiver'\n",
    "\n",
    "if args.model == 'perceiver':\n",
    "    # https://wandb.ai/university-of-tartu-2/multi-sensor-dropout/runs/stu3mw4u/overview\n",
    "    args.resume = 'checkpoint_epoch_31.pth'\n",
    "    args.output_dir = \"../not_tracked_dir/output_perceiver_detection_2025-04-23_19-42-48/\"\n",
    "    args.resize_frame = 320\n",
    "elif args.model == 'YOLO':\n",
    "    args.output_dir = \"../not_tracked_dir/output_yolo_v8_2025-04-11/\" # YOLO\n",
    "    args.resume = 'weights/best.pt'\n",
    "    args.resize_frame = 320\n",
    "else:\n",
    "    raise Error(f\"Unsupported model: {args.model}\")\n",
    "\n",
    "args.test_dataset_fraction = 1\n",
    "# args.num_workers = 1\n",
    "\n",
    "checkpoint_path = os.path.join(args.output_dir, args.resume)\n",
    "\n",
    "dataset_test = build_dataset(split='test', args=args)\n",
    "\n",
    "print(f\"ds size: {len(dataset_test)}\")\n",
    "\n",
    "sampler_test = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "dataloader_test = DataLoader(dataset_test, sampler=sampler_test, batch_size=args.batch_size,\n",
    "                                collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "if args.model == 'YOLO':\n",
    "    print(\"Loading YOLO model for RecurrentVideoObjectModule...\")\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"YOLO checkpoint (.pt file) not found at {checkpoint_path}\")\n",
    "\n",
    "    # Load the base YOLO detector\n",
    "    yolo_detector = YOLO(checkpoint_path)\n",
    "    print(f\"YOLO detector loaded from {checkpoint_path}\")\n",
    "\n",
    "    # Get number of classes from the loaded YOLO model\n",
    "    num_classes = len(yolo_detector.names)\n",
    "    print(f\"Detected {num_classes} classes from YOLO model.\")\n",
    "\n",
    "    # Instantiate the wrapper module\n",
    "    model = RecurrentVideoObjectModule(detector_module=yolo_detector, num_classes=num_classes)\n",
    "    print(\"RecurrentVideoObjectModule created with YOLO detector.\")\n",
    "\n",
    "    print(\"Clear norm transforms\")\n",
    "    dataset_test.norm_transforms = T.Compose([])\n",
    "    \n",
    "elif args.model == 'perceiver' and os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "    model = build_model(args, (0, 0))\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=args.device)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "else:\n",
    "    raise Error(\"Model not supported.\")\n",
    "    \n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "postprocessors = {\n",
    "    #'bbox': PostProcess() # SeqNMSEvaluationPostprocessor(PostProcess())\n",
    "    # '': \n",
    "}\n",
    "\n",
    "matcher = build_matcher(args)\n",
    "matcher.to(device)\n",
    "\n",
    "evaluators = [\n",
    "    MeanAveragePrecisionEvaluator(device=device, postprocessor = PostProcess(), matcher=matcher)\n",
    "]\n",
    "\n",
    "criterion = DummyCriterion(device)\n",
    "\n",
    "model.to(args.device)\n",
    "\n",
    "results = evaluate(\n",
    "\tmodel=model,\n",
    "\tdataloader=dataloader_test,\n",
    "\tcriterion=criterion,  # Not needed for evaluation\n",
    "\tpostprocessors=postprocessors,\n",
    "\tepoch=-1,\n",
    "\tdevice=device,\n",
    "    evaluators=evaluators\n",
    ")\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "for metric_name, value in results.items():\n",
    "\tprint(f\"{metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c3a5e1-cf6e-4669-b88a-8d174a6a7d75",
   "metadata": {},
   "source": [
    "## Perceiver\n",
    "\n",
    "https://wandb.ai/university-of-tartu-2/multi-sensor-dropout/runs/stu3mw4u/overview\n",
    "args.resume = 'checkpoint_epoch_31.pth'\n",
    "args.output_dir = \"../not_tracked_dir/output_perceiver_detection_2025-04-23_19-42-48/\"\n",
    "\n",
    "\n",
    "Raw mAP Results (Evaluator): {'map': tensor(0.9025), 'map_50': tensor(0.9688), 'map_75': tensor(0.9442), 'map_small': tensor(0.9025), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.7367), 'mar_10': tensor(0.9240), 'mar_100': tensor(0.9240), 'mar_small': tensor(0.9240), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "\n",
    "Raw Overlap mAP Results (Evaluator): {'map': tensor(0.6940), 'map_50': tensor(0.8853), 'map_75': tensor(0.7777), 'map_small': tensor(0.6940), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.6686), 'mar_10': tensor(0.7389), 'mar_100': tensor(0.7389), 'mar_small': tensor(0.7389), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "\n",
    "Raw Boundary mAP Results (Evaluator): {'map': tensor(0.8470), 'map_50': tensor(0.9552), 'map_75': tensor(0.9101), 'map_small': tensor(0.8470), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.8512), 'mar_10': tensor(0.8739), 'mar_100': tensor(0.8739), 'mar_small': tensor(0.8739), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "## YOLO\n",
    "Raw mAP Results (Evaluator): {'map': tensor(0.9209), 'map_50': tensor(0.9588), 'map_75': tensor(0.9367), 'map_small': tensor(0.9210), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.7747), 'mar_10': tensor(0.9375), 'mar_100': tensor(0.9375), 'mar_small': tensor(0.9375), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "\n",
    "Raw Overlap mAP Results (Evaluator): {'map': tensor(0.8149), 'map_50': tensor(0.9072), 'map_75': tensor(0.8693), 'map_small': tensor(0.8149), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.7976), 'mar_10': tensor(0.8430), 'mar_100': tensor(0.8430), 'mar_small': tensor(0.8430), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "\n",
    "Raw Boundary mAP Results (Evaluator): {'map': tensor(0.7145), 'map_50': tensor(0.7703), 'map_75': tensor(0.7448), 'map_small': tensor(0.7145), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.7244), 'mar_10': tensor(0.7425), 'mar_100': tensor(0.7425), 'mar_small': tensor(0.7425), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "\n",
    "Raw mAP Results: {'map': tensor(0.9209), 'map_50': tensor(0.9588), 'map_75': tensor(0.9367), 'map_small': tensor(0.9210), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.7747), 'mar_10': tensor(0.9375), 'mar_100': tensor(0.9375), 'mar_small': tensor(0.9375), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9535900e-5d53-4331-8a02-c0c165a22b57",
   "metadata": {},
   "source": [
    "## RESULT\n",
    "\n",
    "Raw mAP Results: {'map': tensor(0.7331), 'map_50': tensor(0.9379), 'map_75': tensor(0.8674), 'map_small': tensor(0.7331), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.6338), 'mar_10': tensor(0.7885), 'mar_100': tensor(0.7885), 'mar_small': tensor(0.7885), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "Evaluation Results:\n",
    "loss_running: 0.0000\n",
    "class_error_running: nan\n",
    "loss_center_point_running: nan\n",
    "loss_ce_running: 0.0000\n",
    "loss: 0.0000\n",
    "loss_ce_unscaled: 0.0000\n",
    "loss_bbox_unscaled: 0.0000\n",
    "loss_giou_unscaled: 0.0000\n",
    "loss_unscaled: 0.0000\n",
    "loss_ce: 0.0000\n",
    "mAP_map: 0.7331\n",
    "mAP_map_50: 0.9379\n",
    "mAP_map_75: 0.8674\n",
    "mAP_map_small: 0.7331\n",
    "mAP_map_medium: -1.0000\n",
    "mAP_map_large: -1.0000\n",
    "mAP_mar_1: 0.6338\n",
    "mAP_mar_10: 0.7885\n",
    "mAP_mar_100: 0.7885\n",
    "mAP_mar_small: 0.7885\n",
    "mAP_mar_medium: -1.0000\n",
    "mAP_mar_large: -1.0000\n",
    "mAP_map_per_class: -1.0000\n",
    "mAP_mar_100_per_class: -1.0000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b464e-fad7-4efd-bb4a-8316b48e3f58",
   "metadata": {},
   "source": [
    "## Result\n",
    "top 10 and threashold 0.5\n",
    "\n",
    "Raw mAP Results: {'map': tensor(0.6828), 'map_50': tensor(0.8640), 'map_75': tensor(0.8098), 'map_small': tensor(0.6828), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.5995), 'mar_10': tensor(0.7301), 'mar_100': tensor(0.7301), 'mar_small': tensor(0.7301), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "Evaluation Results:\n",
    "loss_running: 0.0000\n",
    "class_error_running: nan\n",
    "loss_center_point_running: nan\n",
    "loss_ce_running: 0.0000\n",
    "loss: 0.0000\n",
    "loss_ce_unscaled: 0.0000\n",
    "loss_bbox_unscaled: 0.0000\n",
    "loss_giou_unscaled: 0.0000\n",
    "loss_unscaled: 0.0000\n",
    "loss_ce: 0.0000\n",
    "mAP_map: 0.6828\n",
    "mAP_map_50: 0.8640\n",
    "mAP_map_75: 0.8098\n",
    "mAP_map_small: 0.6828\n",
    "mAP_map_medium: -1.0000\n",
    "mAP_map_large: -1.0000\n",
    "mAP_mar_1: 0.5995\n",
    "mAP_mar_10: 0.7301\n",
    "mAP_mar_100: 0.7301\n",
    "mAP_mar_small: 0.7301\n",
    "mAP_mar_medium: -1.0000\n",
    "mAP_mar_large: -1.0000\n",
    "mAP_map_per_class: -1.0000\n",
    "mAP_mar_100_per_class: -1.0000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f8dcf-b6a5-4423-8b07-4ab714e2bd8a",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e6760-ec98-49b8-8493-467a1c5ad097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils.plotting import plot_images\n",
    "from engine import evaluate\n",
    "from dataset import build_dataset\n",
    "import torch\n",
    "from engine import evaluate\n",
    "from dataset import build_dataset\n",
    "from models import build_model\n",
    "from models.perceiver import PostProcess\n",
    "import os\n",
    "from pathlib import Path\n",
    "from util.misc import collate_fn, is_main_process, get_sha, get_rank\n",
    "from torch.utils.data import DataLoader\n",
    "from ultralytics import YOLO\n",
    "import torchvision.transforms as T\n",
    "\n",
    "args.model = 'YOLO' # 'perceiver'\n",
    "\n",
    "if args.model == 'YOLO':\n",
    "\n",
    "    args.output_dir = \"../not_tracked_dir/output_yolo_v8_2025-04-11/\" # YOLO\n",
    "    args.resume = 'weights/best.pt'\n",
    "    args.resize_frame = 320\n",
    "    \n",
    "    checkpoint_path = os.path.join(args.output_dir, args.resume)\n",
    "    print(\"Loading YOLO model for RecurrentVideoObjectModule...\")\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"YOLO checkpoint (.pt file) not found at {checkpoint_path}\")\n",
    "\n",
    "    # Load the base YOLO detector\n",
    "    yolo_detector = YOLO(checkpoint_path)\n",
    "    print(f\"YOLO detector loaded from {checkpoint_path}\")\n",
    "\n",
    "    # Get number of classes from the loaded YOLO model\n",
    "    num_classes = len(yolo_detector.names)\n",
    "    print(f\"Detected {num_classes} classes from YOLO model.\")\n",
    "\n",
    "    # Instantiate the wrapper module\n",
    "    model = RecurrentVideoObjectModule(detector_module=yolo_detector, num_classes=num_classes)\n",
    "    print(\"RecurrentVideoObjectModule created with YOLO detector.\")\n",
    "\n",
    "    args.test_dataset_fraction = 0.01\n",
    "    \n",
    "elif args.model == 'perceiver' and os.path.exists(checkpoint_path):\n",
    "\n",
    "    # https://wandb.ai/university-of-tartu-2/multi-sensor-dropout/runs/stu3mw4u/overview\n",
    "    args.resume = 'checkpoint_epoch_31.pth'\n",
    "    args.output_dir = \"../not_tracked_dir/output_perceiver_detection_2025-04-23_19-42-48\"\n",
    "    args.resize_frame = 320\n",
    "    args.test_dataset_fraction = 0.01\n",
    "    checkpoint_path = os.path.join(args.output_dir, args.resume)\n",
    "    model = build_model(args, (0,0))\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=args.device)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "\n",
    "save_dir = os.path.join(args.output_dir, 'test_eval_best')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "dataset_test = build_dataset(split='test', args=args)\n",
    "\n",
    "if args.model == 'YOLO':\n",
    "    print(\"Clear norm transforms\")\n",
    "    dataset_test.norm_transforms = T.Compose([])\n",
    "\n",
    "sampler_test = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "dataloader_test = DataLoader(dataset_test, sampler=sampler_test, batch_size=args.batch_size,\n",
    "                                collate_fn=collate_fn, num_workers=args.num_workers, pin_memory=True)\n",
    "\n",
    "\n",
    "model.to(args.device)\n",
    "model.eval()\n",
    "\n",
    "def get_batch_by_index(dataloader, index):\n",
    "    iterator = iter(dataloader)\n",
    "    for i in range(index + 1):\n",
    "        try:\n",
    "            batch = next(iterator)\n",
    "            if i == index:\n",
    "                return batch\n",
    "        except StopIteration:\n",
    "            return None  # Index out of bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f27a4e2-48b3-411c-a2e8-50d6a4727500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from util import box_ops\n",
    "import numpy as np\n",
    "\n",
    "ni = 2\n",
    "\n",
    "samples=None\n",
    "\n",
    "if samples is None:\n",
    "    samples, targets = get_batch_by_index(dataloader_test, ni)\n",
    "    original_video, targets_ = dataloader_test.dataset.dataset[ni]\n",
    "\n",
    "out, targets_flat = model(samples, targets)\n",
    "\n",
    "out_logits, pred_boxes = out['pred_logits'], out['pred_boxes']\n",
    "batch_size = out_logits.shape[0]\n",
    "\n",
    "if args.model == 'perceiver':\n",
    "    prob = F.softmax(out_logits, -1)\n",
    "elif args.model == 'YOLO':\n",
    "    prob = out_logits # confs instead\n",
    "\n",
    "scores, indices = prob[..., :-1].max(-1)\n",
    "\n",
    "keep = scores > 0.1\n",
    "\n",
    "# Prepare lists to store filtered results across the batch\n",
    "all_boxes_pixels = []\n",
    "all_scores = []\n",
    "all_classes = []\n",
    "all_batch_idx = []\n",
    "paths = []\n",
    "\n",
    "for i in range(batch_size): # frames\n",
    "    keep_batch = keep[i] # Boolean mask for queries for this image\n",
    "    paths.append(f'{args.model}_vd_{ni}_frm_{i}')\n",
    "    if keep_batch.any():\n",
    "        # Get filtered scores, classes, and boxes for this image\n",
    "        batch_scores = scores[i][keep_batch]\n",
    "        batch_classes = indices[i][keep_batch]\n",
    "        batch_boxes_norm = pred_boxes[i][keep_batch] # Normalized cxcywh\n",
    "\n",
    "        # Convert boxes to pixel xyxy\n",
    "        # batch_boxes_pixels = rescale_bboxes(batch_boxes_norm, img_size_wh)\n",
    "        batch_boxes_pixels = batch_boxes_norm# box_ops.box_cxcywh_to_xyxy(batch_boxes_norm)\n",
    "        # batch_boxes_pixels = batch_boxes_pixels * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32, device=batch_boxes_norm.device)\n",
    "\n",
    "        # Store results\n",
    "        all_boxes_pixels.append(batch_boxes_pixels)\n",
    "        all_scores.append(batch_scores)\n",
    "        all_classes.append(batch_classes)\n",
    "        # Add batch index for each kept box\n",
    "        all_batch_idx.append(torch.full_like(batch_scores, fill_value=i, dtype=torch.long))\n",
    "\n",
    "# Concatenate results from all images into single tensors\n",
    "if all_batch_idx: # Check if any boxes were kept\n",
    "    final_pred_boxes = torch.cat(all_boxes_pixels, dim=0)\n",
    "    final_pred_scores = torch.cat(all_scores, dim=0)\n",
    "    final_pred_classes = torch.cat(all_classes, dim=0)\n",
    "    final_pred_batch_idx = torch.cat(all_batch_idx, dim=0)\n",
    "else:\n",
    "    # Handle case with no detections\n",
    "    final_pred_boxes = torch.empty((0, 4), device=samples.device)\n",
    "    final_pred_scores = torch.empty((0,), device=samples.device)\n",
    "    final_pred_classes = torch.empty((0,), dtype=torch.long, device=samples.device)\n",
    "    final_pred_batch_idx = torch.empty((0,), dtype=torch.long, device=samples.device)\n",
    "\n",
    "# --- 3. Prepare Images ---\n",
    "\n",
    "def denormalize_image(tensor, mean, std):\n",
    "    \"\"\"Denormalize a tensor image\"\"\"\n",
    "    # Clone to avoid modifying original tensor\n",
    "    tensor = tensor.clone()\n",
    "    mean = torch.tensor(mean, device=tensor.device).view(-1, 1, 1)\n",
    "    std = torch.tensor(std, device=tensor.device).view(-1, 1, 1)\n",
    "    tensor.mul_(std).add_(mean)\n",
    "    # Clamp and convert to uint8\n",
    "    tensor = torch.clamp(tensor * 255.0, min=0.0, max=255.0).to(torch.uint8)\n",
    "    return tensor\n",
    "\n",
    "mmnist_stat = {\n",
    "    'perceiver': (0.023958550628466375, 0.14140212075592035), # mean, std\n",
    "    'YOLO': (0, 1), # mean, std\n",
    "}\n",
    "\n",
    "# Denormalize and convert B C H W -> B H W C\n",
    "images_to_plot = []\n",
    "samples_flat = samples.squeeze(0) # B T C H W -> T C H W\n",
    "for i in range(samples_flat.shape[0]):\n",
    "    img = denormalize_image(samples_flat[i], mmnist_stat[args.model][0], mmnist_stat[args.model][1]) # B C H W -> C H W (uint8)\n",
    "    # img = img.permute(1, 2, 0) # C H W -> H W C\n",
    "    images_to_plot.append(img.cpu().numpy()) # plot_images expects numpy HWC uint8\n",
    "\n",
    "# # plot_images expects a single array/tensor if plotting multiple images from a batch\n",
    "images_to_plot_batch = np.stack(images_to_plot) # Create batch H W C\n",
    "\n",
    "\n",
    "# Plot Predictions\n",
    "plot_images(\n",
    "    images=images_to_plot_batch,\n",
    "    batch_idx=final_pred_batch_idx.cpu(),  # Index mapping box to image\n",
    "    cls=final_pred_classes.cpu(),          # Class index for each box\n",
    "    bboxes=final_pred_boxes.detach().cpu(),         # Boxes in pixel xyxy format\n",
    "    confs=final_pred_scores.cpu(),      # Optional: Include scores on plot\n",
    "    fname=os.path.join(save_dir, f\"{args.model}_val_batch{ni}_pred.jpg\"),\n",
    "    paths=paths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d02ea7-d4b3-4463-9799-51db437692d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensor_dropout",
   "language": "python",
   "name": "sensor_dropout"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
