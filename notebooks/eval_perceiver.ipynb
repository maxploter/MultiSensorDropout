{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13192b60-b5f1-443d-a672-f857e0541af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('/gpfs/helios/home/ploter/projects/MultiSensorDropout/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cac28c36-f0e5-4227-a054-72da44cd974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    # Basic training parameters\n",
    "    seed=42,\n",
    "    batch_size=1,\n",
    "    epochs=18,\n",
    "    learning_rate=1e-3,\n",
    "    learning_rate_backbone=1e-4,\n",
    "    learning_rate_backbone_names=[\"backbone\"],\n",
    "    weight_decay=0.01,\n",
    "    scheduler_step_size=12,\n",
    "    eval_interval=1,\n",
    "    patience=5,\n",
    "    model='perceiver',\n",
    "    backbone='cnn',  # Set specific value for evaluation\n",
    "    eval=True,\n",
    "    weight_loss_center_point=5,\n",
    "    weight_loss_bce=1,\n",
    "    shuffle_views=False,\n",
    "    object_detection=True,  # Set to True for object detection\n",
    "    resize_frame = None,\n",
    "    \n",
    "    # Matcher parameters\n",
    "    set_cost_class=1,\n",
    "    set_cost_bbox=5,\n",
    "    set_cost_giou=2,\n",
    "    focal_alpha=0.25,\n",
    "    focal_gamma=2,\n",
    "    \n",
    "    # Loss coefficients\n",
    "    bbox_loss_coef=5,\n",
    "    giou_loss_coef=2,\n",
    "    eos_coef=0.1,\n",
    "    \n",
    "    # Checkpoint parameters\n",
    "    resume='checkpoint_epoch_17.pth',  # Replace with actual checkpoint path\n",
    "    output_dir=\"./output\",  # Provide a default output directory\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # Dataset parameters\n",
    "    dataset='moving-mnist-medium',\n",
    "    dataset_path='Max-Ploter/detection-moving-mnist-medium',\n",
    "    generate_dataset_runtime=False,\n",
    "    num_workers=4,\n",
    "    num_frames=20,\n",
    "    train_dataset_fraction=1.0,\n",
    "    train_dataset_size=1.0,  # Additional parameter from notebook\n",
    "    test_dataset_fraction=1.0,\n",
    "    frame_dropout_pattern=None,\n",
    "    view_dropout_probs=[],#[0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85],\n",
    "    sampler_steps=[], #[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
    "    sequential_sampler=False,\n",
    "    grid_size=(1, 1),\n",
    "    tile_overlap=0.0,\n",
    "    \n",
    "    # Wandb parameters\n",
    "    wandb_project='multi-sensor-dropout',\n",
    "    wandb_id=None,\n",
    "    \n",
    "    # Perceiver model parameters\n",
    "    num_freq_bands=6,  # Set to 4 as in the notebook\n",
    "    max_freq=10, \n",
    "    enc_layers=1,\n",
    "    num_queries=16,  # Set to 16 as in the notebook\n",
    "    hidden_dim=128,\n",
    "    enc_nheads_cross=1,\n",
    "    nheads=1,\n",
    "    dropout=0.0,\n",
    "    self_per_cross_attn=1,\n",
    "    multi_classification_heads=False,\n",
    "    \n",
    "    # LSTM model parameters\n",
    "    lstm_hidden_size=128,\n",
    "    \n",
    "    # Additional parameters for complete compatibility\n",
    "    focal_loss=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2185130c-1eff-4867-8213-fefb17f6e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyCriterion:\n",
    "    def __init__(self, device):\n",
    "        # Initialize with the necessary attributes\n",
    "        self.device = device\n",
    "        self.weight_dict = {'loss_ce': 1}\n",
    "        \n",
    "    def __call__(self, outputs, targets, *args, **kwargs):\n",
    "        # Return a dictionary with zero losses to maintain the expected interface\n",
    "        return {\n",
    "            'loss_ce': torch.tensor(0.0, device=self.device),\n",
    "            'loss_bbox': torch.tensor(0.0, device=self.device),\n",
    "            'loss_giou': torch.tensor(0.0, device=self.device),\n",
    "            'loss': torch.tensor(0.0, device=self.device)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45f8ec69-480d-4b0c-b8ec-c263ea52cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class PostProcessTopK(nn.Module):\n",
    "    \"\"\" Wrapper that applies a post-processor and keeps only the top-k predictions by score and a score threshold \"\"\"\n",
    "\n",
    "    def __init__(self, post_processor, top_k=10, score_threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.post_processor = post_processor\n",
    "        self.top_k = top_k\n",
    "        self.score_threshold = score_threshold\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, target_sizes):\n",
    "        # Get results from the original post processor\n",
    "        results = self.post_processor(outputs, target_sizes)\n",
    "\n",
    "        # Filter to keep only top-k results and results above the score threshold\n",
    "        filtered_results = []\n",
    "        for result in results:\n",
    "            scores, labels, boxes = result['scores'], result['labels'], result['boxes']\n",
    "\n",
    "            # Get top-k indices by score\n",
    "            top_k = min(self.top_k, len(scores))\n",
    "            if top_k > 0:  # Check if there are any predictions\n",
    "                top_indices = torch.topk(scores, top_k).indices\n",
    "\n",
    "                # Filter by top-k indices\n",
    "                top_scores = scores[top_indices]\n",
    "                top_labels = labels[top_indices]\n",
    "                top_boxes = boxes[top_indices]\n",
    "\n",
    "                # Filter by score threshold\n",
    "                threshold_indices = top_scores >= self.score_threshold\n",
    "                final_scores = top_scores[threshold_indices]\n",
    "                final_labels = top_labels[threshold_indices]\n",
    "                final_boxes = top_boxes[threshold_indices]\n",
    "\n",
    "                filtered_results.append({'scores': final_scores, 'labels': final_labels, 'boxes': final_boxes})\n",
    "            else:\n",
    "                # Keep empty result if no predictions\n",
    "                filtered_results.append({'scores': torch.empty(0), 'labels': torch.empty(0, dtype=torch.int64), 'boxes': torch.empty(0, 4)})\n",
    "\n",
    "        return filtered_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a11936a-3054-47ad-9310-dda772cf7490",
   "metadata": {},
   "source": [
    "### Seq NMS post processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9bbf31d4-b1d2-44a3-b1c9-846264341b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # Required for softmax\n",
    "\n",
    "# Try to import seq_nms from the pt_seq_nms library\n",
    "try:\n",
    "    from pt_seq_nms import seq_nms\n",
    "except ImportError:\n",
    "    print(\"Warning: 'pt_seq_nms' library not found or 'seq_nms' function is missing.\")\n",
    "    print(\"SeqNMSEvaluationPostprocessor will not function correctly.\")\n",
    "    print(\"Ensure the library is installed: pip install git+https://github.com/MrParosk/seq_nms.git\")\n",
    "    seq_nms = None\n",
    "\n",
    "# Attempt to import box_ops, usually from a 'util' module in DETR-like projects\n",
    "try:\n",
    "    from util import box_ops # Assuming box_ops.box_cxcywh_to_xyxy is available\n",
    "except ImportError:\n",
    "    print(\"Warning: 'util.box_ops' not found. Box conversion utilities (box_cxcywh_to_xyxy) will be missing.\")\n",
    "    print(\"Ensure you have a 'util.box_ops' module with 'box_cxcywh_to_xyxy' or provide an alternative.\")\n",
    "    box_ops = None\n",
    "\n",
    "\n",
    "class SeqNMSEvaluationPostprocessor(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies Sequence Non-Maximum Suppression (Seq-NMS) using the `seq_nms` function\n",
    "    and incorporates the logic for converting raw model outputs to the final\n",
    "    detection format directly within this class.\n",
    "\n",
    "    The output per frame will retain NumQueries entries, with scores modified by Seq-NMS.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 min_score_threshold: float = 0.01, # Score threshold applied BEFORE Seq-NMS\n",
    "                 linkage_threshold: float = 0.5,\n",
    "                 iou_threshold: float = 0.5):\n",
    "        \"\"\"\n",
    "        Initializes the SeqNMSEvaluationPostprocessor.\n",
    "\n",
    "        Args:\n",
    "            min_score_threshold (float): Minimum confidence score for a detection's\n",
    "                                         initial score to be considered by `seq_nms`\n",
    "                                         (scores below this are set to 0 before `seq_nms`).\n",
    "                                         Defaults to 0.01.\n",
    "            linkage_threshold (float): The linkage threshold for Seq-NMS. Defaults to 0.5.\n",
    "            iou_threshold (float): The IoU threshold for Seq-NMS. Defaults to 0.5.\n",
    "\n",
    "        Raises:\n",
    "            ImportError: If the 'pt_seq_nms.seq_nms' or 'util.box_ops' cannot be imported.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if seq_nms is None:\n",
    "            raise ImportError(\"'pt_seq_nms.seq_nms' function is required but could not be imported.\")\n",
    "        if box_ops is None: # Ensure box_ops is available for coordinate conversions\n",
    "            raise ImportError(\"'util.box_ops' module with 'box_cxcywh_to_xyxy' is required for box conversion.\")\n",
    "\n",
    "        self.min_score_threshold = min_score_threshold\n",
    "        self.linkage_threshold = linkage_threshold\n",
    "        self.iou_threshold = iou_threshold\n",
    "        print(f\"Initialized SeqNMSEvaluationPostprocessor (Integrated PostProcessing, NumQueries output) with: \"\n",
    "              f\"min_score_thresh_pre_seqnms={min_score_threshold}, \"\n",
    "              f\"linkage_thresh={linkage_threshold}, iou_thresh={iou_threshold}\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs: dict, target_sizes: torch.Tensor) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Processes model output: prepares data for Seq-NMS by converting raw outputs,\n",
    "        applies Seq-NMS, and then formats the final list of detections, maintaining\n",
    "        NumQueries entries per frame.\n",
    "\n",
    "        Args:\n",
    "            outputs (dict): Raw output from the video object detector. Expected keys:\n",
    "                             - 'pred_logits': Tensor [T, NumQueries, NumClasses+1]\n",
    "                             - 'pred_boxes': Tensor [T, NumQueries, 4] (relative cxcywh)\n",
    "                             Where T is the number of frames.\n",
    "            target_sizes (torch.Tensor): Shape [T, 2], original (height, width) per frame.\n",
    "\n",
    "        Returns:\n",
    "            list[dict]: A list of T dictionaries (one per frame). Each dict contains:\n",
    "                        - 'scores': Tensor [NumQueries], scores after Seq-NMS (suppressed ~0).\n",
    "                        - 'labels': Tensor [NumQueries], original labels for each query.\n",
    "                        - 'boxes': Tensor [NumQueries, 4] (absolute xyxy) for each query.\n",
    "        \"\"\"\n",
    "        pred_logits_seq = outputs.get('pred_logits')\n",
    "        pred_boxes_seq = outputs.get('pred_boxes') # Relative cxcywh\n",
    "\n",
    "        # --- Input Validation ---\n",
    "        if pred_logits_seq is None or pred_boxes_seq is None:\n",
    "            raise KeyError(\"Input 'outputs' must contain 'pred_logits' and 'pred_boxes'.\")\n",
    "        if not (isinstance(pred_logits_seq, torch.Tensor) and isinstance(pred_boxes_seq, torch.Tensor)):\n",
    "             raise TypeError(\"'pred_logits' and 'pred_boxes' must be torch tensors.\")\n",
    "        if pred_logits_seq.dim() != 3 or pred_boxes_seq.dim() != 3:\n",
    "             raise ValueError(f\"Expected 3D tensors for logits and boxes [T, NumQueries, Dim], \"\n",
    "                              f\"got {pred_logits_seq.shape} and {pred_boxes_seq.shape}\")\n",
    "        if pred_logits_seq.shape[0:2] != pred_boxes_seq.shape[0:2]:\n",
    "             raise ValueError(f\"Shape mismatch (T, NumQueries) between 'pred_logits' {pred_logits_seq.shape} \"\n",
    "                              f\"and 'pred_boxes' {pred_boxes_seq.shape}\")\n",
    "\n",
    "        num_frames = pred_logits_seq.shape[0]\n",
    "        num_queries = pred_logits_seq.shape[1]\n",
    "        device = pred_logits_seq.device\n",
    "        \n",
    "        # Print the number of initial predictions\n",
    "        # print(f\"SeqNMSEvaluationPostprocessor: Received {num_frames} frames, each with {num_queries} initial prediction slots. Total slots: {num_frames * num_queries}\")\n",
    "\n",
    "        if not isinstance(target_sizes, torch.Tensor):\n",
    "             raise TypeError(\"'target_sizes' must be a torch tensor.\")\n",
    "        if target_sizes.shape != (num_frames, 2):\n",
    "            raise ValueError(f\"Expected 'target_sizes' shape [{num_frames}, 2], got {target_sizes.shape}\")\n",
    "\n",
    "        # --- 1. Prepare inputs for Seq-NMS (absolute boxes, initial scores, labels) ---\n",
    "        all_abs_boxes_for_seq_nms = []\n",
    "        all_masked_scores_for_seq_nms = []\n",
    "        all_labels_for_seq_nms = [] # Stores original labels corresponding to scores/boxes\n",
    "\n",
    "        for t in range(num_frames):\n",
    "            logits_t = pred_logits_seq[t]  # [NumQueries, NumClasses+1]\n",
    "            boxes_rel_t = pred_boxes_seq[t] # [NumQueries, 4] (cxcywh)\n",
    "            img_h, img_w = target_sizes[t]\n",
    "\n",
    "            # Convert relative boxes to absolute xyxy for seq_nms\n",
    "            abs_boxes_t = box_ops.box_cxcywh_to_xyxy(boxes_rel_t)\n",
    "            scale_fct = torch.tensor([img_w, img_h, img_w, img_h], device=device, dtype=torch.float)\n",
    "            abs_boxes_t = abs_boxes_t * scale_fct\n",
    "\n",
    "            # Get initial scores and labels from logits (mimicking PostProcess)\n",
    "            prob_t = F.softmax(logits_t, -1)\n",
    "            scores_t, labels_t = prob_t[..., :-1].max(-1) # Max over foreground classes\n",
    "\n",
    "            # Apply min_score_threshold to scores before passing to seq_nms\n",
    "            scores_masked_t = scores_t.clone()\n",
    "            scores_masked_t[scores_t < self.min_score_threshold] = 0.0\n",
    "\n",
    "            all_abs_boxes_for_seq_nms.append(abs_boxes_t)\n",
    "            all_masked_scores_for_seq_nms.append(scores_masked_t)\n",
    "            all_labels_for_seq_nms.append(labels_t) # Store original labels\n",
    "\n",
    "        if not all_abs_boxes_for_seq_nms:\n",
    "            return [] # Should only happen if num_frames is 0\n",
    "\n",
    "        stacked_abs_boxes = torch.stack(all_abs_boxes_for_seq_nms)\n",
    "        stacked_masked_scores = torch.stack(all_masked_scores_for_seq_nms)\n",
    "        stacked_original_labels = torch.stack(all_labels_for_seq_nms)\n",
    "\n",
    "\n",
    "        # --- 2. Apply Seq-NMS ---\n",
    "        # updated_scores_from_seq_nms will have shape [T, NumQueries]\n",
    "        # Scores for suppressed boxes are expected to be 0 or very low.\n",
    "        try:\n",
    "            updated_scores_from_seq_nms = seq_nms(\n",
    "                boxes=stacked_abs_boxes,\n",
    "                scores=stacked_masked_scores,\n",
    "                classes=stacked_original_labels.to(torch.int32), # CAST TO INT32\n",
    "                linkage_threshold=self.linkage_threshold,\n",
    "                iou_threshold=self.iou_threshold\n",
    "            )\n",
    "        except Exception as e:\n",
    "             print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "             print(f\"Error occurred during 'seq_nms' execution: {e}\")\n",
    "             print(f\"Inputs shapes to seq_nms:\")\n",
    "             print(f\"  stacked_abs_boxes: {stacked_abs_boxes.shape}\")\n",
    "             print(f\"  stacked_masked_scores: {stacked_masked_scores.shape}\")\n",
    "             print(f\"  stacked_original_labels: {stacked_original_labels.shape} (dtype before cast: {stacked_original_labels.dtype})\")\n",
    "             print(f\"Proceeding by using pre-SeqNMS scores (Seq-NMS is effectively bypassed).\")\n",
    "             print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "             # Fallback: use scores that went into seq_nms if seq_nms fails\n",
    "             updated_scores_from_seq_nms = stacked_masked_scores\n",
    "\n",
    "        # --- 3. Format Final Results, maintaining NumQueries dimension ---\n",
    "        final_results_list = []\n",
    "        for t in range(num_frames):\n",
    "            # These are the scores after Seq-NMS (or fallback), shape [NumQueries]\n",
    "            final_scores_t = updated_scores_from_seq_nms[t]\n",
    "\n",
    "            # Corresponding original labels and absolute boxes for all NumQueries slots\n",
    "            final_labels_t = stacked_original_labels[t] # Shape [NumQueries]\n",
    "            final_boxes_t = stacked_abs_boxes[t]        # Shape [NumQueries, 4]\n",
    "\n",
    "            final_results_list.append({\n",
    "                'scores': final_scores_t,\n",
    "                'labels': final_labels_t,\n",
    "                'boxes': final_boxes_t\n",
    "            })\n",
    "\n",
    "        return final_results_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5143de3f-149c-43fb-908c-312d6dac58cb",
   "metadata": {},
   "source": [
    "## Recurrent module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f6ceff5c-17f6-4a87-abd9-fb9a737c1d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # Needed for padding\n",
    "\n",
    "class RecurrentVideoObjectModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Refactored module using a YOLO detector, producing output compatible\n",
    "    with the original implementation's format {'pred_logits': Tensor, 'pred_boxes': Tensor}.\n",
    "    Detections from each frame are padded to a maximum number before stacking.\n",
    "    Includes fix for grayscale input.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 detector_module: nn.Module, # E.g., a loaded YOLO model\n",
    "                 num_classes: int = 10, # Needed for converting scores/classes to logits\n",
    "\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.detector_module = detector_module\n",
    "        self.num_classes = num_classes + 1\n",
    "        # Store the number of foreground classes if needed elsewhere\n",
    "        self.num_foreground_classes = num_classes\n",
    "\n",
    "    @torch.no_grad() # Crucial for evaluation/inference mode\n",
    "    def forward(self, samples, targets: list = None):\n",
    "        \"\"\"\n",
    "        Processes video frames sequentially, collects raw detections, pads them\n",
    "        to the maximum number of detections found in any frame, and stacks them.\n",
    "        Handles potential grayscale input.\n",
    "\n",
    "        Args:\n",
    "            samples (torch.Tensor): Input video tensor (B, T, C, H, W).\n",
    "                                    Module assumes B=1. Can handle C=1 or C=3.\n",
    "            targets (list): List containing target dictionaries for the video.\n",
    "                            Passed through unmodified.\n",
    "\n",
    "        Returns:\n",
    "            tuple[dict, list]:\n",
    "                - dict: {'pred_logits': Tensor [T, MaxDetsPerFrame, NumClasses],\n",
    "                         'pred_boxes': Tensor [T, MaxDetsPerFrame, 4]}\n",
    "                         Padded and stacked detections across all frames.\n",
    "                - list: The original targets list (passed through).\n",
    "        \"\"\"\n",
    "        if samples.dim() != 5:\n",
    "             raise ValueError(f\"Expected input tensor with 5 dimensions (B, T, C, H, W), but got {samples.dim()}\")\n",
    "\n",
    "        if samples.shape[0] != 1:\n",
    "            print(f\"Warning: RecurrentVideoObjectModule expects batch size 1, but got {samples.shape[0]}. Processing only the first item.\")\n",
    "            samples = samples[:1]\n",
    "\n",
    "        # samples shape: (B, T, C, H, W), B=1 assumed\n",
    "        num_frames = samples.shape[1]\n",
    "        src = samples.permute(1, 0, 2, 3, 4).squeeze(1) # Shape: (T, C, H, W)\n",
    "        device = samples.device\n",
    "        input_channels = src.shape[1] # Get the number of channels C\n",
    "\n",
    "        # Check if input is grayscale (C=1) or color (C=3)\n",
    "        if input_channels not in [1, 3]:\n",
    "            raise ValueError(f\"Expected input frames to have 1 or 3 channels, but got {input_channels}\")\n",
    "\n",
    "        logits_accumulator = []\n",
    "        boxes_accumulator = []\n",
    "        max_detections = 0 # Keep track of the maximum number of detections in any frame\n",
    "\n",
    "        if isinstance(self.detector_module, nn.Module):\n",
    "              self.detector_module.eval()\n",
    "\n",
    "        for timestamp, frame in enumerate(src): # Iterate through frames (C, H, W)\n",
    "\n",
    "            # --- Ensure 3 Channels for YOLO ---\n",
    "            if input_channels == 1:\n",
    "                # Repeat the grayscale channel 3 times\n",
    "                frame = frame.repeat(3, 1, 1) # Shape becomes (3, H, W)\n",
    "\n",
    "            frame_batch = frame.unsqueeze(0) # Add batch dimension (1, 3, H, W)\n",
    "\n",
    "            # --- Run Detector (YOLO) ---\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    # Use model.predict for easier handling of results and NMS\n",
    "                    # Adjust conf/iou as needed for your specific detector/task\n",
    "                    detector_preds = self.detector_module.predict(frame_batch, conf=0.001, iou=0.8, device=device, verbose=False)\n",
    "                    # 'preds' is typically a list of Results objects, one per image\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during detector_module.predict on frame {timestamp}: {e}\")\n",
    "                print(\"Skipping frame due to prediction error.\")\n",
    "                # Decide how to handle: continue, raise, or append empty tensors?\n",
    "                # Appending empty tensors might be safer for padding later.\n",
    "                # logits_accumulator.append(torch.empty((0, self.num_classes), device=device))\n",
    "                # boxes_accumulator.append(torch.empty((0, 4), device=device))\n",
    "                raise e\n",
    "\n",
    "            # --- Process Detections ---\n",
    "            results_obj = detector_preds[0] # Assuming batch size 1 for predict\n",
    "\n",
    "\n",
    "            # Use normalized xywh format directly if available\n",
    "            frame_boxes_cxcywh = results_obj.boxes.xywhn.to(device)\n",
    "\n",
    "            # print(f\"frame_boxes_cxcywh: {frame_boxes_cxcywh.shape}\")\n",
    "            \n",
    "            frame_scores = results_obj.boxes.conf.to(device)\n",
    "\n",
    "            # print(f\"frame_scores: {frame_scores.shape}\")\n",
    "            # print(frame_scores)\n",
    "            \n",
    "            frame_classes = results_obj.boxes.cls.to(device).long()\n",
    "\n",
    "            # print(f\"frame_classes: {frame_classes.shape}\")\n",
    "\n",
    "            num_detections = frame_classes.shape[0]\n",
    "\n",
    "            # print(f\"num_detections: {num_detections}\")\n",
    "\n",
    "            # --- Convert scores/classes to approximate logits ---\n",
    "            frame_logits = torch.zeros((num_detections, self.num_classes), device=device)\n",
    "            \n",
    "            # Place the score logits at the correct class index\n",
    "            frame_logits[torch.arange(num_detections, device=device), frame_classes] = frame_scores\n",
    "            \n",
    "            logits_accumulator.append(frame_logits)\n",
    "            boxes_accumulator.append(frame_boxes_cxcywh)\n",
    "\n",
    "            # Update max detections found so far\n",
    "            if num_detections > max_detections:\n",
    "                max_detections = num_detections\n",
    "\n",
    "        # --- Pad tensors to max_detections ---\n",
    "        padded_logits = []\n",
    "        padded_boxes = []\n",
    "        pad_val_boxes = 0.0\n",
    "        background_class_index = self.num_classes - 1 # Last index is background\n",
    "        for frame_logits, frame_boxes in zip(logits_accumulator, boxes_accumulator):\n",
    "            num_dets = frame_logits.shape[0]\n",
    "            pad_size = max_detections - num_dets\n",
    "\n",
    "            if pad_size > 0:\n",
    "                # --- Pad logits ---\n",
    "                # Create padding with 0.0 everywhere first\n",
    "                logit_padding = torch.zeros((pad_size, self.num_classes), device=device, dtype=frame_logits.dtype)\n",
    "                # Set the background class logit to +inf for padded entries\n",
    "                logit_padding[:, background_class_index] = float('inf')\n",
    "                # Concatenate original logits with the padding\n",
    "                padded_frame_logits = torch.cat((frame_logits, logit_padding), dim=0)\n",
    "\n",
    "                # --- Pad boxes ---\n",
    "                # Pad boxes with 0.0\n",
    "                box_padding = torch.full((pad_size, 4), pad_val_boxes, device=device, dtype=frame_boxes.dtype)\n",
    "                # Concatenate original boxes with the padding\n",
    "                padded_frame_boxes = torch.cat((frame_boxes, box_padding), dim=0)\n",
    "            else:\n",
    "                # No padding needed if already max_detections\n",
    "                padded_frame_logits = frame_logits\n",
    "                padded_frame_boxes = frame_boxes\n",
    "\n",
    "            padded_logits.append(padded_frame_logits)\n",
    "            padded_boxes.append(padded_frame_boxes)\n",
    "\n",
    "\n",
    "        final_logits = torch.stack(padded_logits) # Shape: [T, max_detections, num_classes]\n",
    "        final_boxes = torch.stack(padded_boxes)   # Shape: [T, max_detections, 4]\n",
    "\n",
    "        result = {\n",
    "            'pred_logits': final_logits,\n",
    "            'pred_boxes': final_boxes\n",
    "        }\n",
    "\n",
    "        return result, targets[0]\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"Sets the detector module to evaluation mode.\"\"\"\n",
    "        if isinstance(self.detector_module, nn.Module):\n",
    "            self.detector_module.eval()\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6075d841-970f-40ae-b52d-033347d01a6f",
   "metadata": {},
   "source": [
    "## MAP Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3df20ff-2fea-4e25-a9e3-626f746a4be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "# Assuming box_ops contains the necessary bounding box utility functions like box_cxcywh_to_xyxy\n",
    "# Import the actual box_ops module\n",
    "from util import box_ops\n",
    "# Assuming PostProcess is importable, e.g.:\n",
    "# from models.perceiver import PostProcess # Adjust import path as needed\n",
    "# Assuming box_iou is available from torchvision.ops\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "\n",
    "class MeanAveragePrecisionEvaluator:\n",
    "    \"\"\"\n",
    "    Computes Mean Average Precision (mAP) for object detection tasks using a provided postprocessor.\n",
    "    Also computes separate mAP scores for:\n",
    "    1. Predictions matched to overlapping ground truth objects.\n",
    "    2. Predictions matched to ground truth objects near the frame boundary.\n",
    "\n",
    "    Processes model outputs assuming shape [sequence_length, num_queries, ...] and targets_flat\n",
    "    as a list of dictionaries of length sequence_length.\n",
    "\n",
    "    Standard mAP is updated per timestamp. Overlap and Boundary mAP use results\n",
    "    from a single matcher call performed once per sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, device, postprocessor, matcher, box_format='xyxy', iou_thresholds=None, overlap_iou_threshold=0.01, boundary_pixel_tolerance=0):\n",
    "        \"\"\"\n",
    "        Initializes the mAP evaluator.\n",
    "\n",
    "        Parameters:\n",
    "        device (torch.device): The device to run computations on (e.g., 'cuda', 'cpu').\n",
    "        postprocessor (torch.nn.Module): Postprocessing module (e.g., PostProcess).\n",
    "        matcher (torch.nn.Module): Matcher instance (e.g., HungarianMatcher). Assumed to handle\n",
    "                                   sequence dim as batch dim or be adapted accordingly.\n",
    "        box_format (str): Box format for torchmetrics ('xyxy').\n",
    "        iou_thresholds (list, optional): IoU thresholds for mAP calculation.\n",
    "        overlap_iou_threshold (float): Min IoU between two GT boxes to be considered overlapping.\n",
    "        boundary_pixel_tolerance (int): Pixel distance from edge to consider a GT box as 'on boundary'.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.postprocessor = postprocessor.to(self.device)\n",
    "        self.matcher = matcher # Store matcher instance\n",
    "        self.overlap_iou_threshold = overlap_iou_threshold\n",
    "        self.boundary_pixel_tolerance = boundary_pixel_tolerance\n",
    "\n",
    "        # --- Initialize Metrics ---\n",
    "        self.map_metric = torchmetrics.detection.MeanAveragePrecision(\n",
    "            box_format='xyxy', iou_type='bbox', iou_thresholds=iou_thresholds\n",
    "        ).to(self.device)\n",
    "        self.map_results = None\n",
    "\n",
    "        self.map_metric_overlap = torchmetrics.detection.MeanAveragePrecision(\n",
    "            box_format='xyxy', iou_type='bbox', iou_thresholds=iou_thresholds\n",
    "        ).to(self.device)\n",
    "        self.map_results_overlap = None\n",
    "\n",
    "        self.map_metric_boundary = torchmetrics.detection.MeanAveragePrecision(\n",
    "            box_format='xyxy', iou_type='bbox', iou_thresholds=iou_thresholds\n",
    "        ).to(self.device)\n",
    "        self.map_results_boundary = None\n",
    "\n",
    "        # --- Initialize GT Counts ---\n",
    "        self.count_total_gt = 0\n",
    "        self.count_overlap_gt = 0\n",
    "        self.count_boundary_gt = 0\n",
    "\n",
    "\n",
    "    # Helper function provided by user (might need adjustment based on actual matcher output format)\n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "      # This helper might not be directly usable if indices is now a list over sequence length\n",
    "      # The logic using matcher results below accesses indices[t] directly.\n",
    "      print(\"Warning: _get_src_permutation_idx might not be suitable for the new matcher output format.\")\n",
    "      # Placeholder return if called unexpectedly\n",
    "      return torch.tensor([], dtype=torch.long), torch.tensor([], dtype=torch.long)\n",
    "\n",
    "\n",
    "    def update(self, outputs, targets_flat):\n",
    "        \"\"\"\n",
    "        Updates the evaluator state. Standard mAP is updated per timestamp.\n",
    "        Matcher runs once per sequence. Overlap/Boundary metrics are updated\n",
    "        per timestamp using the single matcher result.\n",
    "\n",
    "        Parameters:\n",
    "        outputs (dict): Raw outputs. Keys: 'pred_logits', 'pred_boxes'. Shape [seq_len, num_queries, ...].\n",
    "        targets_flat (list[dict]): GT dicts. Len=seq_len. Keys: 'orig_size', 'boxes', 'labels'.\n",
    "        \"\"\"\n",
    "        # --- Input Validation ---\n",
    "        if not isinstance(targets_flat, list): return\n",
    "        if 'pred_logits' not in outputs or 'pred_boxes' not in outputs: return\n",
    "\n",
    "        pred_logits_seq = outputs['pred_logits'].to(self.device) # Shape [seq_len, num_queries, ...]\n",
    "        pred_boxes_seq = outputs['pred_boxes'].to(self.device)   # Shape [seq_len, num_queries, ...]\n",
    "        seq_len = pred_logits_seq.shape[0]\n",
    "\n",
    "        if pred_logits_seq.dim() < 2 or seq_len != len(targets_flat):\n",
    "             print(f\"Error: Mismatch/Shape issue between outputs ({pred_logits_seq.shape}) and targets_flat ({len(targets_flat)}). Skipping.\")\n",
    "             return\n",
    "\n",
    "        # --- Store per-timestamp processed data ---\n",
    "        all_postprocessed_preds = []\n",
    "        all_processed_targets = []\n",
    "        all_orig_sizes = [] # Store original sizes (tensor[2]) for boundary check\n",
    "\n",
    "        # --- First Pass: Update Standard mAP & Prepare Data ---\n",
    "        for t in range(seq_len):\n",
    "            target_dict = targets_flat[t]\n",
    "            if not isinstance(target_dict, dict) or not all(k in target_dict for k in ['orig_size', 'boxes', 'labels']):\n",
    "                 print(f\"Error: Invalid target dict at timestamp {t}. Skipping timestamp.\")\n",
    "                 all_postprocessed_preds.append(None)\n",
    "                 all_processed_targets.append(None)\n",
    "                 all_orig_sizes.append(None)\n",
    "                 continue\n",
    "\n",
    "            # Prepare Predictions (Post-processed)\n",
    "            orig_size_tensor_t = None\n",
    "            prediction_dict_postprocessed_t = None\n",
    "            try:\n",
    "                orig_size_tensor_t = target_dict['orig_size'].to(self.device)\n",
    "                if orig_size_tensor_t.shape != (2,): raise ValueError(\"Incorrect orig_size shape\")\n",
    "                orig_target_size_tensor_batch = orig_size_tensor_t.unsqueeze(0) # Shape [1, 2]\n",
    "\n",
    "                outputs_raw_t_batch = { # Need batch dim for postprocessor\n",
    "                    'pred_logits': pred_logits_seq[t].unsqueeze(0),\n",
    "                    'pred_boxes': pred_boxes_seq[t].unsqueeze(0)\n",
    "                }\n",
    "                with torch.no_grad():\n",
    "                    predictions_postprocessed_t_list = self.postprocessor(outputs_raw_t_batch, orig_target_size_tensor_batch)\n",
    "                if len(predictions_postprocessed_t_list) != 1: raise RuntimeError(\"Postprocessor failed\")\n",
    "                prediction_dict_postprocessed_t = predictions_postprocessed_t_list[0]\n",
    "                all_postprocessed_preds.append(prediction_dict_postprocessed_t)\n",
    "                all_orig_sizes.append(orig_size_tensor_t) # Store shape [2] tensor\n",
    "\n",
    "            except Exception as e:\n",
    "                 print(f\"Error processing predictions for timestamp {t}: {e}. Skipping.\")\n",
    "                 all_postprocessed_preds.append(None)\n",
    "                 all_processed_targets.append(None)\n",
    "                 all_orig_sizes.append(None)\n",
    "                 continue\n",
    "\n",
    "            # Prepare Targets (Absolute xyxy)\n",
    "            target_dict_processed_t = None\n",
    "            try:\n",
    "                target_dict_device = {k: v.to(self.device) for k, v in target_dict.items() if isinstance(v, torch.Tensor)}\n",
    "                img_h_target_tensor = orig_size_tensor_t[0]\n",
    "                img_w_target_tensor = orig_size_tensor_t[1]\n",
    "                scale_fct_target = torch.stack([img_w_target_tensor, img_h_target_tensor, img_w_target_tensor, img_h_target_tensor], dim=0)\n",
    "\n",
    "                boxes_target = target_dict_device['boxes'] # Assumed relative cxcywh\n",
    "                labels_target = target_dict_device['labels'] # Original labels\n",
    "\n",
    "                gt_boxes_xyxy = torch.empty((0,4), device=self.device) # Default empty boxes\n",
    "                labels_final = torch.empty(0, dtype=torch.long, device=self.device) # Default empty labels\n",
    "\n",
    "                if boxes_target.numel() > 0:\n",
    "                     # Only process boxes and use original labels if boxes exist\n",
    "                     gt_boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes_target) * scale_fct_target\n",
    "                     # Ensure labels correspond to the boxes processed\n",
    "                     if labels_target.shape[0] == boxes_target.shape[0]:\n",
    "                          labels_final = labels_target\n",
    "                     else:\n",
    "                          print(f\"Warning: Mismatch between number of boxes ({boxes_target.shape[0]}) and labels ({labels_target.shape[0]}) at timestamp {t}. Using empty labels.\")\n",
    "                          # Keep labels_final empty if mismatch occurs\n",
    "                # else: gt_boxes_xyxy and labels_final remain empty\n",
    "\n",
    "                target_dict_processed_t = {\n",
    "                    'boxes': gt_boxes_xyxy, # Shape [N, 4] or [0, 4]\n",
    "                    'labels': labels_final   # Shape [N] or [0]\n",
    "                }\n",
    "                all_processed_targets.append(target_dict_processed_t)\n",
    "\n",
    "                # --- Update Total GT Count ---\n",
    "                if gt_boxes_xyxy.numel() > 0: # Count only if there are GT boxes\n",
    "                    self.count_total_gt += gt_boxes_xyxy.shape[0]\n",
    "            except Exception as e:\n",
    "                 print(f\"Error preparing targets for timestamp {t}: {e}. Skipping.\")\n",
    "                 all_processed_targets.append(None)\n",
    "                 if len(all_postprocessed_preds) > len(all_processed_targets):\n",
    "                      all_postprocessed_preds.pop()\n",
    "                      all_orig_sizes.pop()\n",
    "                 continue\n",
    "\n",
    "            # Update Standard mAP Metric for this timestamp\n",
    "            # Ensure both prediction and target dicts are valid before updating\n",
    "            if prediction_dict_postprocessed_t is not None and target_dict_processed_t is not None:\n",
    "                try:\n",
    "                    self.map_metric.update([prediction_dict_postprocessed_t], [target_dict_processed_t])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error updating standard mAP metric for timestamp {t}: {e}\")\n",
    "            else:\n",
    "                 print(f\"Skipping standard mAP update for timestamp {t} due to previous errors.\")\n",
    "\n",
    "\n",
    "        # --- Run Matcher ONCE for the whole sequence ---\n",
    "        indices_seq = None # Will store list of tuples: [(pred_idx_t, gt_idx_t)] * seq_len\n",
    "        try:\n",
    "            # Prepare inputs for matcher (treating sequence as batch)\n",
    "            outputs_raw_seq = {'pred_logits': pred_logits_seq, 'pred_boxes': pred_boxes_seq}\n",
    "            # Ensure targets are on device and have required keys\n",
    "            targets_flat_device = []\n",
    "            valid_targets_exist = False\n",
    "            for t_idx, td in enumerate(targets_flat):\n",
    "                 # Use the already processed targets if available and valid\n",
    "                 processed_target = all_processed_targets[t_idx]\n",
    "                 if processed_target is not None and isinstance(processed_target.get('boxes'), torch.Tensor) and isinstance(processed_target.get('labels'), torch.Tensor):\n",
    "                      # Matcher might expect original format, re-prepare if necessary\n",
    "                      # For simplicity, let's assume matcher works with processed targets (abs xyxy)\n",
    "                      # If matcher needs relative cxcywh, re-fetch from targets_flat\n",
    "                      original_target = targets_flat[t_idx]\n",
    "                      if isinstance(original_target, dict) and all(k in original_target for k in ['boxes', 'labels']):\n",
    "                           targets_flat_device.append({k: v.to(self.device) for k, v in original_target.items() if k in ['boxes', 'labels']})\n",
    "                           valid_targets_exist = True\n",
    "                      else:\n",
    "                           # Add placeholder if original target was invalid\n",
    "                           targets_flat_device.append({'boxes': torch.empty((0,4), device=self.device), 'labels': torch.empty(0, dtype=torch.long, device=self.device)})\n",
    "\n",
    "                 else:\n",
    "                      # Add placeholder if processing failed\n",
    "                      targets_flat_device.append({'boxes': torch.empty((0,4), device=self.device), 'labels': torch.empty(0, dtype=torch.long, device=self.device)})\n",
    "\n",
    "\n",
    "            if valid_targets_exist: # Only run matcher if there's something to match\n",
    "                 with torch.no_grad():\n",
    "                      # Assumes matcher takes [Seq, N, C] outputs and List[Dict] targets (len=Seq)\n",
    "                      indices_seq = self.matcher(outputs_raw_seq, targets_flat_device)\n",
    "            else:\n",
    "                 print(\"Skipping matcher run as no valid targets were found in the sequence.\")\n",
    "\n",
    "        except Exception as e:\n",
    "             print(f\"Error running matcher for the sequence: {e}\")\n",
    "             indices_seq = None # Ensure indices_seq is None if matcher fails\n",
    "\n",
    "        # --- Second Pass: Update Overlap and Boundary Metrics ---\n",
    "        if indices_seq is not None and len(indices_seq) == seq_len: # Check if matcher ran successfully and returned expected length\n",
    "            for t in range(seq_len):\n",
    "                # Retrieve pre-processed data for timestamp t\n",
    "                prediction_dict_postprocessed_t = all_postprocessed_preds[t]\n",
    "                target_dict_processed_t = all_processed_targets[t]\n",
    "                orig_size_tensor_t = all_orig_sizes[t]\n",
    "\n",
    "                # Skip if data preparation failed in the first pass for this timestamp\n",
    "                if prediction_dict_postprocessed_t is None or target_dict_processed_t is None or orig_size_tensor_t is None:\n",
    "                    continue\n",
    "\n",
    "                # Get matcher results for this specific timestamp\n",
    "                # Add check for empty matcher results for this timestamp\n",
    "                if t >= len(indices_seq) or not indices_seq[t] or len(indices_seq[t]) != 2:\n",
    "                     # print(f\"Matcher results missing or invalid for timestamp {t}. Skipping overlap/boundary.\")\n",
    "                     continue # Skip if no valid match for this frame\n",
    "                #matched_pred_indices_t, matched_gt_indices_t = indices_seq[t]\n",
    "                matched_pred_indices_t = indices_seq[t][0].to(self.device)\n",
    "                matched_gt_indices_t = indices_seq[t][1].to(self.device)\n",
    "\n",
    "                gt_boxes_xyxy = target_dict_processed_t['boxes']\n",
    "                num_gt = gt_boxes_xyxy.shape[0]\n",
    "\n",
    "                # --- Overlap Calculation ---\n",
    "                if num_gt > 1: # Overlap requires at least 2 GT boxes\n",
    "                    try:\n",
    "                        gt_iou_matrix = box_iou(gt_boxes_xyxy, gt_boxes_xyxy)\n",
    "                        gt_iou_matrix.fill_diagonal_(0)\n",
    "                        overlaps_exist = (gt_iou_matrix > self.overlap_iou_threshold).any(dim=1)\n",
    "                        overlap_gt_indices = torch.where(overlaps_exist)[0].to(self.device)\n",
    "\n",
    "                        if overlap_gt_indices.numel() > 0:\n",
    "                            self.count_overlap_gt += overlap_gt_indices.numel() # Add count of GTs involved in overlap\n",
    "                            targets_overlap = {\n",
    "                                'boxes': gt_boxes_xyxy[overlap_gt_indices],\n",
    "                                'labels': target_dict_processed_t['labels'][overlap_gt_indices]\n",
    "                            }\n",
    "                            if matched_pred_indices_t.numel() > 0: # Check if matcher found matches for this frame\n",
    "                                is_overlap_match = torch.isin(matched_gt_indices_t, overlap_gt_indices)\n",
    "                                pred_indices_for_overlap = matched_pred_indices_t[is_overlap_match]\n",
    "                                if pred_indices_for_overlap.numel() > 0:\n",
    "                                    predictions_overlap = {\n",
    "                                        'scores': prediction_dict_postprocessed_t['scores'][pred_indices_for_overlap],\n",
    "                                        'labels': prediction_dict_postprocessed_t['labels'][pred_indices_for_overlap],\n",
    "                                        'boxes': prediction_dict_postprocessed_t['boxes'][pred_indices_for_overlap],\n",
    "                                    }\n",
    "                                    self.map_metric_overlap.update([predictions_overlap], [targets_overlap])\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error calculating/updating overlap mAP for timestamp {t}: {e}\")\n",
    "\n",
    "                # --- Boundary Calculation ---\n",
    "                if num_gt > 0: # Boundary check needs at least 1 GT box\n",
    "                     try:\n",
    "                        img_h_target_tensor = orig_size_tensor_t[0]\n",
    "                        img_w_target_tensor = orig_size_tensor_t[1]\n",
    "                        xmin, ymin, xmax, ymax = gt_boxes_xyxy.unbind(-1)\n",
    "                        tol = self.boundary_pixel_tolerance\n",
    "                        is_on_boundary = (xmin <= tol) | (ymin <= tol) | \\\n",
    "                                         (xmax >= img_w_target_tensor - tol) | (ymax >= img_h_target_tensor - tol)\n",
    "                        boundary_gt_indices = torch.where(is_on_boundary)[0].to(self.device)\n",
    "\n",
    "                        if boundary_gt_indices.numel() > 0:\n",
    "                            self.count_boundary_gt += boundary_gt_indices.numel() # Add count of GTs on boundary\n",
    "                            targets_boundary = {\n",
    "                                'boxes': gt_boxes_xyxy[boundary_gt_indices],\n",
    "                                'labels': target_dict_processed_t['labels'][boundary_gt_indices]\n",
    "                            }\n",
    "                            if matched_pred_indices_t.numel() > 0: # Check if matcher found matches for this frame\n",
    "                                is_boundary_match = torch.isin(matched_gt_indices_t, boundary_gt_indices)\n",
    "                                pred_indices_for_boundary = matched_pred_indices_t[is_boundary_match]\n",
    "                                if pred_indices_for_boundary.numel() > 0:\n",
    "                                    predictions_boundary = {\n",
    "                                        'scores': prediction_dict_postprocessed_t['scores'][pred_indices_for_boundary],\n",
    "                                        'labels': prediction_dict_postprocessed_t['labels'][pred_indices_for_boundary],\n",
    "                                        'boxes': prediction_dict_postprocessed_t['boxes'][pred_indices_for_boundary],\n",
    "                                    }\n",
    "                                    self.map_metric_boundary.update([predictions_boundary], [targets_boundary])\n",
    "                     except Exception as e:\n",
    "                         print(f\"Error calculating/updating boundary mAP for timestamp {t}: {e}\")\n",
    "        else:\n",
    "             print(\"Matcher did not run successfully or returned unexpected format. Skipping overlap/boundary metric updates.\")\n",
    "\n",
    "\n",
    "    def accumulate(self):\n",
    "        \"\"\"\n",
    "        Computes the final mAP results across all updated timestamps from all sequences.\n",
    "        Stores the results internally.\n",
    "        \"\"\"\n",
    "        # Accumulate standard mAP\n",
    "        try:\n",
    "            self.map_results = self.map_metric.compute()\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing standard mAP metric: {e}\")\n",
    "            self.map_results = None\n",
    "\n",
    "        # Accumulate overlap mAP\n",
    "        try:\n",
    "            self.map_results_overlap = self.map_metric_overlap.compute()\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing overlap mAP metric: {e}\")\n",
    "            self.map_results_overlap = None\n",
    "\n",
    "        # Accumulate boundary mAP\n",
    "        try:\n",
    "            self.map_results_boundary = self.map_metric_boundary.compute()\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing boundary mAP metric: {e}\")\n",
    "            self.map_results_boundary = None\n",
    "\n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"\n",
    "        Processes and returns the computed mAP results in a dictionary, including overlap and boundary metrics.\n",
    "        NOTE: Metrics reflect performance averaged over *all timestamps* processed.\n",
    "\n",
    "        Returns:\n",
    "        dict: Dictionary containing scalar mAP results prefixed with 'mAP_', 'mAP_overlap_', and 'mAP_boundary_'.\n",
    "        \"\"\"\n",
    "        summary_dict = {}\n",
    "\n",
    "        # Add GT Counts\n",
    "        summary_dict['num_total_gt_objects'] = self.count_total_gt\n",
    "        summary_dict['num_overlap_gt_objects'] = self.count_overlap_gt\n",
    "        summary_dict['num_boundary_gt_objects'] = self.count_boundary_gt\n",
    "\n",
    "        summary_dict['percentage_overlap_gt_objects'] = (self.count_overlap_gt / self.count_total_gt) * 100\n",
    "        summary_dict['percentage_boundary_gt_objects'] = (self.count_boundary_gt / self.count_total_gt) * 100\n",
    "\n",
    "        # Process standard mAP results\n",
    "        if self.map_results is not None:\n",
    "            print(f\"\\nRaw mAP Results (Evaluator): {self.map_results}\\n\")\n",
    "            for k, v in self.map_results.items():\n",
    "                if isinstance(v, torch.Tensor) and v.numel() == 1:\n",
    "                    summary_dict[f'mAP_{k}'] = v.item() # Prefix standard keys\n",
    "            if not any(k.startswith('mAP_') for k in summary_dict) and self.map_results:\n",
    "                 if 'map' in self.map_results and self.map_results['map'] == 0: pass\n",
    "                 else: print(\"Warning (Evaluator): No scalar standard mAP metrics found.\")\n",
    "        else:\n",
    "             print(\"Warning (Evaluator): Standard mAP metric computation failed or produced no results.\")\n",
    "\n",
    "        # Process overlap mAP results\n",
    "        if self.map_results_overlap is not None:\n",
    "            print(f\"\\nRaw Overlap mAP Results (Evaluator): {self.map_results_overlap}\\n\")\n",
    "            for k, v in self.map_results_overlap.items():\n",
    "                if isinstance(v, torch.Tensor) and v.numel() == 1:\n",
    "                    summary_dict[f'mAP_overlap_{k}'] = v.item() # Prefix overlap keys\n",
    "            if not any(k.startswith('mAP_overlap_') for k in summary_dict) and self.map_results_overlap:\n",
    "                 if 'map' in self.map_results_overlap and self.map_results_overlap['map'] == 0: pass\n",
    "                 else: print(\"Warning (Evaluator): No scalar overlap mAP metrics found.\")\n",
    "        else:\n",
    "             print(\"Warning (Evaluator): Overlap mAP metric computation failed or produced no results.\")\n",
    "\n",
    "        # Process boundary mAP results\n",
    "        if self.map_results_boundary is not None:\n",
    "            print(f\"\\nRaw Boundary mAP Results (Evaluator): {self.map_results_boundary}\\n\")\n",
    "            for k, v in self.map_results_boundary.items():\n",
    "                if isinstance(v, torch.Tensor) and v.numel() == 1:\n",
    "                    summary_dict[f'mAP_boundary_{k}'] = v.item() # Prefix boundary keys\n",
    "            if not any(k.startswith('mAP_boundary_') for k in summary_dict) and self.map_results_boundary:\n",
    "                 if 'map' in self.map_results_boundary and self.map_results_boundary['map'] == 0: pass\n",
    "                 else: print(\"Warning (Evaluator): No scalar boundary mAP metrics found.\")\n",
    "        else:\n",
    "             print(\"Warning (Evaluator): Boundary mAP metric computation failed or produced no results.\")\n",
    "\n",
    "\n",
    "        return summary_dict\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the internal state of all metrics.\"\"\"\n",
    "        self.map_metric.reset()\n",
    "        self.map_results = None\n",
    "        self.map_metric_overlap.reset()\n",
    "        self.map_results_overlap = None\n",
    "        self.map_metric_boundary.reset()\n",
    "        self.map_results_boundary = None\n",
    "        # Reset GT Counts\n",
    "        self.count_total_gt = 0\n",
    "        self.count_overlap_gt = 0\n",
    "        self.count_boundary_gt = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4e1bd5-97f5-4bb7-a2e8-53fb59a9cead",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206fe301-b091-4e49-a2f6-e460bc7a05dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test huggingface MovingMNIST dataset...\n",
      "Using object detection mode\n",
      "Resizing frames to 320x320\n",
      "Transforms: Compose(\n",
      "    <dataset.transformations.RandomResize object at 0x1533d453d890>\n",
      "    <dataset.transformations.NormBoxesTransform object at 0x1533d453c5d0>\n",
      ")\n",
      "ds size: 10000\n",
      "Loading YOLO model for RecurrentVideoObjectModule...\n",
      "YOLO detector loaded from ../not_tracked_dir/output_yolo_v8_2025-05-14T15/weights/best.pt\n",
      "Detected 10 classes from YOLO model.\n",
      "RecurrentVideoObjectModule created with YOLO detector.\n",
      "Clear norm transforms\n",
      "Using HungarianMatcher for object detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval -1::  61%|    | 6091/10000 [39:30<27:08,  2.40it/s, loss_running=0, class_error_running=nan, loss_center_point_running=nan, loss_ce_running=0, loss=0, loss_ce_unscaled=0, loss_bbox_unscaled=0, loss_giou_unscaled=0, loss_unscaled=0, loss_ce=0, view_dropout_prob=-1]  "
     ]
    }
   ],
   "source": [
    "from engine import evaluate\n",
    "from dataset import build_dataset\n",
    "import torch\n",
    "from engine import evaluate\n",
    "from dataset import build_dataset\n",
    "from models import build_model\n",
    "from models.perceiver import PostProcess\n",
    "import os\n",
    "from pathlib import Path\n",
    "from util.misc import collate_fn, is_main_process, get_sha, get_rank\n",
    "from torch.utils.data import DataLoader\n",
    "from ultralytics import YOLO\n",
    "import torchvision.transforms as T\n",
    "from models.matcher import build_matcher\n",
    "\n",
    "args.model = 'YOLO' # 'perceiver'\n",
    "\n",
    "if args.model == 'perceiver':\n",
    "    # https://wandb.ai/university-of-tartu-2/multi-sensor-dropout/runs/stu3mw4u/overview\n",
    "    args.resume = 'checkpoint_epoch_31.pth'\n",
    "    args.output_dir = \"../not_tracked_dir/output_perceiver_detection_2025-04-23_19-42-48/\"\n",
    "    args.resize_frame = 320\n",
    "elif args.model == 'YOLO':\n",
    "    # args.output_dir = \"../not_tracked_dir/output_yolo_v8_2025-04-11/\" # YOLO\n",
    "    args.output_dir = \"../not_tracked_dir/output_yolo_v8_2025-05-14T15\"\n",
    "    args.resume = 'weights/best.pt'\n",
    "    args.resize_frame = 320\n",
    "else:\n",
    "    raise Error(f\"Unsupported model: {args.model}\")\n",
    "\n",
    "args.test_dataset_fraction = 1.0\n",
    "# args.num_workers = 1\n",
    "\n",
    "checkpoint_path = os.path.join(args.output_dir, args.resume)\n",
    "\n",
    "dataset_test = build_dataset(split='test', args=args)\n",
    "\n",
    "print(f\"ds size: {len(dataset_test)}\")\n",
    "\n",
    "sampler_test = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "dataloader_test = DataLoader(dataset_test, sampler=sampler_test, batch_size=args.batch_size,\n",
    "                                collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "if args.model == 'YOLO':\n",
    "    print(\"Loading YOLO model for RecurrentVideoObjectModule...\")\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"YOLO checkpoint (.pt file) not found at {checkpoint_path}\")\n",
    "\n",
    "    # Load the base YOLO detector\n",
    "    yolo_detector = YOLO(checkpoint_path)\n",
    "    print(f\"YOLO detector loaded from {checkpoint_path}\")\n",
    "\n",
    "    # Get number of classes from the loaded YOLO model\n",
    "    num_classes = len(yolo_detector.names)\n",
    "    print(f\"Detected {num_classes} classes from YOLO model.\")\n",
    "\n",
    "    # Instantiate the wrapper module\n",
    "    model = RecurrentVideoObjectModule(detector_module=yolo_detector, num_classes=num_classes)\n",
    "    print(\"RecurrentVideoObjectModule created with YOLO detector.\")\n",
    "\n",
    "    print(\"Clear norm transforms\")\n",
    "    dataset_test.norm_transforms = T.Compose([])\n",
    "    \n",
    "elif args.model == 'perceiver' and os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "    model = build_model(args, (0, 0))\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=args.device)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "else:\n",
    "    raise Error(\"Model not supported.\")\n",
    "    \n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "postprocessors = {\n",
    "    # 'bbox': SeqNMSEvaluationPostprocessor() # SeqNMSEvaluationPostprocessor(PostProcess())\n",
    "    # '': \n",
    "}\n",
    "\n",
    "matcher = build_matcher(args)\n",
    "matcher.to(device)\n",
    "\n",
    "evaluators = [\n",
    "    MeanAveragePrecisionEvaluator(device=device, postprocessor = PostProcess(), matcher=matcher, overlap_iou_threshold=0.1) #  \n",
    "]\n",
    "\n",
    "criterion = DummyCriterion(device)\n",
    "\n",
    "model.to(args.device)\n",
    "\n",
    "results = evaluate(\n",
    "\tmodel=model,\n",
    "\tdataloader=dataloader_test,\n",
    "\tcriterion=criterion,  # Not needed for evaluation\n",
    "\tpostprocessors=postprocessors,\n",
    "\tepoch=-1,\n",
    "\tdevice=device,\n",
    "    evaluators=evaluators\n",
    ")\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "for metric_name, value in results.items():\n",
    "\tprint(f\"{metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c641914-e10c-4297-bdcc-3228f97bca13",
   "metadata": {},
   "source": [
    "## YOLO + Seq-NMS\n",
    "\n",
    "\n",
    "Raw mAP Results: {'map': tensor(0.9259), 'map_50': tensor(0.9597), 'map_75': tensor(0.9478), 'map_small': tensor(0.9259), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.7464), 'mar_10': tensor(0.9462), 'mar_100': tensor(0.9462), 'mar_small': tensor(0.9462), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "Evaluation Results:\n",
    "loss_running: 0.0000\n",
    "class_error_running: nan\n",
    "loss_center_point_running: nan\n",
    "loss_ce_running: 0.0000\n",
    "loss: 0.0000\n",
    "loss_ce_unscaled: 0.0000\n",
    "loss_bbox_unscaled: 0.0000\n",
    "loss_giou_unscaled: 0.0000\n",
    "loss_unscaled: 0.0000\n",
    "loss_ce: 0.0000\n",
    "mAP_map: 0.9259\n",
    "mAP_map_50: 0.9597\n",
    "mAP_map_75: 0.9478\n",
    "mAP_map_small: 0.9259\n",
    "mAP_map_medium: -1.0000\n",
    "mAP_map_large: -1.0000\n",
    "mAP_mar_1: 0.7464\n",
    "mAP_mar_10: 0.9462\n",
    "mAP_mar_100: 0.9462\n",
    "mAP_mar_small: 0.9462\n",
    "mAP_mar_medium: -1.0000\n",
    "mAP_mar_large: -1.0000\n",
    "mAP_map_per_class: -1.0000\n",
    "mAP_mar_100_per_class: -1.0000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239ecb80-eac9-42ab-ae10-a8a72a574a56",
   "metadata": {},
   "source": [
    "## YOLO (from random weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c3a5e1-cf6e-4669-b88a-8d174a6a7d75",
   "metadata": {},
   "source": [
    "## Perceiver\n",
    "\n",
    "https://wandb.ai/university-of-tartu-2/multi-sensor-dropout/runs/stu3mw4u/overview\n",
    "args.resume = 'checkpoint_epoch_31.pth'\n",
    "args.output_dir = \"../not_tracked_dir/output_perceiver_detection_2025-04-23_19-42-48/\"\n",
    "\n",
    "\n",
    "Raw mAP Results (Evaluator): {'map': tensor(0.9025), 'map_50': tensor(0.9688), 'map_75': tensor(0.9442), 'map_small': tensor(0.9025), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.7367), 'mar_10': tensor(0.9240), 'mar_100': tensor(0.9240), 'mar_small': tensor(0.9240), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "\n",
    "Raw Overlap mAP Results (Evaluator): {'map': tensor(0.6940), 'map_50': tensor(0.8853), 'map_75': tensor(0.7777), 'map_small': tensor(0.6940), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.6686), 'mar_10': tensor(0.7389), 'mar_100': tensor(0.7389), 'mar_small': tensor(0.7389), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "\n",
    "Raw Boundary mAP Results (Evaluator): {'map': tensor(0.8470), 'map_50': tensor(0.9552), 'map_75': tensor(0.9101), 'map_small': tensor(0.8470), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.8512), 'mar_10': tensor(0.8739), 'mar_100': tensor(0.8739), 'mar_small': tensor(0.8739), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "### OVERLAP IOU 0.1\n",
    "\n",
    "Raw mAP Results (Evaluator): {'map': tensor(0.9025), 'map_50': tensor(0.9688), 'map_75': tensor(0.9442), 'map_small': tensor(0.9025), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.7367), 'mar_10': tensor(0.9240), 'mar_100': tensor(0.9240), 'mar_small': tensor(0.9240), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "\n",
    "Raw Overlap mAP Results (Evaluator): {'map': tensor(0.6100), 'map_50': tensor(0.8396), 'map_75': tensor(0.6883), 'map_small': tensor(0.6100), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.6138), 'mar_10': tensor(0.6676), 'mar_100': tensor(0.6676), 'mar_small': tensor(0.6676), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "\n",
    "Raw Boundary mAP Results (Evaluator): {'map': tensor(0.8470), 'map_50': tensor(0.9552), 'map_75': tensor(0.9101), 'map_small': tensor(0.8470), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.8512), 'mar_10': tensor(0.8739), 'mar_100': tensor(0.8739), 'mar_small': tensor(0.8739), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "Evaluation Results:\n",
    "loss_running: 0.0000\n",
    "class_error_running: nan\n",
    "loss_center_point_running: nan\n",
    "loss_ce_running: 0.0000\n",
    "loss: 0.0000\n",
    "loss_ce_unscaled: 0.0000\n",
    "loss_bbox_unscaled: 0.0000\n",
    "loss_giou_unscaled: 0.0000\n",
    "loss_unscaled: 0.0000\n",
    "loss_ce: 0.0000\n",
    "num_total_gt_objects: 851740.0000\n",
    "num_overlap_gt_objects: 88680.0000\n",
    "num_boundary_gt_objects: 97997.0000\n",
    "percentage_overlap_gt_objects: 10.4116\n",
    "percentage_boundary_gt_objects: 11.5055\n",
    "mAP_map: 0.9025\n",
    "mAP_map_50: 0.9688\n",
    "mAP_map_75: 0.9442\n",
    "mAP_map_small: 0.9025\n",
    "mAP_map_medium: -1.0000\n",
    "mAP_map_large: -1.0000\n",
    "mAP_mar_1: 0.7367\n",
    "mAP_mar_10: 0.9240\n",
    "mAP_mar_100: 0.9240\n",
    "mAP_mar_small: 0.9240\n",
    "mAP_mar_medium: -1.0000\n",
    "mAP_mar_large: -1.0000\n",
    "mAP_map_per_class: -1.0000\n",
    "mAP_mar_100_per_class: -1.0000\n",
    "mAP_overlap_map: 0.6100\n",
    "mAP_overlap_map_50: 0.8396\n",
    "mAP_overlap_map_75: 0.6883\n",
    "mAP_overlap_map_small: 0.6100\n",
    "mAP_overlap_map_medium: -1.0000\n",
    "mAP_overlap_map_large: -1.0000\n",
    "mAP_overlap_mar_1: 0.6138\n",
    "mAP_overlap_mar_10: 0.6676\n",
    "mAP_overlap_mar_100: 0.6676\n",
    "mAP_overlap_mar_small: 0.6676\n",
    "mAP_overlap_mar_medium: -1.0000\n",
    "mAP_overlap_mar_large: -1.0000\n",
    "mAP_overlap_map_per_class: -1.0000\n",
    "mAP_overlap_mar_100_per_class: -1.0000\n",
    "mAP_boundary_map: 0.8470\n",
    "mAP_boundary_map_50: 0.9552\n",
    "mAP_boundary_map_75: 0.9101\n",
    "mAP_boundary_map_small: 0.8470\n",
    "mAP_boundary_map_medium: -1.0000\n",
    "mAP_boundary_map_large: -1.0000\n",
    "mAP_boundary_mar_1: 0.8512\n",
    "mAP_boundary_mar_10: 0.8739\n",
    "mAP_boundary_mar_100: 0.8739\n",
    "mAP_boundary_mar_small: 0.8739\n",
    "mAP_boundary_mar_medium: -1.0000\n",
    "mAP_boundary_mar_large: -1.0000\n",
    "mAP_boundary_map_per_class: -1.0000\n",
    "mAP_boundary_mar_100_per_class: -1.0000\n",
    "\n",
    "\n",
    "## YOLO\n",
    "\n",
    "### OVERLAP IOU 0.01\n",
    "Raw mAP Results (Evaluator): {'map': tensor(0.9209), 'map_50': tensor(0.9588), 'map_75': tensor(0.9367), 'map_small': tensor(0.9210), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.7747), 'mar_10': tensor(0.9375), 'mar_100': tensor(0.9375), 'mar_small': tensor(0.9375), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "\n",
    "Raw Overlap mAP Results (Evaluator): {'map': tensor(0.8149), 'map_50': tensor(0.9072), 'map_75': tensor(0.8693), 'map_small': tensor(0.8149), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.7976), 'mar_10': tensor(0.8430), 'mar_100': tensor(0.8430), 'mar_small': tensor(0.8430), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "\n",
    "Raw Boundary mAP Results (Evaluator): {'map': tensor(0.7145), 'map_50': tensor(0.7703), 'map_75': tensor(0.7448), 'map_small': tensor(0.7145), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.7244), 'mar_10': tensor(0.7425), 'mar_100': tensor(0.7425), 'mar_small': tensor(0.7425), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "\n",
    "Raw mAP Results: {'map': tensor(0.9209), 'map_50': tensor(0.9588), 'map_75': tensor(0.9367), 'map_small': tensor(0.9210), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.7747), 'mar_10': tensor(0.9375), 'mar_100': tensor(0.9375), 'mar_small': tensor(0.9375), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "### OVERLAP IOU 0.5\n",
    "\n",
    "Raw mAP Results (Evaluator): {'map': tensor(0.9231), 'map_50': tensor(0.9621), 'map_75': tensor(0.9410), 'map_small': tensor(0.9231), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.7471), 'mar_10': tensor(0.9393), 'mar_100': tensor(0.9393), 'mar_small': tensor(0.9393), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "\n",
    "Raw Overlap mAP Results (Evaluator): {'map': tensor(0.4955), 'map_50': tensor(0.6500), 'map_75': tensor(0.5667), 'map_small': tensor(0.4955), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.5651), 'mar_10': tensor(0.5717), 'mar_100': tensor(0.5717), 'mar_small': tensor(0.5717), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "\n",
    "Raw Boundary mAP Results (Evaluator): {'map': tensor(0.6989), 'map_50': tensor(0.7667), 'map_75': tensor(0.7365), 'map_small': tensor(0.6989), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.7119), 'mar_10': tensor(0.7280), 'mar_100': tensor(0.7280), 'mar_small': tensor(0.7280), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "Evaluation Results:\n",
    "loss_running: 0.0000\n",
    "class_error_running: nan\n",
    "loss_center_point_running: nan\n",
    "loss_ce_running: 0.0000\n",
    "loss: 0.0000\n",
    "loss_ce_unscaled: 0.0000\n",
    "loss_bbox_unscaled: 0.0000\n",
    "loss_giou_unscaled: 0.0000\n",
    "loss_unscaled: 0.0000\n",
    "loss_ce: 0.0000\n",
    "num_total_gt_objects: 851740.0000\n",
    "num_overlap_gt_objects: 9503.0000\n",
    "num_boundary_gt_objects: 97997.0000\n",
    "percentage_overlap_gt_objects: 1.1157\n",
    "percentage_boundary_gt_objects: 11.5055\n",
    "mAP_map: 0.9231\n",
    "mAP_map_50: 0.9621\n",
    "mAP_map_75: 0.9410\n",
    "mAP_map_small: 0.9231\n",
    "mAP_map_medium: -1.0000\n",
    "mAP_map_large: -1.0000\n",
    "mAP_mar_1: 0.7471\n",
    "mAP_mar_10: 0.9393\n",
    "mAP_mar_100: 0.9393\n",
    "mAP_mar_small: 0.9393\n",
    "mAP_mar_medium: -1.0000\n",
    "mAP_mar_large: -1.0000\n",
    "mAP_map_per_class: -1.0000\n",
    "mAP_mar_100_per_class: -1.0000\n",
    "mAP_overlap_map: 0.4955\n",
    "mAP_overlap_map_50: 0.6500\n",
    "mAP_overlap_map_75: 0.5667\n",
    "mAP_overlap_map_small: 0.4955\n",
    "mAP_overlap_map_medium: -1.0000\n",
    "mAP_overlap_map_large: -1.0000\n",
    "mAP_overlap_mar_1: 0.5651\n",
    "mAP_overlap_mar_10: 0.5717\n",
    "mAP_overlap_mar_100: 0.5717\n",
    "mAP_overlap_mar_small: 0.5717\n",
    "mAP_overlap_mar_medium: -1.0000\n",
    "mAP_overlap_mar_large: -1.0000\n",
    "mAP_overlap_map_per_class: -1.0000\n",
    "mAP_overlap_mar_100_per_class: -1.0000\n",
    "mAP_boundary_map: 0.6989\n",
    "mAP_boundary_map_50: 0.7667\n",
    "mAP_boundary_map_75: 0.7365\n",
    "mAP_boundary_map_small: 0.6989\n",
    "mAP_boundary_map_medium: -1.0000\n",
    "mAP_boundary_map_large: -1.0000\n",
    "mAP_boundary_mar_1: 0.7119\n",
    "mAP_boundary_mar_10: 0.7280\n",
    "mAP_boundary_mar_100: 0.7280\n",
    "mAP_boundary_mar_small: 0.7280\n",
    "mAP_boundary_mar_medium: -1.0000\n",
    "mAP_boundary_mar_large: -1.0000\n",
    "mAP_boundary_map_per_class: -1.0000\n",
    "mAP_boundary_mar_100_per_class: -1.0000\n",
    "\n",
    "### OVERLAP IOU 0.1\n",
    "\n",
    "Raw mAP Results (Evaluator): {'map': tensor(0.9231), 'map_50': tensor(0.9621), 'map_75': tensor(0.9410), 'map_small': tensor(0.9231), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.7471), 'mar_10': tensor(0.9393), 'mar_100': tensor(0.9393), 'mar_small': tensor(0.9393), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "\n",
    "Raw Overlap mAP Results (Evaluator): {'map': tensor(0.7570), 'map_50': tensor(0.8749), 'map_75': tensor(0.8246), 'map_small': tensor(0.7570), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.7398), 'mar_10': tensor(0.7959), 'mar_100': tensor(0.7959), 'mar_small': tensor(0.7959), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "\n",
    "Raw Boundary mAP Results (Evaluator): {'map': tensor(0.6989), 'map_50': tensor(0.7667), 'map_75': tensor(0.7365), 'map_small': tensor(0.6989), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.7119), 'mar_10': tensor(0.7280), 'mar_100': tensor(0.7280), 'mar_small': tensor(0.7280), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "Evaluation Results:\n",
    "loss_running: 0.0000\n",
    "class_error_running: nan\n",
    "loss_center_point_running: nan\n",
    "loss_ce_running: 0.0000\n",
    "loss: 0.0000\n",
    "loss_ce_unscaled: 0.0000\n",
    "loss_bbox_unscaled: 0.0000\n",
    "loss_giou_unscaled: 0.0000\n",
    "loss_unscaled: 0.0000\n",
    "loss_ce: 0.0000\n",
    "num_total_gt_objects: 851740.0000\n",
    "num_overlap_gt_objects: 88680.0000\n",
    "num_boundary_gt_objects: 97997.0000\n",
    "percentage_overlap_gt_objects: 10.4116\n",
    "percentage_boundary_gt_objects: 11.5055\n",
    "mAP_map: 0.9231\n",
    "mAP_map_50: 0.9621\n",
    "mAP_map_75: 0.9410\n",
    "mAP_map_small: 0.9231\n",
    "mAP_map_medium: -1.0000\n",
    "mAP_map_large: -1.0000\n",
    "mAP_mar_1: 0.7471\n",
    "mAP_mar_10: 0.9393\n",
    "mAP_mar_100: 0.9393\n",
    "mAP_mar_small: 0.9393\n",
    "mAP_mar_medium: -1.0000\n",
    "mAP_mar_large: -1.0000\n",
    "mAP_map_per_class: -1.0000\n",
    "mAP_mar_100_per_class: -1.0000\n",
    "mAP_overlap_map: 0.7570\n",
    "mAP_overlap_map_50: 0.8749\n",
    "mAP_overlap_map_75: 0.8246\n",
    "mAP_overlap_map_small: 0.7570\n",
    "mAP_overlap_map_medium: -1.0000\n",
    "mAP_overlap_map_large: -1.0000\n",
    "mAP_overlap_mar_1: 0.7398\n",
    "mAP_overlap_mar_10: 0.7959\n",
    "mAP_overlap_mar_100: 0.7959\n",
    "mAP_overlap_mar_small: 0.7959\n",
    "mAP_overlap_mar_medium: -1.0000\n",
    "mAP_overlap_mar_large: -1.0000\n",
    "mAP_overlap_map_per_class: -1.0000\n",
    "mAP_overlap_mar_100_per_class: -1.0000\n",
    "mAP_boundary_map: 0.6989\n",
    "mAP_boundary_map_50: 0.7667\n",
    "mAP_boundary_map_75: 0.7365\n",
    "mAP_boundary_map_small: 0.6989\n",
    "mAP_boundary_map_medium: -1.0000\n",
    "mAP_boundary_map_large: -1.0000\n",
    "mAP_boundary_mar_1: 0.7119\n",
    "mAP_boundary_mar_10: 0.7280\n",
    "mAP_boundary_mar_100: 0.7280\n",
    "mAP_boundary_mar_small: 0.7280\n",
    "mAP_boundary_mar_medium: -1.0000\n",
    "mAP_boundary_mar_large: -1.0000\n",
    "mAP_boundary_map_per_class: -1.0000\n",
    "mAP_boundary_mar_100_per_class: -1.0000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9535900e-5d53-4331-8a02-c0c165a22b57",
   "metadata": {},
   "source": [
    "## RESULT\n",
    "\n",
    "Raw mAP Results: {'map': tensor(0.7331), 'map_50': tensor(0.9379), 'map_75': tensor(0.8674), 'map_small': tensor(0.7331), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.6338), 'mar_10': tensor(0.7885), 'mar_100': tensor(0.7885), 'mar_small': tensor(0.7885), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "Evaluation Results:\n",
    "loss_running: 0.0000\n",
    "class_error_running: nan\n",
    "loss_center_point_running: nan\n",
    "loss_ce_running: 0.0000\n",
    "loss: 0.0000\n",
    "loss_ce_unscaled: 0.0000\n",
    "loss_bbox_unscaled: 0.0000\n",
    "loss_giou_unscaled: 0.0000\n",
    "loss_unscaled: 0.0000\n",
    "loss_ce: 0.0000\n",
    "mAP_map: 0.7331\n",
    "mAP_map_50: 0.9379\n",
    "mAP_map_75: 0.8674\n",
    "mAP_map_small: 0.7331\n",
    "mAP_map_medium: -1.0000\n",
    "mAP_map_large: -1.0000\n",
    "mAP_mar_1: 0.6338\n",
    "mAP_mar_10: 0.7885\n",
    "mAP_mar_100: 0.7885\n",
    "mAP_mar_small: 0.7885\n",
    "mAP_mar_medium: -1.0000\n",
    "mAP_mar_large: -1.0000\n",
    "mAP_map_per_class: -1.0000\n",
    "mAP_mar_100_per_class: -1.0000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b464e-fad7-4efd-bb4a-8316b48e3f58",
   "metadata": {},
   "source": [
    "## Result\n",
    "top 10 and threashold 0.5\n",
    "\n",
    "Raw mAP Results: {'map': tensor(0.6828), 'map_50': tensor(0.8640), 'map_75': tensor(0.8098), 'map_small': tensor(0.6828), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.5995), 'mar_10': tensor(0.7301), 'mar_100': tensor(0.7301), 'mar_small': tensor(0.7301), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "Evaluation Results:\n",
    "loss_running: 0.0000\n",
    "class_error_running: nan\n",
    "loss_center_point_running: nan\n",
    "loss_ce_running: 0.0000\n",
    "loss: 0.0000\n",
    "loss_ce_unscaled: 0.0000\n",
    "loss_bbox_unscaled: 0.0000\n",
    "loss_giou_unscaled: 0.0000\n",
    "loss_unscaled: 0.0000\n",
    "loss_ce: 0.0000\n",
    "mAP_map: 0.6828\n",
    "mAP_map_50: 0.8640\n",
    "mAP_map_75: 0.8098\n",
    "mAP_map_small: 0.6828\n",
    "mAP_map_medium: -1.0000\n",
    "mAP_map_large: -1.0000\n",
    "mAP_mar_1: 0.5995\n",
    "mAP_mar_10: 0.7301\n",
    "mAP_mar_100: 0.7301\n",
    "mAP_mar_small: 0.7301\n",
    "mAP_mar_medium: -1.0000\n",
    "mAP_mar_large: -1.0000\n",
    "mAP_map_per_class: -1.0000\n",
    "mAP_mar_100_per_class: -1.0000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f8dcf-b6a5-4423-8b07-4ab714e2bd8a",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "405e6760-ec98-49b8-8493-467a1c5ad097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/helios/home/ploter/.conda/envs/sensor_dropout/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using YOLOv8 backbone for feature extraction\n",
      "num_freq_bands: 6\n",
      "depth: 1\n",
      "max_freq: 10\n",
      "input_channels: 256\n",
      "input_axis: 2\n",
      "num_latents: 16\n",
      "latent_dim: 128\n",
      "cross_heads: 1\n",
      "latent_heads: 1\n",
      "cross_dim_head: 282\n",
      "latent_dim_head: 128\n",
      "num_classes: -1\n",
      "attn_dropout: 0.0\n",
      "ff_dropout: 0.0\n",
      "weight_tie_layers: False\n",
      "fourier_encode_data: True\n",
      "self_per_cross_attn: 1\n",
      "final_classifier_head: False\n",
      "num_sensors: 1\n",
      "__class__: <class 'models.perceiver.Perceiver'>\n",
      "Loading checkpoint from ../not_tracked_dir/output_perceiver_detection_2025-07-18_22-08-06/checkpoint_epoch_17.pth\n",
      "Generating test huggingface MovingMNIST dataset Max-Ploter/detection-moving-mnist-medium...\n",
      "Using object detection mode\n",
      "Resizing frames to 320x320\n",
      "Transforms: Compose(\n",
      "    <dataset.transformations.RandomResize object at 0x145631c7ddd0>\n",
      "    <dataset.transformations.NormBoxesTransform object at 0x1455fbb4dcd0>\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/helios/home/ploter/.conda/envs/sensor_dropout/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ultralytics.utils.plotting import plot_images\n",
    "from engine import evaluate\n",
    "from dataset import build_dataset\n",
    "import torch\n",
    "from engine import evaluate\n",
    "from dataset import build_dataset\n",
    "from models import build_model\n",
    "from models.perceiver import PostProcess\n",
    "import os\n",
    "from pathlib import Path\n",
    "from util.misc import collate_fn, is_main_process, get_sha, get_rank\n",
    "from torch.utils.data import DataLoader\n",
    "from ultralytics import YOLO\n",
    "import torchvision.transforms as T\n",
    "\n",
    "args.model = 'perceiver' # 'perceiver'\n",
    "\n",
    "if args.model == 'YOLO':\n",
    "\n",
    "    args.output_dir = \"../not_tracked_dir/output_yolo_v8_2025-04-11/\" # YOLO\n",
    "    args.resume = 'weights/best.pt'\n",
    "    args.resize_frame = 320\n",
    "    \n",
    "    checkpoint_path = os.path.join(args.output_dir, args.resume)\n",
    "    print(\"Loading YOLO model for RecurrentVideoObjectModule...\")\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"YOLO checkpoint (.pt file) not found at {checkpoint_path}\")\n",
    "\n",
    "    # Load the base YOLO detector\n",
    "    yolo_detector = YOLO(checkpoint_path)\n",
    "    print(f\"YOLO detector loaded from {checkpoint_path}\")\n",
    "\n",
    "    # Get number of classes from the loaded YOLO model\n",
    "    num_classes = len(yolo_detector.names)\n",
    "    print(f\"Detected {num_classes} classes from YOLO model.\")\n",
    "\n",
    "    # Instantiate the wrapper module\n",
    "    model = RecurrentVideoObjectModule(detector_module=yolo_detector, num_classes=num_classes)\n",
    "    print(\"RecurrentVideoObjectModule created with YOLO detector.\")\n",
    "\n",
    "    args.test_dataset_fraction = 0.01\n",
    "    \n",
    "elif args.model == 'perceiver':\n",
    "\n",
    "    # https://wandb.ai/university-of-tartu-2/multi-sensor-dropout/runs/stu3mw4u/overview\n",
    "    args.resume = 'checkpoint_epoch_17.pth'\n",
    "    args.output_dir = \"../not_tracked_dir/output_perceiver_detection_2025-07-18_22-08-06\"\n",
    "    args.resize_frame = 320\n",
    "    args.backbone = 'yolo'\n",
    "    args.test_dataset_fraction = 0.01\n",
    "    checkpoint_path = os.path.join(args.output_dir, args.resume)\n",
    "    model = build_model(args, (0,0))\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=args.device)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "\n",
    "save_dir = os.path.join(args.output_dir, 'test_eval_best')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "dataset_test = build_dataset(split='test', args=args)\n",
    "\n",
    "if args.model == 'YOLO':\n",
    "    print(\"Clear norm transforms\")\n",
    "    dataset_test.norm_transforms = T.Compose([])\n",
    "\n",
    "sampler_test = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "dataloader_test = DataLoader(dataset_test, sampler=sampler_test, batch_size=args.batch_size,\n",
    "                                collate_fn=collate_fn, num_workers=args.num_workers, pin_memory=True)\n",
    "\n",
    "\n",
    "model.to(args.device)\n",
    "model.eval()\n",
    "\n",
    "def get_batch_by_index(dataloader, index):\n",
    "    iterator = iter(dataloader)\n",
    "    for i in range(index + 1):\n",
    "        try:\n",
    "            batch = next(iterator)\n",
    "            if i == index:\n",
    "                return batch\n",
    "        except StopIteration:\n",
    "            return None  # Index out of bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75b14723-83f7-4b6a-80fd-2ce38929f7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([320, 320])\n",
      "tensor([[-1.9247e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01],\n",
      "        [ 3.1175e-01,  4.3121e-01,  4.3318e-01,  4.3515e-01,  4.3417e-01,  4.3220e-01,  4.3072e-01,  4.3072e-01,  4.3072e-01,  4.1890e-01,  4.0313e-01,  3.6076e-01,  2.3859e-01,  1.1641e-01,  2.3649e-03, -1.0897e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01],\n",
      "        [ 2.3286e+00,  2.9259e+00,  2.9358e+00,  2.9456e+00,  2.9407e+00,  2.9309e+00,  2.9235e+00,  2.9235e+00,  2.9235e+00,  2.8644e+00,  2.7855e+00,  2.5737e+00,  1.9628e+00,  1.3520e+00,  7.8172e-01,  2.2503e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01],\n",
      "        [ 4.3455e+00,  5.4207e+00,  5.4384e+00,  5.4561e+00,  5.4473e+00,  5.4295e+00,  5.4162e+00,  5.4162e+00,  5.4162e+00,  5.3098e+00,  5.1680e+00,  4.7867e+00,  3.6871e+00,  2.5875e+00,  1.5611e+00,  5.5904e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01],\n",
      "        [ 5.1793e+00,  6.0362e+00,  6.0530e+00,  6.0697e+00,  6.0614e+00,  6.0446e+00,  6.0320e+00,  6.0320e+00,  6.0320e+00,  5.9493e+00,  5.8389e+00,  5.5424e+00,  4.6871e+00,  3.8319e+00,  2.9759e+00,  2.1197e+00,  1.3638e+00,  9.0858e-01,  4.5338e-01,  1.7996e-01],\n",
      "        [ 5.6187e+00,  6.0254e+00,  6.0382e+00,  6.0510e+00,  6.0446e+00,  6.0318e+00,  6.0222e+00,  6.0222e+00,  6.0222e+00,  5.9867e+00,  5.9394e+00,  5.8123e+00,  5.4458e+00,  5.0793e+00,  4.6026e+00,  4.0893e+00,  3.4388e+00,  2.3767e+00,  1.3145e+00,  6.7654e-01],\n",
      "        [ 5.9549e+00,  6.0173e+00,  6.0271e+00,  6.0370e+00,  6.0320e+00,  6.0222e+00,  6.0148e+00,  6.0148e+00,  6.0148e+00,  6.0148e+00,  6.0148e+00,  6.0148e+00,  6.0148e+00,  6.0148e+00,  5.8426e+00,  5.6130e+00,  5.0946e+00,  3.7092e+00,  2.3239e+00,  1.3820e+00],\n",
      "        [ 5.9815e+00,  6.0173e+00,  6.0271e+00,  6.0370e+00,  6.0320e+00,  6.0222e+00,  6.0148e+00,  6.0148e+00,  6.0148e+00,  6.0148e+00,  6.0148e+00,  6.0148e+00,  6.0148e+00,  6.0148e+00,  5.9224e+00,  5.7993e+00,  5.4926e+00,  4.6354e+00,  3.7782e+00,  2.7141e+00],\n",
      "        [ 6.0081e+00,  6.0173e+00,  6.0271e+00,  6.0370e+00,  6.0320e+00,  6.0222e+00,  6.0148e+00,  6.0148e+00,  6.0148e+00,  6.0148e+00,  6.0148e+00,  6.0148e+00,  6.0148e+00,  6.0148e+00,  6.0022e+00,  5.9855e+00,  5.8907e+00,  5.5616e+00,  5.2325e+00,  4.0462e+00],\n",
      "        [ 6.0148e+00,  6.0173e+00,  6.0271e+00,  6.0370e+00,  6.0099e+00,  5.9705e+00,  5.9439e+00,  5.9557e+00,  5.9675e+00,  5.9838e+00,  6.0015e+00,  6.0148e+00,  6.0148e+00,  6.0148e+00,  6.0222e+00,  6.0320e+00,  6.0042e+00,  5.8633e+00,  5.7224e+00,  4.8278e+00],\n",
      "        [ 6.0148e+00,  6.0173e+00,  6.0271e+00,  6.0370e+00,  5.9803e+00,  5.9015e+00,  5.8493e+00,  5.8769e+00,  5.9044e+00,  5.9424e+00,  5.9838e+00,  6.0148e+00,  6.0148e+00,  6.0148e+00,  6.0222e+00,  6.0320e+00,  6.0229e+00,  5.9569e+00,  5.8909e+00,  5.4258e+00],\n",
      "        [ 6.0148e+00,  5.9825e+00,  5.8535e+00,  5.7244e+00,  5.5909e+00,  5.4559e+00,  5.3670e+00,  5.4162e+00,  5.4655e+00,  5.5473e+00,  5.6399e+00,  5.7399e+00,  5.8621e+00,  5.9843e+00,  6.0222e+00,  6.0320e+00,  6.0370e+00,  6.0271e+00,  6.0173e+00,  5.8884e+00],\n",
      "        [ 6.0148e+00,  5.8436e+00,  5.1588e+00,  4.4741e+00,  4.1218e+00,  3.8804e+00,  3.7216e+00,  3.8102e+00,  3.8989e+00,  4.0910e+00,  4.3177e+00,  4.6403e+00,  5.2512e+00,  5.8621e+00,  6.0222e+00,  6.0320e+00,  6.0370e+00,  6.0271e+00,  6.0173e+00,  5.9446e+00],\n",
      "        [ 6.0148e+00,  5.7047e+00,  4.4642e+00,  3.2237e+00,  2.6528e+00,  2.3050e+00,  2.0761e+00,  2.2042e+00,  2.3323e+00,  2.6348e+00,  2.9954e+00,  3.5408e+00,  4.6403e+00,  5.7399e+00,  6.0222e+00,  6.0320e+00,  6.0370e+00,  6.0271e+00,  6.0173e+00,  6.0008e+00],\n",
      "        [ 5.0105e+00,  4.3302e+00,  3.1784e+00,  2.0266e+00,  1.5421e+00,  1.2800e+00,  1.1076e+00,  1.2042e+00,  1.3007e+00,  1.5672e+00,  1.8904e+00,  2.4848e+00,  3.8927e+00,  5.3007e+00,  5.7732e+00,  5.9338e+00,  6.0510e+00,  6.0382e+00,  6.0254e+00,  6.0222e+00],\n",
      "        [ 3.6715e+00,  2.5439e+00,  1.6956e+00,  8.4724e-01,  5.5092e-01,  4.3860e-01,  3.6470e-01,  4.0608e-01,  4.4746e-01,  6.2925e-01,  8.5784e-01,  1.4433e+00,  3.0996e+00,  4.7559e+00,  5.4411e+00,  5.8027e+00,  6.0697e+00,  6.0530e+00,  6.0362e+00,  6.0320e+00],\n",
      "        [ 2.3990e+00,  1.0645e+00,  5.0584e-01, -5.2813e-02, -1.9247e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01, -8.6063e-02,  5.5818e-02,  6.1126e-01,  2.4074e+00,  4.2036e+00,  5.0820e+00,  5.6544e+00,  6.0791e+00,  6.0604e+00,  6.0416e+00,  6.0370e+00],\n",
      "        [ 1.3261e+00,  5.0585e-01,  1.9548e-01, -1.1488e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01, -1.9247e-01, -1.3336e-01, -5.4534e-02,  4.0731e-01,  2.0183e+00,  3.6292e+00,  4.6416e+00,  5.4544e+00,  6.0604e+00,  6.0456e+00,  6.0308e+00,  6.0271e+00]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get one normalized image from your batch\n",
    "normalized_img_tensor = samples[0, 1, 0, :, :] \n",
    "print(normalized_img_tensor.shape)\n",
    "\n",
    "# print(normalized_img_tensor)\n",
    "print(normalized_img_tensor[35:53,170:190])\n",
    "\n",
    "# Plot it\n",
    "plt.imshow(normalized_img_tensor.cpu().numpy(), cmap='gray')\n",
    "plt.title(\"Image Tensor as Seen by the Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f27a4e2-48b3-411c-a2e8-50d6a4727500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Thread(Thread-6 (plot_images), started daemon 22360422458944)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from util import box_ops\n",
    "import numpy as np\n",
    "\n",
    "ni = 5\n",
    "\n",
    "samples=None\n",
    "\n",
    "if samples is None:\n",
    "    samples, targets = get_batch_by_index(dataloader_test, ni)\n",
    "    original_video, targets_ = dataloader_test.dataset.dataset[ni]\n",
    "\n",
    "out, targets_flat = model(samples, targets)\n",
    "\n",
    "out_logits, pred_boxes = out['pred_logits'], out['pred_boxes']\n",
    "batch_size = out_logits.shape[0]\n",
    "\n",
    "if args.model == 'perceiver':\n",
    "    prob = F.softmax(out_logits, -1)\n",
    "elif args.model == 'YOLO':\n",
    "    prob = out_logits # confs instead\n",
    "\n",
    "scores, indices = prob[..., :].max(-1)\n",
    "\n",
    "keep = scores > 0.1\n",
    "\n",
    "# Prepare lists to store filtered results across the batch\n",
    "all_boxes_pixels = []\n",
    "all_scores = []\n",
    "all_classes = []\n",
    "all_batch_idx = []\n",
    "all_query_indices = [] # <-- ADD THIS LIST\n",
    "\n",
    "paths = []\n",
    "\n",
    "for i in range(batch_size): # frames\n",
    "    keep_batch = keep[i] # Boolean mask for queries for this image\n",
    "    paths.append(f'{args.model}_vd_{ni}_frm_{i}')\n",
    "    if keep_batch.any():\n",
    "        # Get filtered scores, classes, and boxes for this image\n",
    "        batch_scores = scores[i][keep_batch]\n",
    "        batch_classes = indices[i][keep_batch]\n",
    "        batch_boxes_norm = pred_boxes[i][keep_batch] # Normalized cxcywh\n",
    "\n",
    "        # Convert boxes to pixel xyxy\n",
    "        # batch_boxes_pixels = rescale_bboxes(batch_boxes_norm, img_size_wh)\n",
    "        batch_boxes_pixels = batch_boxes_norm# box_ops.box_cxcywh_to_xyxy(batch_boxes_norm)\n",
    "        # batch_boxes_pixels = batch_boxes_pixels * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32, device=batch_boxes_norm.device)\n",
    "\n",
    "        query_indices = torch.where(keep_batch)[0]\n",
    "        all_query_indices.append(query_indices)\n",
    "        \n",
    "        # Store results\n",
    "        all_boxes_pixels.append(batch_boxes_pixels)\n",
    "        all_scores.append(batch_scores)\n",
    "        all_classes.append(batch_classes)\n",
    "        # Add batch index for each kept box\n",
    "        all_batch_idx.append(torch.full_like(batch_scores, fill_value=i, dtype=torch.long))\n",
    "\n",
    "# Concatenate results from all images into single tensors\n",
    "if all_batch_idx: # Check if any boxes were kept\n",
    "    final_pred_boxes = torch.cat(all_boxes_pixels, dim=0)\n",
    "    final_pred_scores = torch.cat(all_scores, dim=0)\n",
    "    final_pred_classes = torch.cat(all_classes, dim=0)\n",
    "    final_pred_batch_idx = torch.cat(all_batch_idx, dim=0)\n",
    "    final_query_indices = torch.cat(all_query_indices, dim=0) # <-- ADD THIS LINE\n",
    "\n",
    "else:\n",
    "    # Handle case with no detections\n",
    "    final_pred_boxes = torch.empty((0, 4), device=samples.device)\n",
    "    final_pred_scores = torch.empty((0,), device=samples.device)\n",
    "    final_pred_classes = torch.empty((0,), dtype=torch.long, device=samples.device)\n",
    "    final_pred_batch_idx = torch.empty((0,), dtype=torch.long, device=samples.device)\n",
    "\n",
    "# --- 3. Prepare Images ---\n",
    "\n",
    "def denormalize_image(tensor, mean, std):\n",
    "    \"\"\"Denormalize a tensor image\"\"\"\n",
    "    # Clone to avoid modifying original tensor\n",
    "    tensor = tensor.clone()\n",
    "    mean = torch.tensor(mean, device=tensor.device).view(-1, 1, 1)\n",
    "    std = torch.tensor(std, device=tensor.device).view(-1, 1, 1)\n",
    "    tensor.mul_(std).add_(mean)\n",
    "    # Clamp and convert to uint8\n",
    "    #tensor = torch.clamp(tensor * 255.0, min=0.0, max=255.0).to(torch.uint8)\n",
    "    return tensor\n",
    "\n",
    "mmnist_stat = {\n",
    "    'perceiver': (0.030643088476670285, 0.15920598247588932), # medium dataset: mean, std\n",
    "    'YOLO': (0, 1), # mean, std\n",
    "}\n",
    "\n",
    "# Denormalize and convert B C H W -> B H W C\n",
    "images_to_plot = []\n",
    "samples_flat = samples.squeeze(0) # B T C H W -> T C H W\n",
    "for i in range(samples_flat.shape[0]):\n",
    "    img = denormalize_image(samples_flat[i], mmnist_stat[args.model][0], mmnist_stat[args.model][1]) # B C H W -> C H W (uint8)\n",
    "    images_to_plot.append(img.cpu().numpy()) # plot_images expects numpy HWC uint8\n",
    "\n",
    "# # plot_images expects a single array/tensor if plotting multiple images from a batch\n",
    "images_to_plot_batch = np.stack(images_to_plot) # Create batch H W C\n",
    "\n",
    "# 1. Generate the list of '{class}_{index}' strings for every detection.\n",
    "CLASS_NAMES = {k: str(k) for k in range(0,11)}\n",
    "custom_labels_list = []\n",
    "if final_pred_classes.numel() > 0:\n",
    "    custom_labels_list = [f'{CLASS_NAMES[c.item()]}_{q.item()}'\n",
    "                          for c, q in zip(final_pred_classes, final_query_indices)]\n",
    "\n",
    "# 2. Find the set of unique labels and create a stable mapping from the label string to a new, consistent integer ID.\n",
    "#    Sorting ensures the mapping is deterministic.\n",
    "unique_labels = sorted(list(set(custom_labels_list)))\n",
    "label_to_id_map = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "# 3. Create the final `cls` tensor for plotting. Each detection is assigned the\n",
    "#    consistent ID corresponding to its label string.\n",
    "#    For example, all '5_10' boxes will now have the same ID.\n",
    "custom_plot_cls = torch.tensor([label_to_id_map[label] for label in custom_labels_list],\n",
    "                               device=final_pred_classes.device)\n",
    "\n",
    "# 4. Create the `names` dictionary mapping the new consistent IDs back to the label strings.\n",
    "#    The plotting function uses this to get the text for the legend/box.\n",
    "custom_plot_names_dict = {i: label for i, label in enumerate(unique_labels)}\n",
    "\n",
    "\n",
    "# Plot Predictions\n",
    "plot_images(\n",
    "    images=images_to_plot_batch,\n",
    "    batch_idx=final_pred_batch_idx.cpu(),  # Index mapping box to image\n",
    "    # cls=final_pred_classes.cpu(),          # Class index for each box\n",
    "    cls=custom_plot_cls.cpu(),                 # <-- MODIFIED\n",
    "    names=custom_plot_names_dict,                   # <-- ADDED\n",
    "    bboxes=final_pred_boxes.detach().cpu(),         # Boxes in pixel xyxy format\n",
    "    confs=final_pred_scores.cpu(),      # Optional: Include scores on plot\n",
    "    fname=os.path.join(save_dir, f\"{args.model}_val_batch{ni}_pred.jpg\"),\n",
    "    paths=paths\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a85596a-d84a-444b-9f59-3f406ebb15e0",
   "metadata": {},
   "source": [
    "## FiftyOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfb7e69f-24c5-421d-b640-74b3f78b45eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1, 320, 320)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_to_plot_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8d24150-9a61-4672-81ba-1590cd6ed440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20 frames saved to: ../not_tracked_dir/fiftyone/frames_perceiver_5\n",
      " 100% || 20/20 [171.6ms elapsed, 0s remaining, 118.1 samples/s]    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 13:55:28,370 - INFO -  100% || 20/20 [171.6ms elapsed, 0s remaining, 118.1 samples/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FiftyOne dataset 'mmnist-predictions-perceiver-5' created with 20 samples.\n",
      "Directory '../not_tracked_dir/fiftyone/mmnist_perceiver_export_5' already exists; export will be merged with existing files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 13:55:28,375 - WARNING - Directory '../not_tracked_dir/fiftyone/mmnist_perceiver_export_5' already exists; export will be merged with existing files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 13:55:28,378 - INFO - Exporting samples...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% || 20/20 [47.2ms elapsed, 0s remaining, 423.6 docs/s] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 13:55:28,432 - INFO -  100% || 20/20 [47.2ms elapsed, 0s remaining, 423.6 docs/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Export complete! Dataset is ready in: '/gpfs/helios/home/ploter/projects/MultiSensorDropout/not_tracked_dir/fiftyone/mmnist_perceiver_export_5'\n",
      "\n",
      "To copy the dataset to your local machine, run:\n",
      "  scp -r <your-user>@<hpc-address>:/gpfs/helios/home/ploter/projects/MultiSensorDropout/not_tracked_dir/fiftyone/mmnist_perceiver_export_5 /path/to/your/local/folder/\n",
      "\n",
      " In the FiftyOne App, you can sort the view by 'filepath' to see frames in order.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fiftyone as fo\n",
    "import numpy as np\n",
    "import imageio  # For saving frames as images\n",
    "\n",
    "# --- 1. Setup Paths and Parameters ---\n",
    "# Define where to save the generated images and the exported dataset\n",
    "SAVE_DIR = \"../not_tracked_dir/fiftyone/\"\n",
    "# Create a subdirectory to hold the individual frames\n",
    "FRAMES_DIR = os.path.join(SAVE_DIR, f\"frames_{args.model}_{ni}\")\n",
    "EXPORT_DIR = os.path.join(SAVE_DIR, f\"mmnist_{args.model}_export_{ni}\")\n",
    "DATASET_NAME = f\"mmnist-predictions-{args.model}-{ni}\"\n",
    "\n",
    "os.makedirs(FRAMES_DIR, exist_ok=True)\n",
    "\n",
    "# Image dimensions (H, W) and frame count from your data\n",
    "IMG_H, IMG_W = 128, 128\n",
    "NUM_FRAMES = samples.shape[1]  # T from B, T, C, H, W\n",
    "\n",
    "# --- 2. Save Each Frame as a Separate Image File ---\n",
    "# Convert batch of images to the correct format for saving\n",
    "video_frames_for_save = np.transpose(images_to_plot_batch, (0, 2, 3, 1))\n",
    "\n",
    "if video_frames_for_save.shape[-1] == 1:\n",
    "    video_frames_for_save = np.squeeze(video_frames_for_save, axis=-1)\n",
    "\n",
    "video_frames_uint8 = np.clip(video_frames_for_save * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "# Loop through the frames and save each one, collecting their paths\n",
    "image_filepaths = []\n",
    "for i in range(NUM_FRAMES):\n",
    "    # Use zero-padding in the filename to ensure correct sorting\n",
    "    filepath = os.path.join(FRAMES_DIR, f\"frame_{i:04d}.png\")\n",
    "    imageio.imwrite(filepath, video_frames_uint8[i])\n",
    "    image_filepaths.append(filepath)\n",
    "\n",
    "print(f\" {NUM_FRAMES} frames saved to: {FRAMES_DIR}\")\n",
    "\n",
    "# --- 3. Create or Re-create FiftyOne Dataset ---\n",
    "# This ensures you start with a clean dataset for each run\n",
    "if fo.dataset_exists(DATASET_NAME):\n",
    "    fo.delete_dataset(DATASET_NAME)\n",
    "\n",
    "dataset = fo.Dataset(DATASET_NAME)\n",
    "dataset.persistent = True\n",
    "\n",
    "# --- 4. Define Bounding Box Conversion Utilities ---\n",
    "# (These helper functions are the same as before)\n",
    "def convert_pred_boxes_to_fo(boxes_cxcywh):\n",
    "    \"\"\"Converts normalized [cx, cy, w, h] to FiftyOne's [x, y, w, h] format.\"\"\"\n",
    "    cx, cy, w, h = boxes_cxcywh.T\n",
    "    return torch.stack([cx - w / 2, cy - h / 2, w, h], dim=1).detach().cpu().numpy()\n",
    "\n",
    "def convert_gt_boxes_to_fo(boxes_xyxy, img_w, img_h):\n",
    "    \"\"\"Converts pixel [x1, y1, x2, y2] to FiftyOne's normalized [x, y, w, h] format.\"\"\"\n",
    "    x1, y1, x2, y2 = boxes_xyxy.T\n",
    "    w = (x2 - x1) / img_w\n",
    "    h = (y2 - y1) / img_h\n",
    "    x = x1 / img_w\n",
    "    y = y1 / img_h\n",
    "    return torch.stack([x, y, w, h], dim=1).detach().cpu().numpy()\n",
    "\n",
    "# --- 5. Create FiftyOne Samples and Populate with Detections ---\n",
    "fiftyone_samples = []\n",
    "for i in range(NUM_FRAMES):\n",
    "    # Create one sample for each image file\n",
    "    sample = fo.Sample(filepath=image_filepaths[i])\n",
    "\n",
    "    # a) Add Ground Truth Detections to the sample\n",
    "    gt_targets = targets_flat[i]\n",
    "    \n",
    "    if gt_targets[\"boxes\"].numel() > 0:\n",
    "        fo_gt_boxes = convert_pred_boxes_to_fo(gt_targets[\"boxes\"])\n",
    "        sample[\"ground_truth\"] = fo.Detections(\n",
    "            detections=[\n",
    "                fo.Detection(label=CLASS_NAMES[label.item()], bounding_box=box)\n",
    "                for box, label in zip(fo_gt_boxes, gt_targets[\"labels\"])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # b) Add Model Predictions to the sample\n",
    "    frame_mask = (final_pred_batch_idx == i)\n",
    "    if frame_mask.any():\n",
    "        pred_boxes_cxcywh = final_pred_boxes[frame_mask]\n",
    "        fo_pred_boxes = convert_pred_boxes_to_fo(pred_boxes_cxcywh)\n",
    "        \n",
    "        pred_scores = final_pred_scores[frame_mask]\n",
    "        pred_classes = final_pred_classes[frame_mask]\n",
    "        pred_queries = final_query_indices[frame_mask]\n",
    "\n",
    "        sample[\"predictions\"] = fo.Detections(\n",
    "            detections=[\n",
    "                fo.Detection(\n",
    "                    label=CLASS_NAMES[cls.item()],\n",
    "                    bounding_box=box,\n",
    "                    confidence=score.item(),\n",
    "                    query_id=query.item()\n",
    "                )\n",
    "                for box, score, cls, query in zip(fo_pred_boxes, pred_scores, pred_classes, pred_queries)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    fiftyone_samples.append(sample)\n",
    "\n",
    "# --- 6. Add Samples to Dataset and Export ---\n",
    "dataset.add_samples(fiftyone_samples)\n",
    "dataset.save()\n",
    "print(f\" FiftyOne dataset '{DATASET_NAME}' created with {len(dataset)} samples.\")\n",
    "\n",
    "# Export the dataset. The default type `fo.types.FiftyOneDataset` is for images.\n",
    "dataset.export(\n",
    "    export_dir=EXPORT_DIR,\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    "    export_media=True\n",
    ")\n",
    "\n",
    "print(f\"\\n Export complete! Dataset is ready in: '{os.path.abspath(EXPORT_DIR)}'\")\n",
    "print(\"\\nTo copy the dataset to your local machine, run:\")\n",
    "print(f\"  scp -r <your-user>@<hpc-address>:{os.path.abspath(EXPORT_DIR)} /path/to/your/local/folder/\")\n",
    "print(\"\\n In the FiftyOne App, you can sort the view by 'filepath' to see frames in order.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "287e0016-beec-4179-9c39-52166f9c40af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging the first frame before saving...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(\"Debugging the first frame before saving...\")\n",
    "plt.imshow(video_frames_for_save[1], cmap='gray')\n",
    "plt.title(\"Is this image correct?\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4feb1b26-391f-423e-921a-24924abea42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output will be saved in: /gpfs/helios/home/ploter/projects/MultiSensorDropout/not_tracked_dir/fiftyone/mmnist-test\n",
      "Loading 'Max-Ploter/detection-moving-mnist-medium' test split...\n",
      "Warning: Class names not found in dataset features. Using integer labels.\n",
      "Deleted existing dataset: 'detection-moving-mnist-medium-test-split'\n",
      "Created new FiftyOne dataset: 'detection-moving-mnist-medium-test-split'\n",
      "Processing 100 videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting Videos:  47%|     | 47/100 [00:02<00:03, 17.28it/s]Exception ignored in: <function PluginV3.__del__ at 0x14556fafa340>\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/helios/home/ploter/.conda/envs/sensor_dropout/lib/python3.11/site-packages/imageio/core/v3_plugin_api.py\", line 370, in __del__\n",
      "    self.close()\n",
      "  File \"/gpfs/helios/home/ploter/.conda/envs/sensor_dropout/lib/python3.11/site-packages/imageio/plugins/pillow.py\", line 144, in close\n",
      "    self._flush_writer()\n",
      "  File \"/gpfs/helios/home/ploter/.conda/envs/sensor_dropout/lib/python3.11/site-packages/imageio/plugins/pillow.py\", line 476, in _flush_writer\n",
      "    if len(self.images_to_write) == 0:\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'PillowPlugin' object has no attribute 'images_to_write'\n",
      "Converting Videos: 100%|| 100/100 [00:06<00:00, 15.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding 2000 frame samples to the dataset...\n",
      "   0% ||--------------|    1/2000 [30.8ms elapsed, 1.0m remaining, 32.5 samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% || 2000/2000 [3.7s elapsed, 0s remaining, 508.6 samples/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 14:10:35,124 - INFO -  100% || 2000/2000 [3.7s elapsed, 0s remaining, 508.6 samples/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FiftyOne dataset created successfully.\n",
      "Exporting dataset to: ../not_tracked_dir/fiftyone/mmnist-test/fiftyone_export\n",
      "Directory '../not_tracked_dir/fiftyone/mmnist-test/fiftyone_export' already exists; export will be merged with existing files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 14:10:35,128 - WARNING - Directory '../not_tracked_dir/fiftyone/mmnist-test/fiftyone_export' already exists; export will be merged with existing files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 14:10:35,130 - INFO - Exporting samples...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% || 2000/2000 [1.5s elapsed, 0s remaining, 1.4K docs/s]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 14:10:36,684 - INFO -  100% || 2000/2000 [1.5s elapsed, 0s remaining, 1.4K docs/s]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Export complete!\n",
      "You can now load the dataset in FiftyOne with:\n",
      "\n",
      "  import fiftyone as fo\n",
      "  dataset = fo.load_dataset('detection-moving-mnist-medium-test-split')\n",
      "  session = fo.launch_app(dataset)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import fiftyone as fo\n",
    "import numpy as np\n",
    "import torch\n",
    "import imageio\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. Helper Function for Bounding Box Conversion ---\n",
    "\n",
    "def convert_gt_boxes_to_fo(boxes_xyxy, img_w, img_h):\n",
    "    \"\"\"\n",
    "    Converts ground truth bounding boxes from pixel [x1, y1, x2, y2] format\n",
    "    to FiftyOne's required normalized [x_min, y_min, width, height] format.\n",
    "\n",
    "    Args:\n",
    "        boxes_xyxy (torch.Tensor): A tensor of shape (N, 4) with boxes in\n",
    "                                   [x_min, y_min, x_max, y_max] pixel format.\n",
    "        img_w (int): The width of the image.\n",
    "        img_h (int): The height of the image.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Bounding boxes in FiftyOne's format.\n",
    "    \"\"\"\n",
    "    if not isinstance(boxes_xyxy, torch.Tensor):\n",
    "        boxes_xyxy = torch.tensor(boxes_xyxy, dtype=torch.float32)\n",
    "\n",
    "    if boxes_xyxy.numel() == 0:\n",
    "        return np.array([])\n",
    "\n",
    "    x1, y1, x2, y2 = boxes_xyxy.T\n",
    "    \n",
    "    # Normalize and calculate width/height\n",
    "    w = (x2 - x1) / img_w\n",
    "    h = (y2 - y1) / img_h\n",
    "    x = x1 / img_w\n",
    "    y = y1 / img_h\n",
    "    \n",
    "    # Stack into [x_min, y_min, width, height] format and convert to numpy\n",
    "    return torch.stack([x, y, w, h], dim=1).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "# --- Setup Paths ---\n",
    "# Base directory for all outputs\n",
    "BASE_OUTPUT_DIR = \"../not_tracked_dir/fiftyone/mmnist-test/\"\n",
    "# Directory to save individual frames\n",
    "FRAMES_DIR = os.path.join(BASE_OUTPUT_DIR, \"frames\")\n",
    "# Directory to save the final FiftyOne dataset\n",
    "EXPORT_DIR = os.path.join(BASE_OUTPUT_DIR, \"fiftyone_export\")\n",
    "# Name for the FiftyOne dataset\n",
    "DATASET_NAME = f\"{os.path.basename(args.dataset_path)}-test-split\"\n",
    "\n",
    "os.makedirs(FRAMES_DIR, exist_ok=True)\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "print(f\"Output will be saved in: {os.path.abspath(BASE_OUTPUT_DIR)}\")\n",
    "\n",
    "# --- Load Hugging Face Dataset ---\n",
    "print(f\"Loading '{args.dataset_path}' test split...\")\n",
    "hf_dataset = load_dataset(args.dataset_path, split='test').select(range(100))\n",
    "\n",
    "# Attempt to get class names from dataset features\n",
    "try:\n",
    "    class_names = hf_dataset.features[\"bboxes_labels\"].feature.names\n",
    "    print(f\"Found class names: {class_names}\")\n",
    "except (AttributeError, KeyError):\n",
    "    print(\"Warning: Class names not found in dataset features. Using integer labels.\")\n",
    "    class_names = None\n",
    "\n",
    "# --- Create or Re-create FiftyOne Dataset ---\n",
    "# This ensures a clean start for each run\n",
    "if fo.dataset_exists(DATASET_NAME):\n",
    "    fo.delete_dataset(DATASET_NAME)\n",
    "    print(f\"Deleted existing dataset: '{DATASET_NAME}'\")\n",
    "\n",
    "dataset = fo.Dataset(DATASET_NAME)\n",
    "dataset.persistent = True\n",
    "print(f\"Created new FiftyOne dataset: '{DATASET_NAME}'\")\n",
    "\n",
    "# --- Process Each Video in the Dataset ---\n",
    "all_fo_samples = []\n",
    "\n",
    "print(f\"Processing {len(hf_dataset)} videos...\")\n",
    "for video_idx, sample in enumerate(tqdm(hf_dataset, desc=\"Converting Videos\")):\n",
    "    # Assumes video is a list of PIL Images or a numpy array (T, H, W, C)\n",
    "    video_frames = np.array(sample['video'])\n",
    "    gt_bboxes = sample['bboxes']          # List of lists of bboxes per frame\n",
    "    gt_labels = sample['bboxes_labels']   # List of lists of labels per frame\n",
    "\n",
    "    num_frames, img_h, img_w = video_frames.shape[:3]\n",
    "    \n",
    "    # Create a subdirectory for this video's frames\n",
    "    video_frame_dir = os.path.join(FRAMES_DIR, f\"video_{video_idx:04d}\")\n",
    "    os.makedirs(video_frame_dir, exist_ok=True)\n",
    "    \n",
    "    for frame_idx in range(num_frames):\n",
    "        # --- a. Save Frame as Image ---\n",
    "        frame_image = video_frames[frame_idx]\n",
    "        filepath = os.path.join(video_frame_dir, f\"frame_{frame_idx:04d}.png\")\n",
    "        \n",
    "        # Ensure image is in uint8 format for saving\n",
    "        if frame_image.dtype != np.uint8:\n",
    "             frame_image = np.clip(frame_image, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        imageio.imwrite(filepath, frame_image)\n",
    "        \n",
    "        # --- b. Create FiftyOne Sample ---\n",
    "        fo_sample = fo.Sample(filepath=filepath)\n",
    "        \n",
    "        # Add video index and frame number for easy reference\n",
    "        fo_sample[\"video_id\"] = video_idx\n",
    "        fo_sample[\"frame_number\"] = frame_idx\n",
    "        \n",
    "        # --- c. Add Ground Truth Detections ---\n",
    "        frame_boxes = gt_bboxes[frame_idx]\n",
    "        frame_labels = gt_labels[frame_idx]\n",
    "        \n",
    "        if frame_boxes and len(frame_boxes) > 0:\n",
    "            # Convert boxes to FiftyOne format\n",
    "            fo_gt_boxes = convert_gt_boxes_to_fo(frame_boxes, img_w, img_h)\n",
    "            \n",
    "            # Create a list of fo.Detection objects\n",
    "            detections = []\n",
    "            for box, label_id in zip(fo_gt_boxes, frame_labels):\n",
    "                label_str = class_names[label_id] if class_names else str(label_id)\n",
    "                detections.append(\n",
    "                    fo.Detection(label=label_str, bounding_box=box)\n",
    "                )\n",
    "            \n",
    "            # Attach the detections to the sample in a \"ground_truth\" field\n",
    "            fo_sample[\"ground_truth\"] = fo.Detections(detections=detections)\n",
    "            \n",
    "        all_fo_samples.append(fo_sample)\n",
    "\n",
    "# --- 4. Add All Samples and Export ---\n",
    "print(f\"\\nAdding {len(all_fo_samples)} frame samples to the dataset...\")\n",
    "dataset.add_samples(all_fo_samples)\n",
    "dataset.save()\n",
    "print(\" FiftyOne dataset created successfully.\")\n",
    "\n",
    "print(f\"Exporting dataset to: {EXPORT_DIR}\")\n",
    "dataset.export(\n",
    "    export_dir=EXPORT_DIR,\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    "    export_media=True, # Copies the saved frame images into the export directory\n",
    ")\n",
    "\n",
    "print(\"\\n Export complete!\")\n",
    "print(f\"You can now load the dataset in FiftyOne with:\\n\")\n",
    "print(f\"  import fiftyone as fo\")\n",
    "print(f\"  dataset = fo.load_dataset('{DATASET_NAME}')\")\n",
    "print(f\"  session = fo.launch_app(dataset)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9449e22-1e5b-47b0-896c-c3f6ebe3dddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensor_dropout",
   "language": "python",
   "name": "sensor_dropout"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
