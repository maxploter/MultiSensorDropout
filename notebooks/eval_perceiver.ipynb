{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13192b60-b5f1-443d-a672-f857e0541af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('/gpfs/helios/home/ploter/projects/MultiSensorDropout/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cac28c36-f0e5-4227-a054-72da44cd974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    # Basic training parameters\n",
    "    seed=42,\n",
    "    batch_size=1,\n",
    "    epochs=18,\n",
    "    learning_rate=1e-3,\n",
    "    learning_rate_backbone=1e-4,\n",
    "    learning_rate_backbone_names=[\"backbone\"],\n",
    "    weight_decay=0.01,\n",
    "    scheduler_step_size=12,\n",
    "    eval_interval=1,\n",
    "    patience=5,\n",
    "    model='perceiver',\n",
    "    backbone='cnn',  # Set specific value for evaluation\n",
    "    eval=True,\n",
    "    weight_loss_center_point=5,\n",
    "    weight_loss_bce=1,\n",
    "    shuffle_views=False,\n",
    "    object_detection=True,  # Set to True for object detection\n",
    "    \n",
    "    # Matcher parameters\n",
    "    set_cost_class=1,\n",
    "    set_cost_bbox=5,\n",
    "    set_cost_giou=2,\n",
    "    focal_alpha=0.25,\n",
    "    focal_gamma=2,\n",
    "    \n",
    "    # Loss coefficients\n",
    "    bbox_loss_coef=5,\n",
    "    giou_loss_coef=2,\n",
    "    eos_coef=0.1,\n",
    "    \n",
    "    # Checkpoint parameters\n",
    "    resume='checkpoint_epoch_17.pth',  # Replace with actual checkpoint path\n",
    "    output_dir=\"./output\",  # Provide a default output directory\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # Dataset parameters\n",
    "    dataset='moving-mnist',\n",
    "    dataset_path='Max-Ploter/detection-moving-mnist-easy',\n",
    "    generate_dataset_runtime=False,\n",
    "    num_workers=4,\n",
    "    num_frames=20,\n",
    "    train_dataset_fraction=1.0,\n",
    "    train_dataset_size=1.0,  # Additional parameter from notebook\n",
    "    test_dataset_fraction=1.0,\n",
    "    frame_dropout_pattern=None,\n",
    "    view_dropout_probs=[],#[0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85],\n",
    "    sampler_steps=[], #[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
    "    sequential_sampler=False,\n",
    "    grid_size=(1, 1),\n",
    "    tile_overlap=0.0,\n",
    "    \n",
    "    # Wandb parameters\n",
    "    wandb_project='multi-sensor-dropout',\n",
    "    wandb_id=None,\n",
    "    \n",
    "    # Perceiver model parameters\n",
    "    num_freq_bands=6,  # Set to 4 as in the notebook\n",
    "    max_freq=10, \n",
    "    enc_layers=1,\n",
    "    num_queries=16,  # Set to 16 as in the notebook\n",
    "    hidden_dim=128,\n",
    "    enc_nheads_cross=1,\n",
    "    nheads=1,\n",
    "    dropout=0.0,\n",
    "    self_per_cross_attn=1,\n",
    "    multi_classification_heads=False,\n",
    "    \n",
    "    # LSTM model parameters\n",
    "    lstm_hidden_size=128,\n",
    "    \n",
    "    # Additional parameters for complete compatibility\n",
    "    focal_loss=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2185130c-1eff-4867-8213-fefb17f6e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyCriterion:\n",
    "    def __init__(self, device):\n",
    "        # Initialize with the necessary attributes\n",
    "        self.device = device\n",
    "        self.weight_dict = {'loss_ce': 1}\n",
    "        \n",
    "    def __call__(self, outputs, targets, *args, **kwargs):\n",
    "        # Return a dictionary with zero losses to maintain the expected interface\n",
    "        return {\n",
    "            'loss_ce': torch.tensor(0.0, device=self.device),\n",
    "            'loss_bbox': torch.tensor(0.0, device=self.device),\n",
    "            'loss_giou': torch.tensor(0.0, device=self.device),\n",
    "            'loss': torch.tensor(0.0, device=self.device)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45f8ec69-480d-4b0c-b8ec-c263ea52cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class PostProcessTopK(nn.Module):\n",
    "    \"\"\" Wrapper that applies a post-processor and keeps only the top-k predictions by score and a score threshold \"\"\"\n",
    "\n",
    "    def __init__(self, post_processor, top_k=10, score_threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.post_processor = post_processor\n",
    "        self.top_k = top_k\n",
    "        self.score_threshold = score_threshold\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, target_sizes):\n",
    "        # Get results from the original post processor\n",
    "        results = self.post_processor(outputs, target_sizes)\n",
    "\n",
    "        # Filter to keep only top-k results and results above the score threshold\n",
    "        filtered_results = []\n",
    "        for result in results:\n",
    "            scores, labels, boxes = result['scores'], result['labels'], result['boxes']\n",
    "\n",
    "            # Get top-k indices by score\n",
    "            top_k = min(self.top_k, len(scores))\n",
    "            if top_k > 0:  # Check if there are any predictions\n",
    "                top_indices = torch.topk(scores, top_k).indices\n",
    "\n",
    "                # Filter by top-k indices\n",
    "                top_scores = scores[top_indices]\n",
    "                top_labels = labels[top_indices]\n",
    "                top_boxes = boxes[top_indices]\n",
    "\n",
    "                # Filter by score threshold\n",
    "                threshold_indices = top_scores >= self.score_threshold\n",
    "                final_scores = top_scores[threshold_indices]\n",
    "                final_labels = top_labels[threshold_indices]\n",
    "                final_boxes = top_boxes[threshold_indices]\n",
    "\n",
    "                filtered_results.append({'scores': final_scores, 'labels': final_labels, 'boxes': final_boxes})\n",
    "            else:\n",
    "                # Keep empty result if no predictions\n",
    "                filtered_results.append({'scores': torch.empty(0), 'labels': torch.empty(0, dtype=torch.int64), 'boxes': torch.empty(0, 4)})\n",
    "\n",
    "        return filtered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "206fe301-b091-4e49-a2f6-e460bc7a05dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test huggingface MovingMNIST dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3882979/1649666546.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=args.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using object detection mode\n",
      "Transforms: Compose(\n",
      "    <dataset.transformations.NormBoxesTransform object at 0x14a82f499f90>\n",
      ")\n",
      "num_freq_bands: 6\n",
      "depth: 1\n",
      "max_freq: 10\n",
      "input_channels: 32\n",
      "input_axis: 2\n",
      "num_latents: 16\n",
      "latent_dim: 128\n",
      "cross_heads: 1\n",
      "latent_heads: 1\n",
      "cross_dim_head: 58\n",
      "latent_dim_head: 128\n",
      "num_classes: -1\n",
      "attn_dropout: 0.0\n",
      "ff_dropout: 0.0\n",
      "weight_tie_layers: False\n",
      "fourier_encode_data: True\n",
      "self_per_cross_attn: 1\n",
      "final_classifier_head: False\n",
      "num_sensors: 1\n",
      "__class__: <class 'models.perceiver.Perceiver'>\n",
      "Loading checkpoint from ../not_tracked_dir/output_perceiver_detection_2025-04-14_09-56-40/checkpoint_epoch_17.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval -1:: 100%|██████████| 10000/10000 [14:22<00:00, 11.59it/s, loss_running=0, class_error_running=nan, loss_center_point_running=nan, loss_ce_running=0, loss=0, loss_ce_unscaled=0, loss_bbox_unscaled=0, loss_giou_unscaled=0, loss_unscaled=0, loss_ce=0, view_dropout_prob=-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw mAP Results: {'map': tensor(0.6828), 'map_50': tensor(0.8640), 'map_75': tensor(0.8098), 'map_small': tensor(0.6828), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.5995), 'mar_10': tensor(0.7301), 'mar_100': tensor(0.7301), 'mar_small': tensor(0.7301), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
      "\n",
      "Evaluation Results:\n",
      "loss_running: 0.0000\n",
      "class_error_running: nan\n",
      "loss_center_point_running: nan\n",
      "loss_ce_running: 0.0000\n",
      "loss: 0.0000\n",
      "loss_ce_unscaled: 0.0000\n",
      "loss_bbox_unscaled: 0.0000\n",
      "loss_giou_unscaled: 0.0000\n",
      "loss_unscaled: 0.0000\n",
      "loss_ce: 0.0000\n",
      "mAP_map: 0.6828\n",
      "mAP_map_50: 0.8640\n",
      "mAP_map_75: 0.8098\n",
      "mAP_map_small: 0.6828\n",
      "mAP_map_medium: -1.0000\n",
      "mAP_map_large: -1.0000\n",
      "mAP_mar_1: 0.5995\n",
      "mAP_mar_10: 0.7301\n",
      "mAP_mar_100: 0.7301\n",
      "mAP_mar_small: 0.7301\n",
      "mAP_mar_medium: -1.0000\n",
      "mAP_mar_large: -1.0000\n",
      "mAP_map_per_class: -1.0000\n",
      "mAP_mar_100_per_class: -1.0000\n"
     ]
    }
   ],
   "source": [
    "from engine import evaluate\n",
    "from dataset import build_dataset\n",
    "import torch\n",
    "from engine import evaluate\n",
    "from dataset import build_dataset\n",
    "from models import build_model\n",
    "from models.perceiver import PostProcess\n",
    "import os\n",
    "from pathlib import Path\n",
    "from util.misc import collate_fn, is_main_process, get_sha, get_rank\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "args.resume = 'checkpoint_epoch_17.pth'\n",
    "args.output_dir = \"../not_tracked_dir/output_perceiver_detection_2025-04-14_09-56-40\"\n",
    "args.test_dataset_fraction = 1\n",
    "\n",
    "checkpoint_path = os.path.join(args.output_dir, args.resume)\n",
    "\n",
    "dataset_test = build_dataset(split='test', args=args)\n",
    "\n",
    "sampler_test = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "dataloader_test = DataLoader(dataset_test, sampler=sampler_test, batch_size=args.batch_size,\n",
    "                                collate_fn=collate_fn, num_workers=args.num_workers, pin_memory=True)\n",
    "\n",
    "model = build_model(args, dataset_test.input_image_view_size)\n",
    "\n",
    "device = torch.device(args.device)\n",
    "postprocessors = {\n",
    "    'bbox': PostProcessTopK(PostProcess())\n",
    "}\n",
    "\n",
    "criterion = DummyCriterion(device)\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "\tprint(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "\tcheckpoint = torch.load(checkpoint_path, map_location=args.device)\n",
    "\tmodel.load_state_dict(checkpoint['model'])\n",
    "else:\n",
    "\traise FileNotFoundError(f\"Checkpoint not found at {checkpoint_path}\")\n",
    "\n",
    "model.to(args.device)\n",
    "\n",
    "results = evaluate(\n",
    "\tmodel=model,\n",
    "\tdataloader=dataloader_test,\n",
    "\tcriterion=criterion,  # Not needed for evaluation\n",
    "\tpostprocessors=postprocessors,\n",
    "\tepoch=-1,\n",
    "\tdevice=device,\n",
    ")\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "for metric_name, value in results.items():\n",
    "\tprint(f\"{metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9535900e-5d53-4331-8a02-c0c165a22b57",
   "metadata": {},
   "source": [
    "## RESULT\n",
    "\n",
    "Raw mAP Results: {'map': tensor(0.7331), 'map_50': tensor(0.9379), 'map_75': tensor(0.8674), 'map_small': tensor(0.7331), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.6338), 'mar_10': tensor(0.7885), 'mar_100': tensor(0.7885), 'mar_small': tensor(0.7885), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "Evaluation Results:\n",
    "loss_running: 0.0000\n",
    "class_error_running: nan\n",
    "loss_center_point_running: nan\n",
    "loss_ce_running: 0.0000\n",
    "loss: 0.0000\n",
    "loss_ce_unscaled: 0.0000\n",
    "loss_bbox_unscaled: 0.0000\n",
    "loss_giou_unscaled: 0.0000\n",
    "loss_unscaled: 0.0000\n",
    "loss_ce: 0.0000\n",
    "mAP_map: 0.7331\n",
    "mAP_map_50: 0.9379\n",
    "mAP_map_75: 0.8674\n",
    "mAP_map_small: 0.7331\n",
    "mAP_map_medium: -1.0000\n",
    "mAP_map_large: -1.0000\n",
    "mAP_mar_1: 0.6338\n",
    "mAP_mar_10: 0.7885\n",
    "mAP_mar_100: 0.7885\n",
    "mAP_mar_small: 0.7885\n",
    "mAP_mar_medium: -1.0000\n",
    "mAP_mar_large: -1.0000\n",
    "mAP_map_per_class: -1.0000\n",
    "mAP_mar_100_per_class: -1.0000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b464e-fad7-4efd-bb4a-8316b48e3f58",
   "metadata": {},
   "source": [
    "## Result\n",
    "top 10 and threashold 0.5\n",
    "\n",
    "Raw mAP Results: {'map': tensor(0.6828), 'map_50': tensor(0.8640), 'map_75': tensor(0.8098), 'map_small': tensor(0.6828), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.5995), 'mar_10': tensor(0.7301), 'mar_100': tensor(0.7301), 'mar_small': tensor(0.7301), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}\n",
    "\n",
    "Evaluation Results:\n",
    "loss_running: 0.0000\n",
    "class_error_running: nan\n",
    "loss_center_point_running: nan\n",
    "loss_ce_running: 0.0000\n",
    "loss: 0.0000\n",
    "loss_ce_unscaled: 0.0000\n",
    "loss_bbox_unscaled: 0.0000\n",
    "loss_giou_unscaled: 0.0000\n",
    "loss_unscaled: 0.0000\n",
    "loss_ce: 0.0000\n",
    "mAP_map: 0.6828\n",
    "mAP_map_50: 0.8640\n",
    "mAP_map_75: 0.8098\n",
    "mAP_map_small: 0.6828\n",
    "mAP_map_medium: -1.0000\n",
    "mAP_map_large: -1.0000\n",
    "mAP_mar_1: 0.5995\n",
    "mAP_mar_10: 0.7301\n",
    "mAP_mar_100: 0.7301\n",
    "mAP_mar_small: 0.7301\n",
    "mAP_mar_medium: -1.0000\n",
    "mAP_mar_large: -1.0000\n",
    "mAP_map_per_class: -1.0000\n",
    "mAP_mar_100_per_class: -1.0000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensor_dropout",
   "language": "python",
   "name": "sensor_dropout"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
