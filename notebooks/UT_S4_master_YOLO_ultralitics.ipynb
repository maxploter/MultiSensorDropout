{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-14T11:52:18.478270Z"
    },
    "id": "hfk7RLMWXByq",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('/gpfs/helios/home/ploter/projects/MultiSensorDropout/')\n",
    "\n",
    "from engine import evaluate\n",
    "\n",
    "GDRIVE_BASE_PATH = '../not_tracked_dir/'\n",
    "os.makedirs(GDRIVE_BASE_PATH, exist_ok=True)\n",
    "CFG_EXPERIMENT_NAME = \"output_yolo_v8_2025-04-11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vqsZ1W76XEzP",
    "outputId": "b65f2d03-7a6f-4e10-d26d-e85f4c9d7e6b"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# import os\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # Define the base directory ON YOUR GOOGLE DRIVE where projects should be saved\n",
    "# # IMPORTANT: Make sure this path exists in your Google Drive or create it.\n",
    "# # Example: Create a folder named 'YOLOv8_Training' in the root of your 'MyDrive'\n",
    "# GDRIVE_BASE_PATH = '/content/drive/MyDrive/YOLOv8_Training' # ADJUST THIS PATH AS NEEDED\n",
    "\n",
    "# # Create the base directory on Drive if it doesn't exist\n",
    "# os.makedirs(GDRIVE_BASE_PATH, exist_ok=True)\n",
    "# print(f\"Google Drive mounted. Using base path: {GDRIVE_BASE_PATH}\")\n",
    "# CFG_EXPERIMENT_NAME = \"train_yolo_nano_24k_dataset3\"\n",
    "\n",
    "# !pip install -q ultralytics\n",
    "# !pip install -q datasets\n",
    "# !pip install -q torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCRptqO7HUjG",
    "outputId": "1b49ebe7-0e06-4815-bda4-aec4942dbee3"
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms.functional import resize, to_pil_image, to_tensor\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from types import SimpleNamespace # For config object\n",
    "\n",
    "# Ultralytics imports\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.models.yolo.detect.train import DetectionTrainer # To subclass\n",
    "from ultralytics.utils import ops, checks\n",
    "\n",
    "\n",
    "# --- NEW: Import Ultralytics LetterBox ---\n",
    "try:\n",
    "    # Common location for augmentations\n",
    "    from ultralytics.data.augment import LetterBox\n",
    "except ImportError:\n",
    "    try:\n",
    "        # Alternative location in some versions might be utils\n",
    "        from ultralytics.utils.ops import LetterBox\n",
    "        print(\"Note: Imported LetterBox from ultralytics.utils.ops\")\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Could not import LetterBox from 'ultralytics.data.augment' or 'ultralytics.utils.ops'. \"\n",
    "                          \"Check Ultralytics version or installation.\")\n",
    "# ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRKezob-4En2"
   },
   "source": [
    "## DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LyHdRjMPNuCU"
   },
   "outputs": [],
   "source": [
    "# === In Cell 2: HFYOLODataset Class Definition ===\n",
    "\n",
    "# === Cell 2: HFYOLODataset Class Definition ===\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "# --- Ultralytics Imports ---\n",
    "# Need YOLODataset only for accessing its collate_fn later\n",
    "from ultralytics.data import YOLODataset\n",
    "# Import necessary transform components used in build_transforms\n",
    "from ultralytics.data.augment import LetterBox, Format, Compose # Standard components\n",
    "# v8_transforms might require many more internal imports if used for augmentation\n",
    "# from ultralytics.data.augment import v8_transforms\n",
    "from ultralytics.utils import DEFAULT_CFG, LOGGER # Used in build_transforms logic\n",
    "from ultralytics.utils.instance import Instances # Make sure this is imported\n",
    "\n",
    "\n",
    "class HFYOLODataset(Dataset):\n",
    "\n",
    "    # ... (Keep __init__ and _build_transforms_internal as before) ...\n",
    "    def __init__(self, hf_dataset_split, imgsz=640, stride=32, augment=True, hyp=None, rect=False, task='detect', trust_remote_code=False, prefix=\"\"):\n",
    "        # ... (Same init as previous step) ...\n",
    "        super().__init__() # Base Dataset init\n",
    "        self.imgsz = imgsz; self.augment = augment; self.hyp = hyp if hyp is not None else DEFAULT_CFG; self.rect = rect; self.stride = stride; self.task = task; self.prefix = prefix; self.hf_dataset = hf_dataset_split; self.trust_remote_code = trust_remote_code\n",
    "        self.use_segments = self.task == \"segment\"; self.use_keypoints = self.task == \"pose\"; self.use_obb = self.task == \"obb\"\n",
    "        print(f\"Initializing HFYOLODataset (Using Ultralytics Transforms)...\")\n",
    "        # Determine frame info & nc/names\n",
    "        self.num_videos = len(self.hf_dataset); # ... (rest of frame count/dim logic) ...\n",
    "        if self.num_videos == 0: raise ValueError(\"Empty dataset.\")\n",
    "        try: # Determine frame info\n",
    "            first_vid = self.hf_dataset[0]; first_data = first_vid['video']; # ... get first_frames_shape_approx ...\n",
    "            if not isinstance(first_data, np.ndarray):\n",
    "                 if isinstance(first_data, list) and first_data: first_frame_np = np.array(first_data[0]); first_video_frames_shape_approx = (len(first_data), *first_frame_np.shape)\n",
    "                 else: first_video_frames = np.array(first_data); first_video_frames_shape_approx = first_video_frames.shape\n",
    "            else: first_video_frames_shape_approx = first_data.shape\n",
    "            if len(first_video_frames_shape_approx) == 4: self.num_frames_per_video, self.frame_height, self.frame_width, _ = first_video_frames_shape_approx\n",
    "            elif len(first_video_frames_shape_approx) == 3: self.num_frames_per_video, self.frame_height, self.frame_width = first_video_frames_shape_approx\n",
    "            else: raise ValueError(f\"Unexpected video dimensions: {first_video_frames_shape_approx}\")\n",
    "            if self.num_frames_per_video <= 0: raise ValueError(f\"Non-positive frame count: {self.num_frames_per_video}\")\n",
    "            print(f\"Frames/Video: {self.num_frames_per_video}, Dim: ({self.frame_height}, {self.frame_width})\")\n",
    "        except Exception as e: raise RuntimeError(f\"Failed getting frame info: {e}\") from e\n",
    "        self.total_frames = self.num_videos * self.num_frames_per_video; print(f\"Total train frames: {self.total_frames}\")\n",
    "\n",
    "        self.nc = 10\n",
    "        self.num_classes = self.nc\n",
    "        self.names = {i:str(i) for i in range(self.nc)}\n",
    "        print(f\"Discovered nc={self.nc}, names={self.names}\")\n",
    "        self.data = {'names': self.names, 'nc': self.nc}\n",
    "        # Build Transforms\n",
    "        self.transforms = self._build_transforms_internal(hyp=self.hyp)\n",
    "        print(f\"Transforms created: {self.transforms}\")\n",
    "\n",
    "    def _build_transforms_internal(self, hyp):\n",
    "        # ... (Same as before, includes LetterBox and Format) ...\n",
    "         if self.augment:\n",
    "             LOGGER.warning(\"Augmentation enabled, but using simple LetterBox transform.\")\n",
    "             transforms = Compose([LetterBox(new_shape=(self.imgsz, self.imgsz), scaleup=getattr(hyp, 'scaleup', True), stride=self.stride)])\n",
    "         else:\n",
    "             transforms = Compose([LetterBox(new_shape=(self.imgsz, self.imgsz), scaleup=False, stride=self.stride)])\n",
    "         transforms.append(\n",
    "             Format(bbox_format=\"xywh\",\n",
    "                    normalize=True, return_mask=self.use_segments, return_keypoint=self.use_keypoints, return_obb=self.use_obb, batch_idx=True, mask_ratio=getattr(hyp, 'mask_ratio', 4), mask_overlap=getattr(hyp, 'overlap_mask', True), bgr=getattr(hyp, 'bgr', 0.0) if self.augment else 0.0))\n",
    "         return transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_frames\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index >= self.total_frames: raise IndexError(...)\n",
    "        video_idx = index // self.num_frames_per_video\n",
    "        frame_idx = index % self.num_frames_per_video\n",
    "        try:\n",
    "            video_example = self.hf_dataset[video_idx]\n",
    "            # --- Load raw image (uint8 HWC NumPy) ---\n",
    "            # ... (same logic) ...\n",
    "            video_data = video_example['video']; img_np = np.array(video_data[frame_idx], dtype=np.uint8) if isinstance(video_data, list) else video_data[frame_idx].astype(np.uint8)\n",
    "            if img_np.ndim == 2: img_np = np.stack([img_np]*3, axis=-1)\n",
    "            elif img_np.shape[-1] == 1: img_np = np.concatenate([img_np]*3, axis=-1)\n",
    "            original_h, original_w = img_np.shape[:2]\n",
    "\n",
    "            # --- Load annotations -> ABSOLUTE PIXEL COORDS (xyxy) ---\n",
    "            bboxes = video_example['bboxes'][frame_idx]\n",
    "            labels = video_example['bboxes_labels'][frame_idx]\n",
    "            bboxes_abs_xyxy_list = []\n",
    "            cls_list_of_lists = [] # <<< Change variable name for clarity\n",
    "            if bboxes and labels:\n",
    "                 for bbox, label in zip(bboxes, labels):\n",
    "                    x_min, y_min, w, h = map(float, bbox) # Ensure float\n",
    "                    # Convert xywh to xyxy, clamp to image bounds\n",
    "                    x1 = max(0.0, x_min)\n",
    "                    y1 = max(0.0, y_min)\n",
    "                    x2 = min(float(original_w), x_min + w)\n",
    "                    y2 = min(float(original_h), y_min + h)\n",
    "                    if x2 > x1 and y2 > y1: # Check for valid box area\n",
    "                         bboxes_abs_xyxy_list.append([x1, y1, x2, y2])\n",
    "\n",
    "                          # --- CHANGE 1: Append label as a list ---\n",
    "                         cls_list_of_lists.append([int(label)])\n",
    "                         # --- End Change 1 ---\n",
    "\n",
    "            # === FIX: Ensure bboxes_np has shape [N, 4] even if N=0 ===\n",
    "            if not bboxes_abs_xyxy_list:\n",
    "                # If list is empty, create array with shape (0, 4)\n",
    "                bboxes_np = np.zeros((0, 4), dtype=np.float32)\n",
    "            else:\n",
    "                # If list has items, convert normally\n",
    "                bboxes_np = np.array(bboxes_abs_xyxy_list, dtype=np.float32)\n",
    "            # cls_np is okay as shape (0,) if cls_list is empty\n",
    "            # --- CHANGE 2: Create cls_np with shape (N, 1) or (0, 1) ---\n",
    "            if not cls_list_of_lists:\n",
    "                # Explicitly create shape (0, 1) for empty case\n",
    "                cls_np = np.array([], dtype=np.int64).reshape(0, 1)\n",
    "            else:\n",
    "                # np.array([[0], [2], [0]]) directly creates shape (N, 1)\n",
    "                cls_np = np.array(cls_list_of_lists, dtype=np.int64)\n",
    "            # --- End Change 2 ---\n",
    "\n",
    "            # --- Create Instances object ---\n",
    "            # Instances expects bboxes in xyxy format by default if normalized=False\n",
    "            segments = np.zeros((0, 1000, 2), dtype=np.float32)\n",
    "            instances = Instances(bboxes=bboxes_np, segments=segments, bbox_format='xyxy', normalized=False)\n",
    "\n",
    "            # === START MINIMAL CHANGE ===\n",
    "            # Calculate simple ratio (height_ratio, width_ratio) based on target imgsz and original shape.\n",
    "            # This mimics the structure added by YOLODataset.get_image_and_label before transforms.\n",
    "            # Use float division.\n",
    "            ratio_h = float(self.imgsz) / original_h\n",
    "            ratio_w = float(self.imgsz) / original_w\n",
    "            # Create the simple tuple (rh, rw)\n",
    "            simple_ratio_pad = (ratio_h, ratio_w)\n",
    "            # === END MINIMAL CHANGE ===\n",
    "\n",
    "            # Format expects 'img', 'cls', 'instances'\n",
    "            sample = {\n",
    "                'img': img_np,           # uint8 HWC NumPy\n",
    "                'instances': instances,  # Instances obj with abs pixel xyxy boxes\n",
    "                'cls': cls_np,           # int64 [N] NumPy\n",
    "                'ori_shape': (original_h, original_w), # Add original shape if needed by transforms\n",
    "                'ratio_pad': simple_ratio_pad,\n",
    "            }\n",
    "            # ---------------------------------------------\n",
    "\n",
    "            # --- Apply transforms ---\n",
    "            # This pipeline includes LetterBox and Format\n",
    "            transformed_sample = self.transforms(sample)\n",
    "            # Output should have 'img' (CHW Tensor, uint8), 'cls', 'bboxes' (normalized xywh Tensor), 'batch_idx'\n",
    "            # ------------------------\n",
    "\n",
    "            # --- Add metadata needed by plotting ---\n",
    "            transformed_sample['im_file'] = f\"video_{video_idx}_frame_{frame_idx}.jpg\"\n",
    "            transformed_sample['ori_shape'] = (original_h, original_w) # Ensure ori_shape is present\n",
    "\n",
    "\n",
    "\n",
    "            # --------------------------------------------\n",
    "\n",
    "            return transformed_sample\n",
    "\n",
    "        except Exception as e:\n",
    "             # ... (error handling) ...\n",
    "             print(f\"Error in __getitem__ for index {index}: {e}\")\n",
    "             import traceback; traceback.print_exc()\n",
    "             raise e\n",
    "\n",
    "\n",
    "    # --- Keep compatibility properties ---\n",
    "    @property\n",
    "    def build_type(self): return 'build_detection_dataset'\n",
    "    ## @property\n",
    "    # def data(self): return {'names': self.names, 'nc': self.num_classes} # Trainer handles this now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QieZuznNCUwa"
   },
   "source": [
    "## VALIDATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "p853PubBCUZa"
   },
   "outputs": [],
   "source": [
    "# === Add Custom Validator Class (e.g., Cell 4a) ===\n",
    "from ultralytics.models.yolo.detect import DetectionValidator\n",
    "from ultralytics.utils import LOGGER, emojis # For logging/errors if needed\n",
    "from copy import copy\n",
    "import torch\n",
    "\n",
    "from ultralytics.cfg import get_cfg, get_save_dir\n",
    "from ultralytics.data.utils import check_cls_dataset, check_det_dataset\n",
    "from ultralytics.nn.autobackend import AutoBackend\n",
    "from ultralytics.utils import LOGGER, TQDM, callbacks, colorstr, emojis\n",
    "from ultralytics.utils.checks import check_imgsz\n",
    "from ultralytics.utils.ops import Profile\n",
    "from ultralytics.utils.torch_utils import de_parallel, select_device, smart_inference_mode\n",
    "\n",
    "class CustomDetectionValidator(DetectionValidator):\n",
    "    @smart_inference_mode()\n",
    "    def __call__(self, trainer=None, model=None):\n",
    "        \"\"\"\n",
    "        Execute validation process, running inference on dataloader and computing performance metrics.\n",
    "\n",
    "        Args:\n",
    "            trainer (object, optional): Trainer object that contains the model to validate.\n",
    "            model (nn.Module, optional): Model to validate if not using a trainer.\n",
    "\n",
    "        Returns:\n",
    "            stats (dict): Dictionary containing validation statistics.\n",
    "        \"\"\"\n",
    "        self.training = trainer is not None\n",
    "        augment = self.args.augment and (not self.training)\n",
    "        if self.training:\n",
    "            self.device = trainer.device\n",
    "            self.data = trainer.data\n",
    "            # Force FP16 val during training\n",
    "            self.args.half = self.device.type != \"cpu\" and trainer.amp\n",
    "            model = trainer.ema.ema or trainer.model\n",
    "            model = model.half() if self.args.half else model.float()\n",
    "            # self.model = model\n",
    "            self.loss = torch.zeros_like(trainer.loss_items, device=trainer.device)\n",
    "            self.args.plots &= trainer.stopper.possible_stop or (trainer.epoch == trainer.epochs - 1)\n",
    "            model.eval()\n",
    "        else:\n",
    "            if str(self.args.model).endswith(\".yaml\") and model is None:\n",
    "                LOGGER.warning(\"WARNING ‚ö†Ô∏è validating an untrained model YAML will result in 0 mAP.\")\n",
    "            callbacks.add_integration_callbacks(self)\n",
    "            model = AutoBackend(\n",
    "                weights=model or self.args.model,\n",
    "                device=select_device(self.args.device, self.args.batch),\n",
    "                dnn=self.args.dnn,\n",
    "                data=self.args.data,\n",
    "                fp16=self.args.half,\n",
    "            )\n",
    "            # self.model = model\n",
    "            self.device = model.device  # update device\n",
    "            self.args.half = model.fp16  # update half\n",
    "            stride, pt, jit, engine = model.stride, model.pt, model.jit, model.engine\n",
    "            imgsz = check_imgsz(self.args.imgsz, stride=stride)\n",
    "            if engine:\n",
    "                self.args.batch = model.batch_size\n",
    "            elif not pt and not jit:\n",
    "                self.args.batch = model.metadata.get(\"batch\", 1)  # export.py models default to batch-size 1\n",
    "                LOGGER.info(f\"Setting batch={self.args.batch} input of shape ({self.args.batch}, 3, {imgsz}, {imgsz})\")\n",
    "\n",
    "            if str(self.args.data).split(\".\")[-1] in {\"yaml\", \"yml\"}:\n",
    "                self.data = {} # check_det_dataset(self.args.data)\n",
    "            elif self.args.task == \"classify\":\n",
    "                self.data = check_cls_dataset(self.args.data, split=self.args.split)\n",
    "            else:\n",
    "                raise FileNotFoundError(emojis(f\"Dataset '{self.args.data}' for task={self.args.task} not found ‚ùå\"))\n",
    "\n",
    "            if self.device.type in {\"cpu\", \"mps\"}:\n",
    "                self.args.workers = 0  # faster CPU val as time dominated by inference, not dataloading\n",
    "            if not pt:\n",
    "                self.args.rect = False\n",
    "            self.stride = model.stride  # used in get_dataloader() for padding\n",
    "            self.dataloader = self.dataloader or self.get_dataloader(self.data.get(self.args.split), self.args.batch)\n",
    "\n",
    "            model.eval()\n",
    "            model.warmup(imgsz=(1 if pt else self.args.batch, 3, imgsz, imgsz))  # warmup\n",
    "\n",
    "        self.run_callbacks(\"on_val_start\")\n",
    "        dt = (\n",
    "            Profile(device=self.device),\n",
    "            Profile(device=self.device),\n",
    "            Profile(device=self.device),\n",
    "            Profile(device=self.device),\n",
    "        )\n",
    "        bar = TQDM(self.dataloader, desc=self.get_desc(), total=len(self.dataloader))\n",
    "        self.init_metrics(de_parallel(model))\n",
    "        self.jdict = []  # empty before each val\n",
    "        for batch_i, batch in enumerate(bar):\n",
    "            self.run_callbacks(\"on_val_batch_start\")\n",
    "            self.batch_i = batch_i\n",
    "            # Preprocess\n",
    "            with dt[0]:\n",
    "                batch = self.preprocess(batch)\n",
    "\n",
    "            # Inference\n",
    "            with dt[1]:\n",
    "                preds = model(batch[\"img\"], augment=augment)\n",
    "\n",
    "            # Loss\n",
    "            with dt[2]:\n",
    "                if self.training:\n",
    "                    self.loss += model.loss(batch, preds)[1]\n",
    "\n",
    "            # Postprocess\n",
    "            with dt[3]:\n",
    "                preds = self.postprocess(preds)\n",
    "\n",
    "            self.update_metrics(preds, batch)\n",
    "            if self.args.plots and batch_i < 3:\n",
    "                self.plot_val_samples(batch, batch_i)\n",
    "                self.plot_predictions(batch, preds, batch_i)\n",
    "\n",
    "            self.run_callbacks(\"on_val_batch_end\")\n",
    "        stats = self.get_stats()\n",
    "        self.check_stats(stats)\n",
    "        self.speed = dict(zip(self.speed.keys(), (x.t / len(self.dataloader.dataset) * 1e3 for x in dt)))\n",
    "        self.finalize_metrics()\n",
    "        self.print_results()\n",
    "        self.run_callbacks(\"on_val_end\")\n",
    "        if self.training:\n",
    "            model.float()\n",
    "            results = {**stats, **trainer.label_loss_items(self.loss.cpu() / len(self.dataloader), prefix=\"val\")}\n",
    "            return {k: round(float(v), 5) for k, v in results.items()}  # return results as 5 decimal place floats\n",
    "        else:\n",
    "            LOGGER.info(\n",
    "                \"Speed: {:.1f}ms preprocess, {:.1f}ms inference, {:.1f}ms loss, {:.1f}ms postprocess per image\".format(\n",
    "                    *tuple(self.speed.values())\n",
    "                )\n",
    "            )\n",
    "            if self.args.save_json and self.jdict:\n",
    "                with open(str(self.save_dir / \"predictions.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                    LOGGER.info(f\"Saving {f.name}...\")\n",
    "                    json.dump(self.jdict, f)  # flatten and save\n",
    "                stats = self.eval_json(stats)  # update stats\n",
    "            if self.args.plots or self.args.save_json:\n",
    "                LOGGER.info(f\"Results saved to {colorstr('bold', self.save_dir)}\")\n",
    "            return stats\n",
    "\n",
    "    # --- Keep the _prepare_batch override if it was needed for the 0-D tensor error ---\n",
    "    # def _prepare_batch(self, si, batch):\n",
    "    #      ... (implementation that handles 0-D cls tensor) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-biaMFvu4IES"
   },
   "source": [
    "## TRAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BXIB2EjUHjlS"
   },
   "outputs": [],
   "source": [
    "# from ultralytics.utils.plotting import plot_images, plot_labels, plot_results\n",
    "# Import YOLODataset\n",
    "from ultralytics.data.dataset import YOLODataset\n",
    "\n",
    "class CustomDetectionTrainer(DetectionTrainer):\n",
    "    def __init__(self, trust_remote_code=False, debug=False, *args, **kwargs): # Removed hf_dataset_identifier from signature\n",
    "        print(\"CustomDetectionTrainer __init__ started...\")\n",
    "        self.custom_trust_code = trust_remote_code\n",
    "        self.hf_train_split = None; self.hf_val_split = None\n",
    "        self._debug = debug\n",
    "        # We rely on args.data being the HF identifier now\n",
    "        super().__init__(*args, **kwargs) # Calls overridden get_dataset\n",
    "        print(\"CustomDetectionTrainer __init__ finished.\")\n",
    "        # Verification AFTER get_dataset has run (called by super init)\n",
    "        # if not hasattr(self.args, 'nc') or not isinstance(self.args.nc, int) or self.args.nc <= 0:\n",
    "        #      raise RuntimeError(f\"Trainer self.args.nc not set correctly after get_dataset. Check get_dataset override.\")\n",
    "        # if not hasattr(self.args, 'names') or not isinstance(self.args.names, dict):\n",
    "        #      raise RuntimeError(f\"Trainer self.args.names not set correctly after get_dataset.\")\n",
    "        # print(f\"Trainer initialized: nc={self.args.nc}, names={self.args.names}\")\n",
    "\n",
    "    # --- Override get_dataset ---\n",
    "    def get_dataset(self):\n",
    "        \"\"\"\n",
    "        Overrides the base method. Loads HF dataset using args.data.\n",
    "        Instantiates HFYOLODataset (which discovers nc/names).\n",
    "        Sets args.nc and args.names based on the loaded dataset info.\n",
    "        Sets self.trainset and self.testset.\n",
    "        \"\"\"\n",
    "        print(\"****** Custom get_dataset called ******\")\n",
    "        if not hasattr(self, 'args'): raise RuntimeError(\"Trainer arguments missing.\")\n",
    "\n",
    "        hf_dataset_identifier = 'Max-Ploter/detection-moving-mnist-easy' #self.args.data # Get HF ID from data arg\n",
    "        trust_remote_code = self.custom_trust_code # Get from instance attribute\n",
    "\n",
    "        if not hf_dataset_identifier or not isinstance(hf_dataset_identifier, str):\n",
    "             raise ValueError(f\"HF dataset identifier '{hf_dataset_identifier}' (from args.data) is invalid.\")\n",
    "\n",
    "        # --- Load HF Splits ---\n",
    "        # ... (Load self.hf_train_split / self.hf_val_split using hf_dataset_identifier) ...\n",
    "        if self.hf_train_split is None:\n",
    "            try: self.hf_train_split = load_dataset(hf_dataset_identifier, split='train', trust_remote_code=trust_remote_code); print(f\"Loaded train split: {len(self.hf_train_split)} samples.\")\n",
    "            except Exception as e: raise RuntimeError(f\"Failed load HF train '{hf_dataset_identifier}': {e}\") from e\n",
    "        if self.hf_val_split is None:\n",
    "            try: # Try val then test\n",
    "                try: self.hf_val_split = load_dataset(hf_dataset_identifier, split='validation', trust_remote_code=trust_remote_code); print(\"Using 'validation' split.\")\n",
    "                except ValueError: self.hf_val_split = load_dataset(hf_dataset_identifier, split='test', trust_remote_code=trust_remote_code); print(\"Using 'test' split.\")\n",
    "                if not self.hf_val_split: raise ValueError(\"No val/test split found.\")\n",
    "                print(f\"Loaded val/test split: {len(self.hf_val_split)} samples.\")\n",
    "            except Exception as e: # Fallback split train\n",
    "                 print(f\"Warning: Failed loading val/test: {e}. Splitting train.\")\n",
    "                 # ... (Split train logic) ...\n",
    "                 if self.hf_train_split is None: raise RuntimeError(\"Train split not available.\")\n",
    "                 if len(self.hf_train_split) < 2: raise RuntimeError(\"Train split too small.\")\n",
    "                 splits = self.hf_train_split.train_test_split(test_size=0.2, seed=getattr(self.args, 'seed', 42))\n",
    "                 self.hf_train_split, self.hf_val_split = splits['train'], splits['test']\n",
    "                 print(f\"Used 80/20 split of 'train' for train ({len(self.hf_train_split)})/validation ({len(self.hf_val_split)}).\")\n",
    "\n",
    "        if self._debug:\n",
    "          # reduce train and val split sizes\n",
    "          print(\"Reducing train/val split sizes for debugging...\")\n",
    "          self.hf_train_split = self.hf_train_split.select(range(1))\n",
    "          self.hf_val_split = self.hf_val_split.select(range(1))\n",
    "\n",
    "        # --- Instantiate HFYOLODatasets (which find nc/names) ---\n",
    "        print(\"Instantiating HFYOLODataset for trainset...\")\n",
    "        self.trainset = HFYOLODataset(self.hf_train_split, imgsz=self.args.imgsz, trust_remote_code=trust_remote_code)\n",
    "        print(\"Instantiating HFYOLODataset for testset (validation)...\")\n",
    "        self.testset = HFYOLODataset(self.hf_val_split, imgsz=self.args.imgsz, trust_remote_code=trust_remote_code)\n",
    "\n",
    "        # --- Get nc/names FROM the instantiated dataset ---\n",
    "        known_nc = getattr(self.trainset, 'num_classes', 0)\n",
    "        known_names = getattr(self.trainset, 'names', {})\n",
    "        if known_nc <= 0: # Fallback to testset if trainset failed\n",
    "            known_nc = getattr(self.testset, 'num_classes', 0)\n",
    "            known_names = getattr(self.testset, 'names', {})\n",
    "\n",
    "        if known_nc <= 0:\n",
    "            raise ValueError(\"Could not determine number of classes from loaded HFYOLODataset instances.\")\n",
    "        # --------------------------------------------------\n",
    "\n",
    "        # --- Set nc and names on self.args ---\n",
    "        # This is the CRUCIAL step - fulfilling the presumed responsibility\n",
    "        print(f\"Setting trainer args: nc={known_nc}, names={known_names}\")\n",
    "        # self.args.nc = known_nc\n",
    "        # self.args.names = known_names\n",
    "\n",
    "        # --- <<< NEW: Explicitly set self.data attribute >>> ---\n",
    "        # This dictionary is expected by the original get_model method\n",
    "        self.data = {'nc': known_nc, 'names': known_names}\n",
    "        # Add other keys if get_model relies on them, e.g. 'path' (can be dummy)\n",
    "        # self.data['path'] = '.' # Example if path is needed\n",
    "        print(f\"Set self.data attribute: {self.data}\")\n",
    "        # ----------------------------------------------------\n",
    "\n",
    "        # Optional: Update dataset.data for consistency\n",
    "        if hasattr(self.trainset, 'data'): self.trainset.data = self.data\n",
    "        if hasattr(self.testset, 'data'): self.testset.data = self.data\n",
    "\n",
    "\n",
    "        # ------------------------------------\n",
    "\n",
    "        print(f\"****** Custom get_dataset finished. Set args.nc={known_nc}. ******\")\n",
    "        return self.trainset, self.testset\n",
    "\n",
    "\n",
    "    # --- get_dataloader and plot_training_labels overrides remain the same ---\n",
    "    def get_dataloader(self, dataset=None, batch_size=16, rank=0, mode='train'):\n",
    "        # ... (implementation is unchanged) ...\n",
    "        print(f\"****** Custom get_dataloader called for mode: {mode} ******\")\n",
    "        if dataset is None: dataset = self.trainset if mode == 'train' else self.testset\n",
    "        if not isinstance(dataset, HFYOLODataset): print(f\"Warning: Dataset type {type(dataset)}.\")\n",
    "        batch_size_arg = getattr(self.args, 'batch', batch_size); batch_size_to_use = batch_size_arg if isinstance(batch_size_arg, int) and batch_size_arg > 0 else batch_size\n",
    "        if mode != 'train': batch_size_to_use *= 2\n",
    "        workers = getattr(self.args, 'workers', 0); shuffle = (mode == 'train')\n",
    "        print(f\"Creating DataLoader for mode '{mode}' with batch_size={batch_size_to_use}, workers={workers}...\")\n",
    "        loader = DataLoader(dataset, batch_size=batch_size_to_use, shuffle=shuffle, num_workers=workers, pin_memory=True,\n",
    "                            collate_fn=YOLODataset.collate_fn\n",
    "                            )\n",
    "        print(f\"****** Custom DataLoader created for {mode} ******\")\n",
    "        return loader\n",
    "\n",
    "    def plot_training_labels(self): print(\"Skipping plot_training_labels in Custom Trainer.\"); pass\n",
    "\n",
    "    def get_validator(self):\n",
    "        \"\"\"Returns a CustomDetectionValidator instance.\"\"\"\n",
    "        print(\"****** Custom get_validator called (Returning CustomDetectionValidator) ******\")\n",
    "        # Ensure validation dataloader exists\n",
    "        if not hasattr(self, 'test_loader') or self.test_loader is None:\n",
    "             print(\"Creating validation dataloader within get_validator...\")\n",
    "             if not hasattr(self, 'testset') or self.testset is None: raise RuntimeError(\"Validation dataset missing.\")\n",
    "             val_batch_size = getattr(self.args, 'batch', 16) * 2\n",
    "             self.test_loader = self.get_dataloader(self.testset, batch_size=val_batch_size, mode='val')\n",
    "\n",
    "        validator_args = copy(self.args) # Pass copy of trainer args\n",
    "\n",
    "        # Instantiate OUR custom validator\n",
    "        validator = CustomDetectionValidator( # Use the custom class\n",
    "            dataloader=self.test_loader,\n",
    "            save_dir=self.save_dir,\n",
    "            args=validator_args,\n",
    "            # --- FIX: Pass the main self.callbacks dict directly ---\n",
    "            _callbacks=self.callbacks\n",
    "            # --- End Fix ---\n",
    "        )\n",
    "        # Link model and data dict\n",
    "        validator.model = self.model\n",
    "        validator.data = self.data\n",
    "        print(f\"****** Custom get_validator finished. Validator created. Using data: {validator.data} ******\")\n",
    "        return validator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vK-ZOOk53_Jd"
   },
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eLJi3dhOHm6X",
    "outputId": "079d66de-92be-4b1e-b9ce-586f38c2f262"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to resume training from: ../not_tracked_dir/output_yolo_v8_2025-04-11/weights/last.pt\n",
      "Checkpoint file found.\n"
     ]
    }
   ],
   "source": [
    "# === Configuration Cell in Jupyter Notebook ===\n",
    "from types import SimpleNamespace\n",
    "import os\n",
    "\n",
    "# --- Define YAML Content and Filename ---\n",
    "FAKE_YAML_FILENAME = \"fake.yaml\" # Name of the file to create\n",
    "YAML_SHIM_CONTENT = \"\"\"\n",
    "# Minimal YAML to satisfy Ultralytics checks during validation\n",
    "path: ./ignored_path # Ignored, but path key might be checked\n",
    "train: images/train   # Ignored\n",
    "val: images/val       # Ignored\n",
    "\n",
    "# --- Important Part ---\n",
    "nc: 10 # Your known number of classes\n",
    "names:\n",
    "  0: '0'\n",
    "  1: '1'\n",
    "  2: '2'\n",
    "  3: '3'\n",
    "  4: '4'\n",
    "  5: '5'\n",
    "  6: '6'\n",
    "  7: '7'\n",
    "  8: '8'\n",
    "  9: '9'\n",
    "# --- End Important Part ---\n",
    "\"\"\"\n",
    "# ---------------------------------------\n",
    "\n",
    "# --- Configuration Variables ---\n",
    "CFG_HF_DATASET_IDENTIFIER = \"Max-Ploter/detection-moving-mnist-easy\" # Your HF dataset path/name\n",
    "CFG_MODEL_NAME = 'yolov8n.pt'\n",
    "# CFG_NUM_CLASSES = 10  # No longer needed here\n",
    "CFG_EPOCHS = 100\n",
    "CFG_BATCH_SIZE = 64\n",
    "CFG_IMG_SIZE = 320\n",
    "CFG_WORKERS = 2\n",
    "CFG_TRUST_REMOTE_CODE = True # Custom flag for trainer\n",
    "CFG_PROJECT_NAME = \"yolo_hf_custom_trainer\"\n",
    "\n",
    "CFG_PLOTS = True\n",
    "CFG_SAVE_PERIOD = 1 # <<<<<<< ADDED: Set save_period to 1\n",
    "\n",
    "\n",
    "# --- Construct the path to the last checkpoint ---\n",
    "# !!! This path MUST exist from your previous training run !!!\n",
    "checkpoint_dir = os.path.join(GDRIVE_BASE_PATH, CFG_EXPERIMENT_NAME, 'weights')\n",
    "resume_checkpoint_path = os.path.join(checkpoint_dir, 'last.pt')\n",
    "\n",
    "print(f\"Attempting to resume training from: {resume_checkpoint_path}\")\n",
    "\n",
    "# --- Check if the checkpoint exists ---\n",
    "if not os.path.exists(resume_checkpoint_path):\n",
    "    raise FileNotFoundError(f\"Checkpoint file not found at: {resume_checkpoint_path}. Cannot resume.\")\n",
    "else:\n",
    "    print(\"Checkpoint file found.\")\n",
    "    # --- Set the model path to the checkpoint for resuming ---\n",
    "    CFG_MODEL_TO_LOAD = resume_checkpoint_path\n",
    "\n",
    "\n",
    "# --- Prepare config_args with standard args ---\n",
    "# 'data' now holds the HF identifier\n",
    "# 'nc' is NOT set here\n",
    "config_args = SimpleNamespace(\n",
    "    model = CFG_MODEL_NAME,\n",
    "    data = FAKE_YAML_FILENAME, #CFG_HF_DATASET_IDENTIFIER, # Pass HF ID as data arg\n",
    "    epochs = CFG_EPOCHS,\n",
    "    batch = CFG_BATCH_SIZE,\n",
    "    imgsz = CFG_IMG_SIZE,\n",
    "    project = GDRIVE_BASE_PATH,\n",
    "    name = CFG_EXPERIMENT_NAME,\n",
    "    workers = CFG_WORKERS,\n",
    "    device = None,\n",
    "    plots = CFG_PLOTS,\n",
    "    resume=CFG_MODEL_TO_LOAD,\n",
    "    # trust_remote_code is handled separately\n",
    "    # nc is handled by get_dataset\n",
    ")\n",
    "\n",
    "# with open(FAKE_YAML_FILENAME, 'w') as f:\n",
    "#     f.write(YAML_SHIM_CONTENT)\n",
    "# print(f\"Created fake YAML file: {FAKE_YAML_FILENAME}\")\n",
    "\n",
    "# --- Sanity Check ---\n",
    "if config_args.data == \"your_huggingface_dataset_identifier\": # Or similar check\n",
    "     print(f\"üõë Error: Please set HF dataset identifier in CFG_HF_DATASET_IDENTIFIER.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vTa5xlaG8XnR",
    "outputId": "cbd4a23e-a4c3-4d01-eba8-1332368641cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Instantiating CustomDetectionTrainer manually...\n",
      "CustomDetectionTrainer __init__ started...\n",
      "Ultralytics 8.3.109 üöÄ Python-3.11.11 torch-2.5.1 CUDA:0 (Tesla V100-PCIE-16GB, 16144MiB)\n",
      "\n",
      "‚ùå Training failed with an error:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/helios/home/ploter/.conda/envs/sensor_dropout/lib/python3.11/pathlib.py\", line 1116, in mkdir\n",
      "    os.mkdir(self, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/YOLOv8_Training/train_yolo_nano_24k_dataset3/weights'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/helios/home/ploter/.conda/envs/sensor_dropout/lib/python3.11/pathlib.py\", line 1116, in mkdir\n",
      "    os.mkdir(self, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/YOLOv8_Training/train_yolo_nano_24k_dataset3'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/helios/home/ploter/.conda/envs/sensor_dropout/lib/python3.11/pathlib.py\", line 1116, in mkdir\n",
      "    os.mkdir(self, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/YOLOv8_Training'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/helios/home/ploter/.conda/envs/sensor_dropout/lib/python3.11/pathlib.py\", line 1116, in mkdir\n",
      "    os.mkdir(self, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/helios/home/ploter/.conda/envs/sensor_dropout/lib/python3.11/pathlib.py\", line 1116, in mkdir\n",
      "    os.mkdir(self, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3882716/359971511.py\", line 7, in <module>\n",
      "    trainer = CustomDetectionTrainer(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3882716/3204420693.py\", line 12, in __init__\n",
      "    super().__init__(*args, **kwargs) # Calls overridden get_dataset\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfs/helios/home/ploter/.conda/envs/sensor_dropout/lib/python3.11/site-packages/ultralytics/engine/trainer.py\", line 118, in __init__\n",
      "    self.wdir.mkdir(parents=True, exist_ok=True)  # make dir\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfs/helios/home/ploter/.conda/envs/sensor_dropout/lib/python3.11/pathlib.py\", line 1120, in mkdir\n",
      "    self.parent.mkdir(parents=True, exist_ok=True)\n",
      "  File \"/gpfs/helios/home/ploter/.conda/envs/sensor_dropout/lib/python3.11/pathlib.py\", line 1120, in mkdir\n",
      "    self.parent.mkdir(parents=True, exist_ok=True)\n",
      "  File \"/gpfs/helios/home/ploter/.conda/envs/sensor_dropout/lib/python3.11/pathlib.py\", line 1120, in mkdir\n",
      "    self.parent.mkdir(parents=True, exist_ok=True)\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"/gpfs/helios/home/ploter/.conda/envs/sensor_dropout/lib/python3.11/pathlib.py\", line 1116, in mkdir\n",
      "    os.mkdir(self, mode)\n",
      "PermissionError: [Errno 13] Permission denied: '/content'\n"
     ]
    }
   ],
   "source": [
    "# === Execution Cell in Jupyter Notebook ===\n",
    "# Ensure CustomDetectionTrainer etc are defined\n",
    "\n",
    "if config_args.data != \"your_huggingface_dataset_identifier\": # Or similar check\n",
    "    print(\"üöÄ Instantiating CustomDetectionTrainer manually...\")\n",
    "    try:\n",
    "        trainer = CustomDetectionTrainer(\n",
    "            overrides=vars(config_args), # Contains standard args + HF ID in 'data'\n",
    "            # Pass ONLY custom args directly\n",
    "            trust_remote_code=CFG_TRUST_REMOTE_CODE,\n",
    "            debug=False\n",
    "        )\n",
    "\n",
    "        print(\"\\nüöÄ Starting trainer.train()...\")\n",
    "        trainer.train()\n",
    "        print(\"\\n‚úÖ Training finished successfully!\")\n",
    "        # print(f\"   Best model weights: {trainer.best}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Training failed with an error:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Training not started. Please set HF dataset identifier.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMAQgMs4ItDi"
   },
   "source": [
    "## EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ekKbyq_tXfiE",
    "outputId": "8d576dd9-095c-4bcb-d682-06f17db7014c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 'best.pt', evaluating it: ../not_tracked_dir/output_yolo_v8_2025-04-11/weights/best.pt\n",
      "Selected checkpoint for evaluation: ../not_tracked_dir/output_yolo_v8_2025-04-11/weights/best.pt\n",
      "Loading trained model checkpoint...\n",
      "Loading 'test' split from Hugging Face...\n",
      "Loaded test split: 10000 samples.\n",
      "Reducing test split sizes for debugging...\n",
      "Reduced test split to 5000 samples.\n",
      "Initializing HFYOLODataset (Using Ultralytics Transforms)...\n",
      "Frames/Video: 20, Dim: (128, 128)\n",
      "Total train frames: 100000\n",
      "Discovered nc=10, names={0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
      "Augmentation enabled, but using simple LetterBox transform.\n",
      "Transforms created: Compose(<ultralytics.data.augment.LetterBox object at 0x150fcdd38b90>, <ultralytics.data.augment.Format object at 0x1510dd857090>)\n",
      "Test dataset created.\n",
      "Creating test dataloader...\n",
      "Test dataloader created.\n",
      "Preparing validator...\n",
      "Test evaluation results will be saved in: ../not_tracked_dir/output_yolo_v8_2025-04-11/test_eval_best\n",
      "Test data info: nc=10, names={0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
      "Validator instance created and configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from types import SimpleNamespace\n",
    "from ultralytics import YOLO  # Import the YOLO class\n",
    "from ultralytics.data.dataset import YOLODataset # For collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset # Assuming this is used in HFYOLODataset\n",
    "\n",
    "# --- 1. Define the Checkpoint to Evaluate ---\n",
    "# Usually, you evaluate 'best.pt', but after 1 epoch, 'last.pt' is more likely\n",
    "checkpoint_dir = os.path.join(GDRIVE_BASE_PATH, CFG_EXPERIMENT_NAME, 'weights')\n",
    "checkpoint_to_eval = os.path.join(checkpoint_dir, 'best.pt') # Standard choice\n",
    "\n",
    "# Check if the chosen checkpoint exists\n",
    "best_checkpoint_path = os.path.join(checkpoint_dir, 'best.pt')\n",
    "if os.path.exists(best_checkpoint_path):\n",
    "    checkpoint_to_eval = best_checkpoint_path\n",
    "    print(f\"Found 'best.pt', evaluating it: {checkpoint_to_eval}\")\n",
    "elif os.path.exists(os.path.join(checkpoint_dir, 'last.pt')):\n",
    "    checkpoint_to_eval = os.path.join(checkpoint_dir, 'last.pt')\n",
    "    print(f\"Found 'last.pt', evaluating it: {checkpoint_to_eval}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Neither 'best.pt' nor 'last.pt' found in {checkpoint_dir}\")\n",
    "\n",
    "if not os.path.exists(checkpoint_to_eval):\n",
    "     raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_to_eval}\")\n",
    "print(f\"Selected checkpoint for evaluation: {checkpoint_to_eval}\")\n",
    "\n",
    "# --- 2. Load the Trained Model ---\n",
    "print(\"Loading trained model checkpoint...\")\n",
    "# This loads the model weights into the YOLO architecture\n",
    "#model = YOLO(checkpoint_to_eval)\n",
    "#print(\"Model loaded.\")\n",
    "\n",
    "# --- 3. Load the Test Dataset ---\n",
    "print(\"Loading 'test' split from Hugging Face...\")\n",
    "try:\n",
    "    hf_test_split = load_dataset(CFG_HF_DATASET_IDENTIFIER, split='test', trust_remote_code=CFG_TRUST_REMOTE_CODE)\n",
    "    print(f\"Loaded test split: {len(hf_test_split)} samples.\")\n",
    "\n",
    "    if True: # Debug\n",
    "      print(\"Reducing test split sizes for debugging...\")\n",
    "      hf_test_split = hf_test_split.select(range(5000))\n",
    "      print(f\"Reduced test split to {len(hf_test_split)} samples.\")\n",
    "\n",
    "    # Create the HFYOLODataset instance for the test split\n",
    "    # Ensure HFYOLODataset class is defined/imported\n",
    "    test_dataset = HFYOLODataset(hf_test_split, imgsz=CFG_IMG_SIZE, trust_remote_code=CFG_TRUST_REMOTE_CODE)\n",
    "    print(\"Test dataset created.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to load or process test split: {e}\") from e\n",
    "\n",
    "# --- 4. Create the Test DataLoader ---\n",
    "print(\"Creating test dataloader...\")\n",
    "# Use a larger batch size for validation/testing is common\n",
    "test_batch_size = CFG_BATCH_SIZE * 2\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=test_batch_size,\n",
    "    shuffle=False, # No need to shuffle for evaluation\n",
    "    num_workers=CFG_WORKERS,\n",
    "    pin_memory=True,\n",
    "    collate_fn=YOLODataset.collate_fn # Use the standard collate_fn\n",
    ")\n",
    "print(\"Test dataloader created.\")\n",
    "\n",
    "# --- 5. Prepare Validator Arguments and Instantiate Validator ---\n",
    "print(\"Preparing validator...\")\n",
    "# Define a directory to save test evaluation results/plots\n",
    "test_eval_save_dir = os.path.join(GDRIVE_BASE_PATH, CFG_EXPERIMENT_NAME, f'test_eval_{os.path.basename(checkpoint_to_eval).split(\".\")[0]}')\n",
    "os.makedirs(test_eval_save_dir, exist_ok=True)\n",
    "print(f\"Test evaluation results will be saved in: {test_eval_save_dir}\")\n",
    "\n",
    "# Get nc and names from the test dataset (should match training)\n",
    "test_nc = getattr(test_dataset, 'num_classes', 0)\n",
    "test_names = getattr(test_dataset, 'names', {})\n",
    "if test_nc <= 0:\n",
    "    raise ValueError(\"Could not determine number of classes from test dataset.\")\n",
    "test_data_dict = {'nc': test_nc, 'names': test_names}\n",
    "print(f\"Test data info: nc={test_nc}, names={test_names}\")\n",
    "\n",
    "# Prepare minimal arguments needed by the Validator\n",
    "# Check your CustomDetectionValidator's __init__ and methods if it needs more args\n",
    "validator_args = SimpleNamespace(\n",
    "    data = FAKE_YAML_FILENAME,\n",
    "    save_dir=test_eval_save_dir,\n",
    "    device=None, # Use the device determined during training setup\n",
    "    batch=test_batch_size,\n",
    "    imgsz=CFG_IMG_SIZE,\n",
    "    split='test',    # Specify the split being evaluated\n",
    "    task='detect',   # Specify the task\n",
    "    plots=True,      # Enable saving plots (e.g., confusion matrix, PR curves)\n",
    "    save_json=False, # Set True if you need COCO format JSON output\n",
    "    # save_hybrid=False,\n",
    "    conf=0.001,      # Confidence threshold (adjust if needed)\n",
    "    iou=0.6,         # IoU threshold for NMS (adjust if needed)\n",
    "    max_det=10,\n",
    "    # data=FAKE_YAML_FILENAME, # Maybe needed if validator parses it, but we set validator.data directly\n",
    "    project=GDRIVE_BASE_PATH, # Not directly used by validator usually, but good practice\n",
    "    name=os.path.basename(test_eval_save_dir), # Logical name for this eval run\n",
    ")\n",
    "\n",
    "# Instantiate your CustomDetectionValidator\n",
    "# Ensure the CustomDetectionValidator class is defined/imported\n",
    "validator = CustomDetectionValidator(\n",
    "    dataloader=test_loader,\n",
    "    save_dir=Path(test_eval_save_dir),\n",
    "    args=validator_args,\n",
    "    _callbacks={} # Pass empty callbacks if not needed for pure evaluation\n",
    ")\n",
    "\n",
    "# Link the loaded model (usually the internal torch model) and data info\n",
    "# The YOLO object holds the model in '.model'\n",
    "# validator.model = model\n",
    "validator.data = test_data_dict # Provide nc/names directly\n",
    "\n",
    "print(\"Validator instance created and configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-8DmR7SJIn6",
    "outputId": "b578ed15-0d4d-4785-9c29-27e260dea796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 'best.pt', evaluating it: ../not_tracked_dir/output_yolo_v8_2025-04-11/weights/best.pt\n",
      "Selected checkpoint for evaluation: ../not_tracked_dir/output_yolo_v8_2025-04-11/weights/best.pt\n",
      "Loading trained model checkpoint...\n",
      "Loading 'test' split from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 24000 examples [00:06, 3754.70 examples/s]\n",
      "Generating test split: 10000 examples [00:02, 4042.25 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test split: 10000 samples.\n",
      "Initializing HFYOLODataset (Using Ultralytics Transforms)...\n",
      "Frames/Video: 20, Dim: (128, 128)\n",
      "Total train frames: 200000\n",
      "Discovered nc=10, names={0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
      "Augmentation enabled, but using simple LetterBox transform.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforms created: Compose(<ultralytics.data.augment.LetterBox object at 0x154bae4eda30>, <ultralytics.data.augment.Format object at 0x154bae48f580>)\n",
      "Test dataset created.\n",
      "Creating test dataloader...\n",
      "Test dataloader created.\n",
      "Preparing validator...\n",
      "Test evaluation results will be saved in: ../not_tracked_dir/output_yolo_v8_2025-04-11/test_eval_best\n",
      "Test data info: nc=10, names={0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
      "Validator instance created and configured.\n",
      "Starting evaluation on the test set...\n",
      "Ultralytics 8.3.107 üöÄ Python-3.9.12 torch-2.5.1+cu124 CUDA:0 (Tesla V100-PCIE-16GB, 16144MiB)\n",
      "Model summary (fused): 72 layers, 3,007,598 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 2/1563 [00:07<1:25:49,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ultralytics.com/assets/Arial.ttf to '/gpfs/helios/home/ploter/.config/Ultralytics/Arial.ttf'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 755k/755k [00:00<00:00, 14.3MB/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [28:50<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all     200000     851740      0.968      0.919      0.971      0.939\n",
      "                     0      65391      82934      0.966      0.944       0.98      0.959\n",
      "                     1      73283      95499      0.962      0.909      0.966      0.922\n",
      "                     2      70732      91277      0.981      0.921      0.972      0.928\n",
      "                     3      67769      86001      0.964       0.92      0.972      0.938\n",
      "                     4      66242      82726      0.967      0.926      0.972       0.94\n",
      "                     5      61559      75659      0.968      0.921      0.973      0.938\n",
      "                     6      63775      79171      0.975       0.92      0.974       0.95\n",
      "                     7      69464      87833      0.959      0.908      0.967      0.936\n",
      "                     8      67065      85789      0.973      0.923      0.974      0.943\n",
      "                     9      66988      84851      0.967      0.898      0.964       0.94\n",
      "Speed: 0.0ms preprocess, 0.3ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1m../not_tracked_dir/output_yolo_v8_2025-04-11/test_eval_best\u001b[0m\n",
      "Evaluation finished.\n",
      "\n",
      "--- Test Set Evaluation Metrics ---\n",
      "\n",
      "‚ùå Evaluation failed with an error:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3175597/2828092570.py\", line 125, in <module>\n",
      "    map50_95 = results.maps[0] # mAP50-95 for class 0 (or overall if only 1 class reported directly)\n",
      "AttributeError: 'dict' object has no attribute 'maps'\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Run Evaluation ---\n",
    "print(\"Starting evaluation on the test set...\")\n",
    "try:\n",
    "    # The validator instance is callable and runs the evaluation loop\n",
    "    results = validator(model = checkpoint_to_eval)\n",
    "    print(\"Evaluation finished.\")\n",
    "\n",
    "    # --- 7. Print Results ---\n",
    "    # The 'results' object (often a dictionary) contains the metrics\n",
    "    # Refer to ultralytics documentation or inspect the 'results' keys for specifics\n",
    "    print(\"\\n--- Test Set Evaluation Metrics ---\")\n",
    "    # Common metrics for detection:\n",
    "    map50_95 = results.maps[0] # mAP50-95 for class 0 (or overall if only 1 class reported directly)\n",
    "    map50 = results.maps[50] # mAP50 for class 0 (or overall)\n",
    "    print(f\"mAP50-95: {map50_95:.4f}\")\n",
    "    print(f\"mAP50:    {map50:.4f}\")\n",
    "    # Print all metrics found\n",
    "    print(\"\\nFull metrics dictionary:\")\n",
    "    print(results.metrics_data) # Or just print(results) depending on object type\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Evaluation failed with an error:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found 'best.pt', evaluating it: ../not_tracked_dir/output_yolo_v8_2025-04-11/weights/best.pt\n",
    "Selected checkpoint for evaluation: ../not_tracked_dir/output_yolo_v8_2025-04-11/weights/best.pt\n",
    "Loading trained model checkpoint...\n",
    "Loading 'test' split from Hugging Face...\n",
    "Generating train split: 24000 examples [00:06, 3754.70 examples/s]\n",
    "Generating test split: 10000 examples [00:02, 4042.25 examples/s]\n",
    "Loaded test split: 10000 samples.\n",
    "Initializing HFYOLODataset (Using Ultralytics Transforms)...\n",
    "Frames/Video: 20, Dim: (128, 128)\n",
    "Total train frames: 200000\n",
    "Discovered nc=10, names={0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
    "Augmentation enabled, but using simple LetterBox transform.\n",
    "\n",
    "Transforms created: Compose(<ultralytics.data.augment.LetterBox object at 0x154bae4eda30>, <ultralytics.data.augment.Format object at 0x154bae48f580>)\n",
    "Test dataset created.\n",
    "Creating test dataloader...\n",
    "Test dataloader created.\n",
    "Preparing validator...\n",
    "Test evaluation results will be saved in: ../not_tracked_dir/output_yolo_v8_2025-04-11/test_eval_best\n",
    "Test data info: nc=10, names={0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
    "Validator instance created and configured.\n",
    "Starting evaluation on the test set...\n",
    "Ultralytics 8.3.107 üöÄ Python-3.9.12 torch-2.5.1+cu124 CUDA:0 (Tesla V100-PCIE-16GB, 16144MiB)\n",
    "Model summary (fused): 72 layers, 3,007,598 parameters, 0 gradients, 8.1 GFLOPs\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 2/1563 [00:07<1:25:49,  3.30s/it]\n",
    "Downloading https://ultralytics.com/assets/Arial.ttf to '/gpfs/helios/home/ploter/.config/Ultralytics/Arial.ttf'...\n",
    "\n",
    "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 755k/755k [00:00<00:00, 14.3MB/s]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [28:50<00:00,  1.11s/it]\n",
    "                   all     200000     851740      0.968      0.919      0.971      0.939\n",
    "                     0      65391      82934      0.966      0.944       0.98      0.959\n",
    "                     1      73283      95499      0.962      0.909      0.966      0.922\n",
    "                     2      70732      91277      0.981      0.921      0.972      0.928\n",
    "                     3      67769      86001      0.964       0.92      0.972      0.938\n",
    "                     4      66242      82726      0.967      0.926      0.972       0.94\n",
    "                     5      61559      75659      0.968      0.921      0.973      0.938\n",
    "                     6      63775      79171      0.975       0.92      0.974       0.95\n",
    "                     7      69464      87833      0.959      0.908      0.967      0.936\n",
    "                     8      67065      85789      0.973      0.923      0.974      0.943\n",
    "                     9      66988      84851      0.967      0.898      0.964       0.94\n",
    "Speed: 0.0ms preprocess, 0.3ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
    "Results saved to ../not_tracked_dir/output_yolo_v8_2025-04-11/test_eval_best\n",
    "Evaluation finished.\n",
    "\n",
    "--- Test Set Evaluation Metrics ---\n",
    "\n",
    "‚ùå Evaluation failed with an error:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fB8Slc3-XBy5"
   },
   "source": [
    "## EVAL_OWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3qQrYoXGXBy6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "\n",
    "# Assuming necessary imports from your environment are available:\n",
    "# HFYOLODataset class (your original version)\n",
    "# yolo_hf_collate_fn (or YOLODataset.collate_fn if compatible)\n",
    "# YOLO class from ultralytics\n",
    "# load_dataset from datasets\n",
    "# ops from ultralytics.utils\n",
    "# YOLODataset from ultralytics.data.dataset (for collate_fn reference)\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils import ops\n",
    "from ultralytics.data.dataset import YOLODataset # For collate_fn if using standard\n",
    "from datasets import load_dataset # Assuming you use this\n",
    "\n",
    "N = 50 # Collect garbage every 50 batches (adjust as needed)\n",
    "\n",
    "\n",
    "# Make sure your original HFYOLODataset class definition is available\n",
    "# Example placeholder - replace with your actual class definition if needed\n",
    "# from your_dataset_module import HFYOLODataset, yolo_hf_collate_fn\n",
    "\n",
    "# --- Custom Evaluation Loop for YOLO ---\n",
    "def evaluate_yolo_with_torchmetrics(model, dataloader, device, conf_thres=0.001, iou_thres=0.5):\n",
    "    \"\"\"\n",
    "    Evaluates a YOLO model using torchmetrics mAP with a custom loop.\n",
    "\n",
    "    Args:\n",
    "        model: The loaded YOLO model object.\n",
    "        dataloader: DataLoader for the test set (using original HFYOLODataset).\n",
    "        device: The torch device ('cuda' or 'cpu').\n",
    "        conf_thres (float): Confidence threshold for predictions.\n",
    "        iou_thres (float): IoU threshold for NMS used in model.predict.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the computed mAP metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"conf_thres: {conf_thres}\")\n",
    "    print(f\"iou_thres: {iou_thres}\")\n",
    "\n",
    "    # Ensure model is on the correct device and in evaluation mode\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize the mAP metric\n",
    "    # Ensure box_format matches the format of boxes you provide ('xyxy')\n",
    "    map_metric = torchmetrics.detection.MeanAveragePrecision(box_format='xyxy', iou_type='bbox').to(device)\n",
    "\n",
    "    print(f\"Starting custom evaluation loop on device: {device}\")\n",
    "    progress_bar = tqdm(enumerate(dataloader), desc=\"Evaluating\")\n",
    "\n",
    "    for batcch_index_i, batch in progress_bar:\n",
    "        # Move batch data to the device\n",
    "        # Assumes collate_fn provides 'img', 'bboxes' (norm xywh), 'cls', 'batch_idx', 'ori_shape'\n",
    "        # Use try-except for robustness against missing keys if collate fn varies\n",
    "        try:\n",
    "            samples = batch['img'].to(device).float() / 255.0 # Normalize images\n",
    "            gt_bboxes_norm = batch['bboxes'].to(device) # Normalized xywh\n",
    "            # Ensure gt_cls is 1D\n",
    "            gt_cls = batch['cls'].to(device).squeeze() # Add squeeze()\n",
    "            batch_idx = batch['batch_idx'].to(device)\n",
    "            original_shapes = batch['ori_shape'] # List of tuples [(h, w), ...]\n",
    "        except KeyError as e:\n",
    "            print(f\"\\nError: Missing key {e} in batch. Check dataset and collate_fn.\")\n",
    "            print(f\"Batch keys: {batch.keys()}\")\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing batch data: {e}\")\n",
    "            raise e\n",
    "\n",
    "\n",
    "        if samples.shape[0] == 0: # Handle empty batches if they occur\n",
    "             print(\"Warning: Skipping empty batch.\")\n",
    "             continue\n",
    "\n",
    "        batch_size = samples.shape[0]\n",
    "        resized_shape = samples.shape[2:] # Shape after transforms (e.g., (320, 320))\n",
    "\n",
    "        # --- Run Inference ---\n",
    "        with torch.no_grad():\n",
    "            # Use model.predict for easier handling of results and NMS\n",
    "            preds = model.predict(samples, conf=conf_thres, iou=iou_thres, verbose=False)\n",
    "            # 'preds' is typically a list of Results objects, one per image\n",
    "\n",
    "        # --- Prepare Predictions and Targets for Metric ---\n",
    "        preds_for_metric = []\n",
    "        targets_for_metric = []\n",
    "\n",
    "        # Process Predictions\n",
    "        for i in range(batch_size):\n",
    "            result = preds[i] # Ultralytics Results object for image i\n",
    "            original_shape_i = original_shapes[i] # Original (h, w)\n",
    "\n",
    "            # Clone the tensor to prevent in-place modification error\n",
    "            pred_boxes_resized_xyxy = result.boxes.xyxy.clone() # Clone the tensor\n",
    "            pred_scores = result.boxes.conf             # Tensor [N]\n",
    "             # Ensure pred_labels is 1D and Long type\n",
    "            pred_labels = result.boxes.cls.long().squeeze() # <<< FIX: Use .long() and ensure squeeze\n",
    "\n",
    "            # Ensure pred_labels is 1D, even if squeeze resulted in a scalar\n",
    "            if pred_labels.ndim == 0:\n",
    "                # If prediction is scalar, it means single prediction. Unsqueeze.\n",
    "                # Also handle case where result might be truly empty\n",
    "                if result.boxes.cls.numel() > 0: # Check original number before squeeze\n",
    "                     pred_labels = pred_labels.unsqueeze(0) # Convert scalar tensor to 1D tensor [1]\n",
    "                else: # If original was empty, ensure label is empty 1D\n",
    "                     pred_labels = torch.empty(0, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "            # Scale boxes from resized_shape to original_shape\n",
    "            if pred_boxes_resized_xyxy.numel() > 0:\n",
    "                 # ops.scale_boxes expects shape (h, w)\n",
    "                 scaled_pred_boxes = ops.scale_boxes(resized_shape, pred_boxes_resized_xyxy, original_shape_i)\n",
    "                 # Ensure labels/scores match boxes after potential scaling/filtering if needed\n",
    "                 if scaled_pred_boxes.shape[0] == 0 and pred_labels.shape[0] > 0:\n",
    "                     print(f\"Warning: Scaled boxes became empty but labels exist for pred item {i}. Forcing labels/scores empty.\")\n",
    "                     pred_labels = torch.empty(0, dtype=torch.long, device=device) # Ensure it's size [0] and long\n",
    "                     pred_scores = torch.empty(0, dtype=torch.float32, device=device)\n",
    "                 elif scaled_pred_boxes.shape[0] != pred_labels.shape[0]:\n",
    "                      print(f\"Warning: Mismatch after scaling boxes ({scaled_pred_boxes.shape[0]}) vs labels ({pred_labels.shape[0]}) for pred item {i}. Check scaling logic.\")\n",
    "                      # Attempt to keep only labels/scores corresponding to potentially remaining boxes if possible, otherwise clear?\n",
    "                      # For safety, maybe clear this prediction if inconsistent? Or trust result.boxes structure?\n",
    "                      # Assuming result.boxes components are consistent in length N initially.\n",
    "                      # If scaling removes boxes, it implies coords were invalid. Let's keep labels/scores for now.\n",
    "                      pass # Let metric handle potential issues if shapes mismatch internally? Risky.\n",
    "\n",
    "            else:\n",
    "                 # If no boxes initially, ensure labels/scores are also empty and 1D ([0])\n",
    "                 scaled_pred_boxes = torch.empty((0, 4), device=device)\n",
    "                 pred_labels = torch.empty(0, dtype=torch.long, device=device) # Ensure it's size [0] and long\n",
    "                 pred_scores = torch.empty(0, dtype=torch.float32, device=device) # Ensure scores are also size [0]\n",
    "\n",
    "\n",
    "            preds_for_metric.append({\n",
    "                'boxes': scaled_pred_boxes, # Absolute xyxy\n",
    "                'scores': pred_scores,\n",
    "                'labels': pred_labels, # Already ensured to be 1D Long\n",
    "            })\n",
    "\n",
    "        # Process Ground Truth Targets\n",
    "        for i in range(batch_size):\n",
    "            mask = (batch_idx == i)\n",
    "            img_boxes_norm = gt_bboxes_norm[mask] # Normalized xywh\n",
    "            img_labels = gt_cls[mask] # gt_cls is already squeezed.\n",
    "\n",
    "            # Ensure img_labels is 1D, even if mask selects only one item\n",
    "            if img_labels.ndim == 0:\n",
    "                 # Check if original gt_cls[mask] had elements before making it 1D\n",
    "                 if gt_cls[mask].numel() > 0:\n",
    "                     img_labels = img_labels.unsqueeze(0) # Convert scalar tensor to 1D tensor [1]\n",
    "                 else: # If original was empty, ensure label is empty 1D\n",
    "                     img_labels = torch.empty(0, dtype=torch.long, device=device)\n",
    "\n",
    "            img_h, img_w = original_shapes[i] # Original H, W\n",
    "\n",
    "            if img_boxes_norm.numel() > 0:\n",
    "                # Denormalize from xywh [0,1] to absolute xywh\n",
    "                img_boxes_abs_xywh = img_boxes_norm * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32, device=device)\n",
    "                # Convert absolute xywh to absolute xyxy\n",
    "                # Assumes input 'bboxes' from dataset/collate are center_x, center_y, width, height\n",
    "                gt_boxes_abs_xyxy = ops.xywh2xyxy(img_boxes_abs_xywh)\n",
    "\n",
    "                # Clamp boxes to image dimensions\n",
    "                gt_boxes_abs_xyxy = ops.clip_boxes(gt_boxes_abs_xyxy, (img_h, img_w))\n",
    "            else:\n",
    "                # If no boxes, ensure labels are also empty and 1D ([0])\n",
    "                gt_boxes_abs_xyxy = torch.empty((0, 4), device=device)\n",
    "                img_labels = torch.empty(0, dtype=torch.long, device=device) # Ensure it's size [0]\n",
    "\n",
    "\n",
    "            targets_for_metric.append({\n",
    "                'boxes': gt_boxes_abs_xyxy, # Absolute xyxy\n",
    "                'labels': img_labels.long(), # <<< FIX: Ensure labels are torch.long >>>\n",
    "            })\n",
    "\n",
    "        # --- Update Metric ---\n",
    "        try:\n",
    "            # Ensure consistency in case of empty predictions/targets for an image\n",
    "            if len(preds_for_metric) != len(targets_for_metric):\n",
    "                 print(f\"\\nError: Mismatch between processed predictions ({len(preds_for_metric)}) and targets ({len(targets_for_metric)}) count in batch.\")\n",
    "                 # Skip update for this batch might be safest\n",
    "                 continue\n",
    "\n",
    "            map_metric.update(preds_for_metric, targets_for_metric)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError updating map_metric in batch: {e}\")\n",
    "            # Provide more context on error\n",
    "            print(f\"Batch Index: {progress_bar.n}\")\n",
    "            print(f\"Number of preds: {len(preds_for_metric)}, Number of targets: {len(targets_for_metric)}\")\n",
    "            if preds_for_metric:\n",
    "                 print(f\"Preds[0] keys: {preds_for_metric[0].keys()}\")\n",
    "                 print(f\"  boxes shape: {preds_for_metric[0]['boxes'].shape}, dtype: {preds_for_metric[0]['boxes'].dtype}\")\n",
    "                 print(f\"  scores shape: {preds_for_metric[0]['scores'].shape}, dtype: {preds_for_metric[0]['scores'].dtype}\")\n",
    "                 print(f\"  labels shape: {preds_for_metric[0]['labels'].shape}, dtype: {preds_for_metric[0]['labels'].dtype}\")\n",
    "            if targets_for_metric:\n",
    "                 print(f\"Targets[0] keys: {targets_for_metric[0].keys()}\")\n",
    "                 print(f\"  boxes shape: {targets_for_metric[0]['boxes'].shape}, dtype: {targets_for_metric[0]['boxes'].dtype}\")\n",
    "                 print(f\"  labels shape: {targets_for_metric[0]['labels'].shape}, dtype: {targets_for_metric[0]['labels'].dtype}\")\n",
    "            raise e # Stop evaluation on error\n",
    "\n",
    "        # --- Explicit Garbage Collection ---\n",
    "        # Check if it's time to collect garbage\n",
    "        if (batcch_index_i + 1) % N == 0:\n",
    "            # print(f\"Batch {batcch_index_i + 1}: Triggering gc.collect()\")\n",
    "            # Optional: Explicitly delete large variables from this iteration if possible\n",
    "            try:\n",
    "                del preds # Example: delete model output tensor\n",
    "                del batch   # Example: delete batch data\n",
    "                del samples\n",
    "                del gt_bboxes_norm\n",
    "                del gt_cls\n",
    "                del batch_idx\n",
    "                del original_shapes\n",
    "            except NameError:\n",
    "                pass # In case they weren't created or already deleted\n",
    "\n",
    "            gc.collect() # Force garbage collection\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        del preds # Example: delete model output tensor\n",
    "        del batch   # Example: delete batch data\n",
    "        del samples\n",
    "    except NameError:\n",
    "        pass # In case they weren't created or already deleted\n",
    "\n",
    "    gc.collect() # Force garbage collection\n",
    "\n",
    "\n",
    "    # --- Compute Final Metrics ---\n",
    "    print(\"Computing final metrics...\")\n",
    "    processed_results = {} # Initialize before try block\n",
    "    try:\n",
    "        map_results = map_metric.compute()\n",
    "        # Process results into a flat dictionary for easier logging\n",
    "        for k, v in map_results.items():\n",
    "             if isinstance(v, torch.Tensor):\n",
    "                 processed_results[k] = v.item() if v.numel() == 1 else v.tolist()\n",
    "             else:\n",
    "                 processed_results[k] = v\n",
    "        print(f\"Computed Metrics: {processed_results}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing final metrics: {e}\")\n",
    "        # Assign error message to the dict if compute fails\n",
    "        processed_results = {\"error\": str(e)}\n",
    "\n",
    "\n",
    "    return processed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gjHrMM8xXBy6"
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Main EVAL_OWN Function (using custom loop) ---\n",
    "def main_fn(checkpoint_path, ds_loader):\n",
    "    \"\"\"\n",
    "    Main function to evaluate a YOLO model using the custom evaluation loop.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path (str): Path to the trained YOLO model checkpoint (.pt file).\n",
    "        hf_dataset_id (str): Hugging Face dataset identifier.\n",
    "        dataset_split (str): Split to evaluate ('test', 'validation').\n",
    "        imgsz (int): Image size used for evaluation.\n",
    "        batch_size (int): Batch size for the dataloader.\n",
    "        workers (int): Number of workers for the dataloader.\n",
    "        device (str, optional): Device ('cuda', 'cpu'). Auto-detects if None.\n",
    "        trust_remote_code (bool): Whether to trust remote code for HF dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with evaluation metrics (e.g., mAP).\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting EVAL_OWN (custom loop) ---\")\n",
    "    # --- 1. Setup Device ---\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- 2. Load Model ---\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "    print(f\"Loading model from: {checkpoint_path}\")\n",
    "    model = YOLO(checkpoint_path)\n",
    "    print(\"Model loaded.\")\n",
    "\n",
    "    # --- 4. Run Custom Evaluation ---\n",
    "    print(\"Calling custom evaluation function...\")\n",
    "    results = evaluate_yolo_with_torchmetrics(\n",
    "        model=model,\n",
    "        dataloader=ds_loader,\n",
    "        device=device\n",
    "        # Pass conf/iou thresholds if you want to override defaults\n",
    "    )\n",
    "\n",
    "    # --- 5. Return Results ---\n",
    "    print(f\"--- EVAL_OWN Finished ---\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ok-G_R0EXBy7",
    "outputId": "cb85a1fb-9ecd-42f7-ee27-0ac047bc71d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using best checkpoint: ../not_tracked_dir/output_yolo_v8_2025-04-11/weights/best.pt\n",
      "--- Starting EVAL_OWN (custom loop) ---\n",
      "Using device: cuda\n",
      "Loading model from: ../not_tracked_dir/output_yolo_v8_2025-04-11/weights/best.pt\n",
      "Model loaded.\n",
      "Calling custom evaluation function...\n",
      "conf_thres: 0.001\n",
      "iou_thres: 0.5\n",
      "Starting custom evaluation loop on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 782it [13:05,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final metrics...\n",
      "Computed Metrics: {'map': 0.9237134456634521, 'map_50': 0.9629759788513184, 'map_75': 0.9419248104095459, 'map_small': 0.9237366318702698, 'map_medium': -1.0, 'map_large': -1.0, 'mar_1': 0.7469073534011841, 'mar_10': 0.9397873878479004, 'mar_100': 0.9397911429405212, 'mar_small': 0.9397911429405212, 'mar_medium': -1.0, 'mar_large': -1.0, 'map_per_class': -1.0, 'mar_100_per_class': -1.0, 'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}\n",
      "--- EVAL_OWN Finished ---\n",
      "\n",
      "Final Evaluation Metrics:\n",
      "{'map': 0.9237134456634521, 'map_50': 0.9629759788513184, 'map_75': 0.9419248104095459, 'map_small': 0.9237366318702698, 'map_medium': -1.0, 'map_large': -1.0, 'mar_1': 0.7469073534011841, 'mar_10': 0.9397873878479004, 'mar_100': 0.9397911429405212, 'mar_small': 0.9397911429405212, 'mar_medium': -1.0, 'mar_large': -1.0, 'map_per_class': -1.0, 'mar_100_per_class': -1.0, 'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Find Checkpoint ---\n",
    "\n",
    "print(f\"Using best checkpoint: {checkpoint_to_eval}\")\n",
    "\n",
    "# --- Run Evaluation ---\n",
    "try:\n",
    "    # Make sure load_dataset is defined (even if dummy)\n",
    "    # if 'load_dataset' not in globals():\n",
    "        #  def load_dataset(*args, **kwargs): return list(range(100))\n",
    "    eval_metrics = main_fn(\n",
    "        checkpoint_path=checkpoint_to_eval,\n",
    "        ds_loader=test_loader\n",
    "    )\n",
    "    print(\"\\nFinal Evaluation Metrics:\")\n",
    "    print(eval_metrics)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Evaluation failed: Checkpoint not found - {e}\")\n",
    "except NameError as e:\n",
    "     print(f\"Evaluation failed: A required class or function is not defined - {e}\")\n",
    "     print(\"Ensure HFYOLODataset and its collate function are available.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during evaluation:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vgt5bf5cBGAx"
   },
   "source": [
    "On half dataset\n",
    "\n",
    "Final Evaluation Metrics:\n",
    "{'map': 0.9237134456634521, 'map_50': 0.9629759788513184, 'map_75': 0.9419248104095459, 'map_small': 0.9237366318702698, 'map_medium': -1.0, 'map_large': -1.0, 'mar_1': 0.7469073534011841, 'mar_10': 0.9397873878479004, 'mar_100': 0.9397911429405212, 'mar_small': 0.9397911429405212, 'mar_medium': -1.0, 'mar_large': -1.0, 'map_per_class': -1.0, 'mar_100_per_class': -1.0, 'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "sensor_dropout",
   "language": "python",
   "name": "sensor_dropout"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
