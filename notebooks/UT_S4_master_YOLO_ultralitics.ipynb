{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-14T11:52:18.478270Z"
    },
    "id": "hfk7RLMWXByq",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('/gpfs/helios/home/ploter/projects/MultiSensorDropout/')\n",
    "\n",
    "from engine import evaluate\n",
    "\n",
    "GDRIVE_BASE_PATH = '../not_tracked_dir/'\n",
    "os.makedirs(GDRIVE_BASE_PATH, exist_ok=True)\n",
    "CFG_EXPERIMENT_NAME = \"output_yolo_v8_2025-08-04\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vqsZ1W76XEzP",
    "outputId": "b65f2d03-7a6f-4e10-d26d-e85f4c9d7e6b"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# import os\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # Define the base directory ON YOUR GOOGLE DRIVE where projects should be saved\n",
    "# # IMPORTANT: Make sure this path exists in your Google Drive or create it.\n",
    "# # Example: Create a folder named 'YOLOv8_Training' in the root of your 'MyDrive'\n",
    "# GDRIVE_BASE_PATH = '/content/drive/MyDrive/YOLOv8_Training' # ADJUST THIS PATH AS NEEDED\n",
    "\n",
    "# # Create the base directory on Drive if it doesn't exist\n",
    "# os.makedirs(GDRIVE_BASE_PATH, exist_ok=True)\n",
    "# print(f\"Google Drive mounted. Using base path: {GDRIVE_BASE_PATH}\")\n",
    "# CFG_EXPERIMENT_NAME = \"train_yolo_nano_24k_dataset3\"\n",
    "\n",
    "# !pip install -q ultralytics\n",
    "# !pip install -q datasets\n",
    "# !pip install -q torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCRptqO7HUjG",
    "outputId": "1b49ebe7-0e06-4815-bda4-aec4942dbee3"
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms.functional import resize, to_pil_image, to_tensor\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from types import SimpleNamespace # For config object\n",
    "\n",
    "# Ultralytics imports\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.models.yolo.detect.train import DetectionTrainer # To subclass\n",
    "from ultralytics.utils import ops, checks\n",
    "\n",
    "\n",
    "# --- NEW: Import Ultralytics LetterBox ---\n",
    "try:\n",
    "    # Common location for augmentations\n",
    "    from ultralytics.data.augment import LetterBox\n",
    "except ImportError:\n",
    "    try:\n",
    "        # Alternative location in some versions might be utils\n",
    "        from ultralytics.utils.ops import LetterBox\n",
    "        print(\"Note: Imported LetterBox from ultralytics.utils.ops\")\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Could not import LetterBox from 'ultralytics.data.augment' or 'ultralytics.utils.ops'. \"\n",
    "                          \"Check Ultralytics version or installation.\")\n",
    "# ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRKezob-4En2"
   },
   "source": [
    "## DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LyHdRjMPNuCU"
   },
   "outputs": [],
   "source": [
    "# === In Cell 2: HFYOLODataset Class Definition ===\n",
    "\n",
    "# === Cell 2: HFYOLODataset Class Definition ===\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "# --- Ultralytics Imports ---\n",
    "# Need YOLODataset only for accessing its collate_fn later\n",
    "from ultralytics.data import YOLODataset\n",
    "# Import necessary transform components used in build_transforms\n",
    "from ultralytics.data.augment import LetterBox, Format, Compose # Standard components\n",
    "# v8_transforms might require many more internal imports if used for augmentation\n",
    "# from ultralytics.data.augment import v8_transforms\n",
    "from ultralytics.utils import DEFAULT_CFG, LOGGER # Used in build_transforms logic\n",
    "from ultralytics.utils.instance import Instances # Make sure this is imported\n",
    "\n",
    "\n",
    "class HFYOLODataset(Dataset):\n",
    "\n",
    "    # ... (Keep __init__ and _build_transforms_internal as before) ...\n",
    "    def __init__(self, hf_dataset_split, imgsz=640, stride=32, augment=True, hyp=None, rect=False, task='detect', trust_remote_code=False, prefix=\"\"):\n",
    "        # ... (Same init as previous step) ...\n",
    "        super().__init__() # Base Dataset init\n",
    "        self.imgsz = imgsz; self.augment = augment; self.hyp = hyp if hyp is not None else DEFAULT_CFG; self.rect = rect; self.stride = stride; self.task = task; self.prefix = prefix; self.hf_dataset = hf_dataset_split; self.trust_remote_code = trust_remote_code\n",
    "        self.use_segments = self.task == \"segment\"; self.use_keypoints = self.task == \"pose\"; self.use_obb = self.task == \"obb\"\n",
    "        print(f\"Initializing HFYOLODataset (Using Ultralytics Transforms)...\")\n",
    "        # Determine frame info & nc/names\n",
    "        self.num_videos = len(self.hf_dataset); # ... (rest of frame count/dim logic) ...\n",
    "        if self.num_videos == 0: raise ValueError(\"Empty dataset.\")\n",
    "        try: # Determine frame info\n",
    "            first_vid = self.hf_dataset[0]; first_data = first_vid['video']; # ... get first_frames_shape_approx ...\n",
    "            if not isinstance(first_data, np.ndarray):\n",
    "                 if isinstance(first_data, list) and first_data: first_frame_np = np.array(first_data[0]); first_video_frames_shape_approx = (len(first_data), *first_frame_np.shape)\n",
    "                 else: first_video_frames = np.array(first_data); first_video_frames_shape_approx = first_video_frames.shape\n",
    "            else: first_video_frames_shape_approx = first_data.shape\n",
    "            if len(first_video_frames_shape_approx) == 4: self.num_frames_per_video, self.frame_height, self.frame_width, _ = first_video_frames_shape_approx\n",
    "            elif len(first_video_frames_shape_approx) == 3: self.num_frames_per_video, self.frame_height, self.frame_width = first_video_frames_shape_approx\n",
    "            else: raise ValueError(f\"Unexpected video dimensions: {first_video_frames_shape_approx}\")\n",
    "            if self.num_frames_per_video <= 0: raise ValueError(f\"Non-positive frame count: {self.num_frames_per_video}\")\n",
    "            print(f\"Frames/Video: {self.num_frames_per_video}, Dim: ({self.frame_height}, {self.frame_width})\")\n",
    "        except Exception as e: raise RuntimeError(f\"Failed getting frame info: {e}\") from e\n",
    "        self.total_frames = self.num_videos * self.num_frames_per_video; print(f\"Total train frames: {self.total_frames}\")\n",
    "\n",
    "        self.nc = 10\n",
    "        self.num_classes = self.nc\n",
    "        self.names = {i:str(i) for i in range(self.nc)}\n",
    "        print(f\"Discovered nc={self.nc}, names={self.names}\")\n",
    "        self.data = {'names': self.names, 'nc': self.nc}\n",
    "        # Build Transforms\n",
    "        self.transforms = self._build_transforms_internal(hyp=self.hyp)\n",
    "        print(f\"Transforms created: {self.transforms}\")\n",
    "\n",
    "    def _build_transforms_internal(self, hyp):\n",
    "        # ... (Same as before, includes LetterBox and Format) ...\n",
    "         if self.augment:\n",
    "             LOGGER.warning(\"Augmentation enabled, but using simple LetterBox transform.\")\n",
    "             transforms = Compose([LetterBox(new_shape=(self.imgsz, self.imgsz), scaleup=getattr(hyp, 'scaleup', True), stride=self.stride)])\n",
    "         else:\n",
    "             transforms = Compose([LetterBox(new_shape=(self.imgsz, self.imgsz), scaleup=False, stride=self.stride)])\n",
    "         transforms.append(\n",
    "             Format(bbox_format=\"xywh\",\n",
    "                    normalize=True, return_mask=self.use_segments, return_keypoint=self.use_keypoints, return_obb=self.use_obb, batch_idx=True, mask_ratio=getattr(hyp, 'mask_ratio', 4), mask_overlap=getattr(hyp, 'overlap_mask', True), bgr=getattr(hyp, 'bgr', 0.0) if self.augment else 0.0))\n",
    "         return transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_frames\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index >= self.total_frames: raise IndexError(...)\n",
    "        video_idx = index // self.num_frames_per_video\n",
    "        frame_idx = index % self.num_frames_per_video\n",
    "        try:\n",
    "            video_example = self.hf_dataset[video_idx]\n",
    "            # --- Load raw image (uint8 HWC NumPy) ---\n",
    "            # ... (same logic) ...\n",
    "            video_data = video_example['video']; img_np = np.array(video_data[frame_idx], dtype=np.uint8) if isinstance(video_data, list) else video_data[frame_idx].astype(np.uint8)\n",
    "            if img_np.ndim == 2: img_np = np.stack([img_np]*3, axis=-1)\n",
    "            elif img_np.shape[-1] == 1: img_np = np.concatenate([img_np]*3, axis=-1)\n",
    "            original_h, original_w = img_np.shape[:2]\n",
    "\n",
    "            # --- Load annotations -> ABSOLUTE PIXEL COORDS (xyxy) ---\n",
    "            bboxes = video_example['bboxes'][frame_idx]\n",
    "            labels = video_example['bboxes_labels'][frame_idx]\n",
    "            bboxes_abs_xyxy_list = []\n",
    "            cls_list_of_lists = [] # <<< Change variable name for clarity\n",
    "            if bboxes and labels:\n",
    "                 for bbox, label in zip(bboxes, labels):\n",
    "                    x_min, y_min, w, h = map(float, bbox) # Ensure float\n",
    "                    # Convert xywh to xyxy, clamp to image bounds\n",
    "                    x1 = max(0.0, x_min)\n",
    "                    y1 = max(0.0, y_min)\n",
    "                    x2 = min(float(original_w), x_min + w)\n",
    "                    y2 = min(float(original_h), y_min + h)\n",
    "                    if x2 > x1 and y2 > y1: # Check for valid box area\n",
    "                         bboxes_abs_xyxy_list.append([x1, y1, x2, y2])\n",
    "\n",
    "                          # --- CHANGE 1: Append label as a list ---\n",
    "                         cls_list_of_lists.append([int(label)])\n",
    "                         # --- End Change 1 ---\n",
    "\n",
    "            # === FIX: Ensure bboxes_np has shape [N, 4] even if N=0 ===\n",
    "            if not bboxes_abs_xyxy_list:\n",
    "                # If list is empty, create array with shape (0, 4)\n",
    "                bboxes_np = np.zeros((0, 4), dtype=np.float32)\n",
    "            else:\n",
    "                # If list has items, convert normally\n",
    "                bboxes_np = np.array(bboxes_abs_xyxy_list, dtype=np.float32)\n",
    "            # cls_np is okay as shape (0,) if cls_list is empty\n",
    "            # --- CHANGE 2: Create cls_np with shape (N, 1) or (0, 1) ---\n",
    "            if not cls_list_of_lists:\n",
    "                # Explicitly create shape (0, 1) for empty case\n",
    "                cls_np = np.array([], dtype=np.int64).reshape(0, 1)\n",
    "            else:\n",
    "                # np.array([[0], [2], [0]]) directly creates shape (N, 1)\n",
    "                cls_np = np.array(cls_list_of_lists, dtype=np.int64)\n",
    "            # --- End Change 2 ---\n",
    "\n",
    "            # --- Create Instances object ---\n",
    "            # Instances expects bboxes in xyxy format by default if normalized=False\n",
    "            segments = np.zeros((0, 1000, 2), dtype=np.float32)\n",
    "            instances = Instances(bboxes=bboxes_np, segments=segments, bbox_format='xyxy', normalized=False)\n",
    "\n",
    "            # === START MINIMAL CHANGE ===\n",
    "            # Calculate simple ratio (height_ratio, width_ratio) based on target imgsz and original shape.\n",
    "            # This mimics the structure added by YOLODataset.get_image_and_label before transforms.\n",
    "            # Use float division.\n",
    "            ratio_h = float(self.imgsz) / original_h\n",
    "            ratio_w = float(self.imgsz) / original_w\n",
    "            # Create the simple tuple (rh, rw)\n",
    "            simple_ratio_pad = (ratio_h, ratio_w)\n",
    "            # === END MINIMAL CHANGE ===\n",
    "\n",
    "            # Format expects 'img', 'cls', 'instances'\n",
    "            sample = {\n",
    "                'img': img_np,           # uint8 HWC NumPy\n",
    "                'instances': instances,  # Instances obj with abs pixel xyxy boxes\n",
    "                'cls': cls_np,           # int64 [N] NumPy\n",
    "                'ori_shape': (original_h, original_w), # Add original shape if needed by transforms\n",
    "                'ratio_pad': simple_ratio_pad,\n",
    "            }\n",
    "            # ---------------------------------------------\n",
    "\n",
    "            # --- Apply transforms ---\n",
    "            # This pipeline includes LetterBox and Format\n",
    "            transformed_sample = self.transforms(sample)\n",
    "            # Output should have 'img' (CHW Tensor, uint8), 'cls', 'bboxes' (normalized xywh Tensor), 'batch_idx'\n",
    "            # ------------------------\n",
    "\n",
    "            # --- Add metadata needed by plotting ---\n",
    "            transformed_sample['im_file'] = f\"video_{video_idx}_frame_{frame_idx}.jpg\"\n",
    "            transformed_sample['ori_shape'] = (original_h, original_w) # Ensure ori_shape is present\n",
    "\n",
    "\n",
    "\n",
    "            # --------------------------------------------\n",
    "\n",
    "            return transformed_sample\n",
    "\n",
    "        except Exception as e:\n",
    "             # ... (error handling) ...\n",
    "             print(f\"Error in __getitem__ for index {index}: {e}\")\n",
    "             import traceback; traceback.print_exc()\n",
    "             raise e\n",
    "\n",
    "\n",
    "    # --- Keep compatibility properties ---\n",
    "    @property\n",
    "    def build_type(self): return 'build_detection_dataset'\n",
    "    ## @property\n",
    "    # def data(self): return {'names': self.names, 'nc': self.num_classes} # Trainer handles this now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QieZuznNCUwa"
   },
   "source": [
    "## VALIDATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "p853PubBCUZa"
   },
   "outputs": [],
   "source": [
    "# === Add Custom Validator Class (e.g., Cell 4a) ===\n",
    "from ultralytics.models.yolo.detect import DetectionValidator\n",
    "from ultralytics.utils import LOGGER, emojis # For logging/errors if needed\n",
    "from copy import copy\n",
    "import torch\n",
    "\n",
    "from ultralytics.cfg import get_cfg, get_save_dir\n",
    "from ultralytics.data.utils import check_cls_dataset, check_det_dataset\n",
    "from ultralytics.nn.autobackend import AutoBackend\n",
    "from ultralytics.utils import LOGGER, TQDM, callbacks, colorstr, emojis\n",
    "from ultralytics.utils.checks import check_imgsz\n",
    "from ultralytics.utils.ops import Profile\n",
    "from ultralytics.utils.torch_utils import de_parallel, select_device, smart_inference_mode\n",
    "\n",
    "class CustomDetectionValidator(DetectionValidator):\n",
    "    @smart_inference_mode()\n",
    "    def __call__(self, trainer=None, model=None):\n",
    "        \"\"\"\n",
    "        Execute validation process, running inference on dataloader and computing performance metrics.\n",
    "\n",
    "        Args:\n",
    "            trainer (object, optional): Trainer object that contains the model to validate.\n",
    "            model (nn.Module, optional): Model to validate if not using a trainer.\n",
    "\n",
    "        Returns:\n",
    "            stats (dict): Dictionary containing validation statistics.\n",
    "        \"\"\"\n",
    "        self.training = trainer is not None\n",
    "        augment = self.args.augment and (not self.training)\n",
    "        if self.training:\n",
    "            self.device = trainer.device\n",
    "            self.data = trainer.data\n",
    "            # Force FP16 val during training\n",
    "            self.args.half = self.device.type != \"cpu\" and trainer.amp\n",
    "            model = trainer.ema.ema or trainer.model\n",
    "            model = model.half() if self.args.half else model.float()\n",
    "            # self.model = model\n",
    "            self.loss = torch.zeros_like(trainer.loss_items, device=trainer.device)\n",
    "            self.args.plots &= trainer.stopper.possible_stop or (trainer.epoch == trainer.epochs - 1)\n",
    "            model.eval()\n",
    "        else:\n",
    "            if str(self.args.model).endswith(\".yaml\") and model is None:\n",
    "                LOGGER.warning(\"WARNING ⚠️ validating an untrained model YAML will result in 0 mAP.\")\n",
    "            callbacks.add_integration_callbacks(self)\n",
    "            model = AutoBackend(\n",
    "                weights=model or self.args.model,\n",
    "                device=select_device(self.args.device, self.args.batch),\n",
    "                dnn=self.args.dnn,\n",
    "                data=self.args.data,\n",
    "                fp16=self.args.half,\n",
    "            )\n",
    "            # self.model = model\n",
    "            self.device = model.device  # update device\n",
    "            self.args.half = model.fp16  # update half\n",
    "            stride, pt, jit, engine = model.stride, model.pt, model.jit, model.engine\n",
    "            imgsz = check_imgsz(self.args.imgsz, stride=stride)\n",
    "            if engine:\n",
    "                self.args.batch = model.batch_size\n",
    "            elif not pt and not jit:\n",
    "                self.args.batch = model.metadata.get(\"batch\", 1)  # export.py models default to batch-size 1\n",
    "                LOGGER.info(f\"Setting batch={self.args.batch} input of shape ({self.args.batch}, 3, {imgsz}, {imgsz})\")\n",
    "\n",
    "            if str(self.args.data).split(\".\")[-1] in {\"yaml\", \"yml\"}:\n",
    "                self.data = {} # check_det_dataset(self.args.data)\n",
    "            elif self.args.task == \"classify\":\n",
    "                self.data = check_cls_dataset(self.args.data, split=self.args.split)\n",
    "            else:\n",
    "                raise FileNotFoundError(emojis(f\"Dataset '{self.args.data}' for task={self.args.task} not found ❌\"))\n",
    "\n",
    "            if self.device.type in {\"cpu\", \"mps\"}:\n",
    "                self.args.workers = 0  # faster CPU val as time dominated by inference, not dataloading\n",
    "            if not pt:\n",
    "                self.args.rect = False\n",
    "            self.stride = model.stride  # used in get_dataloader() for padding\n",
    "            self.dataloader = self.dataloader or self.get_dataloader(self.data.get(self.args.split), self.args.batch)\n",
    "\n",
    "            model.eval()\n",
    "            model.warmup(imgsz=(1 if pt else self.args.batch, 3, imgsz, imgsz))  # warmup\n",
    "\n",
    "        self.run_callbacks(\"on_val_start\")\n",
    "        dt = (\n",
    "            Profile(device=self.device),\n",
    "            Profile(device=self.device),\n",
    "            Profile(device=self.device),\n",
    "            Profile(device=self.device),\n",
    "        )\n",
    "        bar = TQDM(self.dataloader, desc=self.get_desc(), total=len(self.dataloader))\n",
    "        self.init_metrics(de_parallel(model))\n",
    "        self.jdict = []  # empty before each val\n",
    "        for batch_i, batch in enumerate(bar):\n",
    "            self.run_callbacks(\"on_val_batch_start\")\n",
    "            self.batch_i = batch_i\n",
    "            # Preprocess\n",
    "            with dt[0]:\n",
    "                batch = self.preprocess(batch)\n",
    "\n",
    "            # Inference\n",
    "            with dt[1]:\n",
    "                preds = model(batch[\"img\"], augment=augment)\n",
    "\n",
    "            # Loss\n",
    "            with dt[2]:\n",
    "                if self.training:\n",
    "                    self.loss += model.loss(batch, preds)[1]\n",
    "\n",
    "            # Postprocess\n",
    "            with dt[3]:\n",
    "                preds = self.postprocess(preds)\n",
    "\n",
    "            self.update_metrics(preds, batch)\n",
    "            if self.args.plots and batch_i < 3:\n",
    "                self.plot_val_samples(batch, batch_i)\n",
    "                self.plot_predictions(batch, preds, batch_i)\n",
    "\n",
    "            self.run_callbacks(\"on_val_batch_end\")\n",
    "        stats = self.get_stats()\n",
    "        self.check_stats(stats)\n",
    "        self.speed = dict(zip(self.speed.keys(), (x.t / len(self.dataloader.dataset) * 1e3 for x in dt)))\n",
    "        self.finalize_metrics()\n",
    "        self.print_results()\n",
    "        self.run_callbacks(\"on_val_end\")\n",
    "        if self.training:\n",
    "            model.float()\n",
    "            results = {**stats, **trainer.label_loss_items(self.loss.cpu() / len(self.dataloader), prefix=\"val\")}\n",
    "            return {k: round(float(v), 5) for k, v in results.items()}  # return results as 5 decimal place floats\n",
    "        else:\n",
    "            LOGGER.info(\n",
    "                \"Speed: {:.1f}ms preprocess, {:.1f}ms inference, {:.1f}ms loss, {:.1f}ms postprocess per image\".format(\n",
    "                    *tuple(self.speed.values())\n",
    "                )\n",
    "            )\n",
    "            if self.args.save_json and self.jdict:\n",
    "                with open(str(self.save_dir / \"predictions.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                    LOGGER.info(f\"Saving {f.name}...\")\n",
    "                    json.dump(self.jdict, f)  # flatten and save\n",
    "                stats = self.eval_json(stats)  # update stats\n",
    "            if self.args.plots or self.args.save_json:\n",
    "                LOGGER.info(f\"Results saved to {colorstr('bold', self.save_dir)}\")\n",
    "            return stats\n",
    "\n",
    "    # --- Keep the _prepare_batch override if it was needed for the 0-D tensor error ---\n",
    "    # def _prepare_batch(self, si, batch):\n",
    "    #      ... (implementation that handles 0-D cls tensor) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-biaMFvu4IES"
   },
   "source": [
    "## TRAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BXIB2EjUHjlS"
   },
   "outputs": [],
   "source": [
    "# from ultralytics.utils.plotting import plot_images, plot_labels, plot_results\n",
    "# Import YOLODataset\n",
    "from ultralytics.data.dataset import YOLODataset\n",
    "\n",
    "class CustomDetectionTrainer(DetectionTrainer):\n",
    "    def __init__(self, trust_remote_code=False, debug=False, *args, **kwargs): # Removed hf_dataset_identifier from signature\n",
    "        print(\"CustomDetectionTrainer __init__ started...\")\n",
    "        self.custom_trust_code = trust_remote_code\n",
    "        self.hf_train_split = None; self.hf_val_split = None\n",
    "        self._debug = debug\n",
    "        # We rely on args.data being the HF identifier now\n",
    "        super().__init__(*args, **kwargs) # Calls overridden get_dataset\n",
    "        print(\"CustomDetectionTrainer __init__ finished.\")\n",
    "        # Verification AFTER get_dataset has run (called by super init)\n",
    "        # if not hasattr(self.args, 'nc') or not isinstance(self.args.nc, int) or self.args.nc <= 0:\n",
    "        #      raise RuntimeError(f\"Trainer self.args.nc not set correctly after get_dataset. Check get_dataset override.\")\n",
    "        # if not hasattr(self.args, 'names') or not isinstance(self.args.names, dict):\n",
    "        #      raise RuntimeError(f\"Trainer self.args.names not set correctly after get_dataset.\")\n",
    "        # print(f\"Trainer initialized: nc={self.args.nc}, names={self.args.names}\")\n",
    "\n",
    "    # --- Override get_dataset ---\n",
    "    def get_dataset(self):\n",
    "        \"\"\"\n",
    "        Overrides the base method. Loads HF dataset using args.data.\n",
    "        Instantiates HFYOLODataset (which discovers nc/names).\n",
    "        Sets args.nc and args.names based on the loaded dataset info.\n",
    "        Sets self.trainset and self.testset.\n",
    "        \"\"\"\n",
    "        print(\"****** Custom get_dataset called ******\")\n",
    "        if not hasattr(self, 'args'): raise RuntimeError(\"Trainer arguments missing.\")\n",
    "\n",
    "        hf_dataset_identifier = 'Max-Ploter/detection-moving-mnist-medium' #self.args.data # Get HF ID from data arg\n",
    "        trust_remote_code = self.custom_trust_code # Get from instance attribute\n",
    "\n",
    "        if not hf_dataset_identifier or not isinstance(hf_dataset_identifier, str):\n",
    "             raise ValueError(f\"HF dataset identifier '{hf_dataset_identifier}' (from args.data) is invalid.\")\n",
    "\n",
    "        # --- Load HF Splits ---\n",
    "        # ... (Load self.hf_train_split / self.hf_val_split using hf_dataset_identifier) ...\n",
    "        if self.hf_train_split is None:\n",
    "            try: self.hf_train_split = load_dataset(hf_dataset_identifier, split='train', trust_remote_code=trust_remote_code); print(f\"Loaded train split: {len(self.hf_train_split)} samples.\")\n",
    "            except Exception as e: raise RuntimeError(f\"Failed load HF train '{hf_dataset_identifier}': {e}\") from e\n",
    "        if self.hf_val_split is None:\n",
    "            try: # Try val then test\n",
    "                try: self.hf_val_split = load_dataset(hf_dataset_identifier, split='validation', trust_remote_code=trust_remote_code); print(\"Using 'validation' split.\")\n",
    "                except ValueError: self.hf_val_split = load_dataset(hf_dataset_identifier, split='test', trust_remote_code=trust_remote_code); print(\"Using 'test' split.\")\n",
    "                if not self.hf_val_split: raise ValueError(\"No val/test split found.\")\n",
    "                print(f\"Loaded val/test split: {len(self.hf_val_split)} samples.\")\n",
    "            except Exception as e: # Fallback split train\n",
    "                 print(f\"Warning: Failed loading val/test: {e}. Splitting train.\")\n",
    "                 # ... (Split train logic) ...\n",
    "                 if self.hf_train_split is None: raise RuntimeError(\"Train split not available.\")\n",
    "                 if len(self.hf_train_split) < 2: raise RuntimeError(\"Train split too small.\")\n",
    "                 splits = self.hf_train_split.train_test_split(test_size=0.2, seed=getattr(self.args, 'seed', 42))\n",
    "                 self.hf_train_split, self.hf_val_split = splits['train'], splits['test']\n",
    "                 print(f\"Used 80/20 split of 'train' for train ({len(self.hf_train_split)})/validation ({len(self.hf_val_split)}).\")\n",
    "\n",
    "        if self._debug:\n",
    "          # reduce train and val split sizes\n",
    "          print(\"Reducing train/val split sizes for debugging...\")\n",
    "          self.hf_train_split = self.hf_train_split.select(range(1))\n",
    "          self.hf_val_split = self.hf_val_split.select(range(1))\n",
    "\n",
    "        # --- Instantiate HFYOLODatasets (which find nc/names) ---\n",
    "        print(\"Instantiating HFYOLODataset for trainset...\")\n",
    "        self.trainset = HFYOLODataset(self.hf_train_split, imgsz=self.args.imgsz, trust_remote_code=trust_remote_code)\n",
    "        print(\"Instantiating HFYOLODataset for testset (validation)...\")\n",
    "        self.testset = HFYOLODataset(self.hf_val_split, imgsz=self.args.imgsz, trust_remote_code=trust_remote_code)\n",
    "\n",
    "        # --- Get nc/names FROM the instantiated dataset ---\n",
    "        known_nc = getattr(self.trainset, 'num_classes', 0)\n",
    "        known_names = getattr(self.trainset, 'names', {})\n",
    "        if known_nc <= 0: # Fallback to testset if trainset failed\n",
    "            known_nc = getattr(self.testset, 'num_classes', 0)\n",
    "            known_names = getattr(self.testset, 'names', {})\n",
    "\n",
    "        if known_nc <= 0:\n",
    "            raise ValueError(\"Could not determine number of classes from loaded HFYOLODataset instances.\")\n",
    "        # --------------------------------------------------\n",
    "\n",
    "        # --- Set nc and names on self.args ---\n",
    "        # This is the CRUCIAL step - fulfilling the presumed responsibility\n",
    "        print(f\"Setting trainer args: nc={known_nc}, names={known_names}\")\n",
    "        # self.args.nc = known_nc\n",
    "        # self.args.names = known_names\n",
    "\n",
    "        # --- <<< NEW: Explicitly set self.data attribute >>> ---\n",
    "        # This dictionary is expected by the original get_model method\n",
    "        self.data = {'nc': known_nc, 'names': known_names}\n",
    "        # Add other keys if get_model relies on them, e.g. 'path' (can be dummy)\n",
    "        # self.data['path'] = '.' # Example if path is needed\n",
    "        print(f\"Set self.data attribute: {self.data}\")\n",
    "        # ----------------------------------------------------\n",
    "\n",
    "        # Optional: Update dataset.data for consistency\n",
    "        if hasattr(self.trainset, 'data'): self.trainset.data = self.data\n",
    "        if hasattr(self.testset, 'data'): self.testset.data = self.data\n",
    "\n",
    "\n",
    "        # ------------------------------------\n",
    "\n",
    "        print(f\"****** Custom get_dataset finished. Set args.nc={known_nc}. ******\")\n",
    "        return self.trainset, self.testset\n",
    "\n",
    "\n",
    "    # --- get_dataloader and plot_training_labels overrides remain the same ---\n",
    "    def get_dataloader(self, dataset=None, batch_size=16, rank=0, mode='train'):\n",
    "        # ... (implementation is unchanged) ...\n",
    "        print(f\"****** Custom get_dataloader called for mode: {mode} ******\")\n",
    "        if dataset is None: dataset = self.trainset if mode == 'train' else self.testset\n",
    "        if not isinstance(dataset, HFYOLODataset): print(f\"Warning: Dataset type {type(dataset)}.\")\n",
    "        batch_size_arg = getattr(self.args, 'batch', batch_size); batch_size_to_use = batch_size_arg if isinstance(batch_size_arg, int) and batch_size_arg > 0 else batch_size\n",
    "        if mode != 'train': batch_size_to_use *= 2\n",
    "        workers = getattr(self.args, 'workers', 0); shuffle = (mode == 'train')\n",
    "        print(f\"Creating DataLoader for mode '{mode}' with batch_size={batch_size_to_use}, workers={workers}...\")\n",
    "        loader = DataLoader(dataset, batch_size=batch_size_to_use, shuffle=shuffle, num_workers=workers, pin_memory=True,\n",
    "                            collate_fn=YOLODataset.collate_fn\n",
    "                            )\n",
    "        print(f\"****** Custom DataLoader created for {mode} ******\")\n",
    "        return loader\n",
    "\n",
    "    def plot_training_labels(self): print(\"Skipping plot_training_labels in Custom Trainer.\"); pass\n",
    "\n",
    "    def get_validator(self):\n",
    "        \"\"\"Returns a CustomDetectionValidator instance.\"\"\"\n",
    "        print(\"****** Custom get_validator called (Returning CustomDetectionValidator) ******\")\n",
    "        # Ensure validation dataloader exists\n",
    "        if not hasattr(self, 'test_loader') or self.test_loader is None:\n",
    "             print(\"Creating validation dataloader within get_validator...\")\n",
    "             if not hasattr(self, 'testset') or self.testset is None: raise RuntimeError(\"Validation dataset missing.\")\n",
    "             val_batch_size = getattr(self.args, 'batch', 16) * 2\n",
    "             self.test_loader = self.get_dataloader(self.testset, batch_size=val_batch_size, mode='val')\n",
    "\n",
    "        validator_args = copy(self.args) # Pass copy of trainer args\n",
    "\n",
    "        # Instantiate OUR custom validator\n",
    "        validator = CustomDetectionValidator( # Use the custom class\n",
    "            dataloader=self.test_loader,\n",
    "            save_dir=self.save_dir,\n",
    "            args=validator_args,\n",
    "            # --- FIX: Pass the main self.callbacks dict directly ---\n",
    "            _callbacks=self.callbacks\n",
    "            # --- End Fix ---\n",
    "        )\n",
    "        # Link model and data dict\n",
    "        validator.model = self.model\n",
    "        validator.data = self.data\n",
    "        print(f\"****** Custom get_validator finished. Validator created. Using data: {validator.data} ******\")\n",
    "        return validator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vK-ZOOk53_Jd"
   },
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eLJi3dhOHm6X",
    "outputId": "079d66de-92be-4b1e-b9ce-586f38c2f262"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to resume training from: ../not_tracked_dir/output_yolo_v8_2025-08-04/weights/last.pt\n",
      "Checkpoint file not found at: ../not_tracked_dir/output_yolo_v8_2025-08-04/weights/last.pt. Cannot resume.\n"
     ]
    }
   ],
   "source": [
    "# === Configuration Cell in Jupyter Notebook ===\n",
    "from types import SimpleNamespace\n",
    "import os\n",
    "\n",
    "# --- Define YAML Content and Filename ---\n",
    "FAKE_YAML_FILENAME = \"fake.yaml\" # Name of the file to create\n",
    "YAML_SHIM_CONTENT = \"\"\"\n",
    "# Minimal YAML to satisfy Ultralytics checks during validation\n",
    "path: ./ignored_path # Ignored, but path key might be checked\n",
    "train: images/train   # Ignored\n",
    "val: images/val       # Ignored\n",
    "\n",
    "# --- Important Part ---\n",
    "nc: 10 # Your known number of classes\n",
    "names:\n",
    "  0: '0'\n",
    "  1: '1'\n",
    "  2: '2'\n",
    "  3: '3'\n",
    "  4: '4'\n",
    "  5: '5'\n",
    "  6: '6'\n",
    "  7: '7'\n",
    "  8: '8'\n",
    "  9: '9'\n",
    "# --- End Important Part ---\n",
    "\"\"\"\n",
    "# ---------------------------------------\n",
    "\n",
    "# --- Configuration Variables ---\n",
    "CFG_HF_DATASET_IDENTIFIER = \"Max-Ploter/detection-moving-mnist-medium\" # Your HF dataset path/name\n",
    "CFG_MODEL_NAME = 'yolov8n.pt'\n",
    "# CFG_NUM_CLASSES = 10  # No longer needed here\n",
    "CFG_EPOCHS = 100\n",
    "CFG_BATCH_SIZE = 64\n",
    "CFG_IMG_SIZE = 320\n",
    "CFG_WORKERS = 2\n",
    "CFG_TRUST_REMOTE_CODE = True # Custom flag for trainer\n",
    "CFG_PROJECT_NAME = \"yolo_hf_custom_trainer\"\n",
    "\n",
    "CFG_PLOTS = True\n",
    "CFG_SAVE_PERIOD = 1 # <<<<<<< ADDED: Set save_period to 1\n",
    "\n",
    "\n",
    "# --- Construct the path to the last checkpoint ---\n",
    "# !!! This path MUST exist from your previous training run !!!\n",
    "checkpoint_dir = os.path.join(GDRIVE_BASE_PATH, CFG_EXPERIMENT_NAME, 'weights')\n",
    "resume_checkpoint_path = os.path.join(checkpoint_dir, 'last.pt')\n",
    "\n",
    "print(f\"Attempting to resume training from: {resume_checkpoint_path}\")\n",
    "\n",
    "# --- Check if the checkpoint exists ---\n",
    "if not os.path.exists(resume_checkpoint_path):\n",
    "    print(f\"Checkpoint file not found at: {resume_checkpoint_path}. Cannot resume.\")\n",
    "    CFG_MODEL_TO_LOAD = None\n",
    "else:\n",
    "    print(\"Checkpoint file found.\")\n",
    "    # --- Set the model path to the checkpoint for resuming ---\n",
    "    CFG_MODEL_TO_LOAD = resume_checkpoint_path\n",
    "\n",
    "\n",
    "# --- Prepare config_args with standard args ---\n",
    "# 'data' now holds the HF identifier\n",
    "# 'nc' is NOT set here\n",
    "config_args = SimpleNamespace(\n",
    "    model = CFG_MODEL_NAME,\n",
    "    data = FAKE_YAML_FILENAME, #CFG_HF_DATASET_IDENTIFIER, # Pass HF ID as data arg\n",
    "    epochs = CFG_EPOCHS,\n",
    "    batch = CFG_BATCH_SIZE,\n",
    "    imgsz = CFG_IMG_SIZE,\n",
    "    project = GDRIVE_BASE_PATH,\n",
    "    name = CFG_EXPERIMENT_NAME,\n",
    "    workers = CFG_WORKERS,\n",
    "    device = None,\n",
    "    plots = CFG_PLOTS,\n",
    "    resume=CFG_MODEL_TO_LOAD,\n",
    "    freeze=22\n",
    "    # trust_remote_code is handled separately\n",
    "    # nc is handled by get_dataset\n",
    ")\n",
    "\n",
    "# with open(FAKE_YAML_FILENAME, 'w') as f:\n",
    "#     f.write(YAML_SHIM_CONTENT)\n",
    "# print(f\"Created fake YAML file: {FAKE_YAML_FILENAME}\")\n",
    "\n",
    "# --- Sanity Check ---\n",
    "if config_args.data == \"your_huggingface_dataset_identifier\": # Or similar check\n",
    "     print(f\"🛑 Error: Please set HF dataset identifier in CFG_HF_DATASET_IDENTIFIER.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vTa5xlaG8XnR",
    "outputId": "cbd4a23e-a4c3-4d01-eba8-1332368641cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Instantiating CustomDetectionTrainer manually...\n",
      "CustomDetectionTrainer __init__ started...\n",
      "Ultralytics 8.3.109 🚀 Python-3.11.11 torch-2.5.1 CUDA:0 (Tesla V100-PCIE-16GB, 16144MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=fake.yaml, epochs=100, time=None, patience=100, batch=64, imgsz=320, save=True, save_period=-1, cache=False, device=None, workers=2, project=../not_tracked_dir/, name=output_yolo_v8_2025-08-04, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=None, amp=True, fraction=1.0, profile=False, freeze=22, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=../not_tracked_dir/output_yolo_v8_2025-08-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'Max-Ploter/detection-moving-mnist-medium' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Custom get_dataset called ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'Max-Ploter/detection-moving-mnist-medium' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train split: 12000 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'Max-Ploter/detection-moving-mnist-medium' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'test' split.\n",
      "Loaded val/test split: 6000 samples.\n",
      "Instantiating HFYOLODataset for trainset...\n",
      "Initializing HFYOLODataset (Using Ultralytics Transforms)...\n",
      "Frames/Video: 20, Dim: (128, 128)\n",
      "Total train frames: 240000\n",
      "Discovered nc=10, names={0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
      "Augmentation enabled, but using simple LetterBox transform.\n",
      "Transforms created: Compose(<ultralytics.data.augment.LetterBox object at 0x14b38fa7a3d0>, <ultralytics.data.augment.Format object at 0x14b38c19aa50>)\n",
      "Instantiating HFYOLODataset for testset (validation)...\n",
      "Initializing HFYOLODataset (Using Ultralytics Transforms)...\n",
      "Frames/Video: 20, Dim: (128, 128)\n",
      "Total train frames: 120000\n",
      "Discovered nc=10, names={0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
      "Augmentation enabled, but using simple LetterBox transform.\n",
      "Transforms created: Compose(<ultralytics.data.augment.LetterBox object at 0x14b38c1d1110>, <ultralytics.data.augment.Format object at 0x14b38c1d2450>)\n",
      "Setting trainer args: nc=10, names={0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
      "Set self.data attribute: {'nc': 10, 'names': {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}}\n",
      "****** Custom get_dataset finished. Set args.nc=10. ******\n",
      "CustomDetectionTrainer __init__ finished.\n",
      "\n",
      "🚀 Starting trainer.train()...\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753262  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \n",
      "Model summary: 129 layers, 3,012,798 parameters, 3,012,782 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.12.cv1.conv.weight'\n",
      "Freezing layer 'model.12.cv1.bn.weight'\n",
      "Freezing layer 'model.12.cv1.bn.bias'\n",
      "Freezing layer 'model.12.cv2.conv.weight'\n",
      "Freezing layer 'model.12.cv2.bn.weight'\n",
      "Freezing layer 'model.12.cv2.bn.bias'\n",
      "Freezing layer 'model.12.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.12.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.12.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.12.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.12.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.12.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.15.cv1.conv.weight'\n",
      "Freezing layer 'model.15.cv1.bn.weight'\n",
      "Freezing layer 'model.15.cv1.bn.bias'\n",
      "Freezing layer 'model.15.cv2.conv.weight'\n",
      "Freezing layer 'model.15.cv2.bn.weight'\n",
      "Freezing layer 'model.15.cv2.bn.bias'\n",
      "Freezing layer 'model.15.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.15.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.15.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.15.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.15.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.15.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.16.conv.weight'\n",
      "Freezing layer 'model.16.bn.weight'\n",
      "Freezing layer 'model.16.bn.bias'\n",
      "Freezing layer 'model.18.cv1.conv.weight'\n",
      "Freezing layer 'model.18.cv1.bn.weight'\n",
      "Freezing layer 'model.18.cv1.bn.bias'\n",
      "Freezing layer 'model.18.cv2.conv.weight'\n",
      "Freezing layer 'model.18.cv2.bn.weight'\n",
      "Freezing layer 'model.18.cv2.bn.bias'\n",
      "Freezing layer 'model.18.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.18.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.18.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.18.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.18.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.18.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.19.conv.weight'\n",
      "Freezing layer 'model.19.bn.weight'\n",
      "Freezing layer 'model.19.bn.bias'\n",
      "Freezing layer 'model.21.cv1.conv.weight'\n",
      "Freezing layer 'model.21.cv1.bn.weight'\n",
      "Freezing layer 'model.21.cv1.bn.bias'\n",
      "Freezing layer 'model.21.cv2.conv.weight'\n",
      "Freezing layer 'model.21.cv2.bn.weight'\n",
      "Freezing layer 'model.21.cv2.bn.bias'\n",
      "Freezing layer 'model.21.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.21.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.21.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.21.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.21.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.21.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "****** Custom get_dataloader called for mode: train ******\n",
      "Creating DataLoader for mode 'train' with batch_size=64, workers=2...\n",
      "****** Custom DataLoader created for train ******\n",
      "****** Custom get_dataloader called for mode: val ******\n",
      "Creating DataLoader for mode 'val' with batch_size=128, workers=2...\n",
      "****** Custom DataLoader created for val ******\n",
      "****** Custom get_validator called (Returning CustomDetectionValidator) ******\n",
      "****** Custom get_validator finished. Validator created. Using data: {'nc': 10, 'names': {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}} ******\n",
      "Skipping plot_training_labels in Custom Trainer.\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 320 train, 320 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1m../not_tracked_dir/output_yolo_v8_2025-08-04\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       Loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/100     0.859G     0.5532      1.046     0.8789        352        320: 100%|██████████| 3750/3750 [29:17<00:00,  2.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 938/938 [16:44<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all     120000     729317      0.917      0.793      0.897      0.787\n",
      "\n",
      "      Epoch    GPU_mem       Loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      2/100      2.02G     0.5042     0.5829     0.8588        367        320: 100%|██████████| 3750/3750 [29:21<00:00,  2.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 938/938 [16:54<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all     120000     729317      0.934      0.814      0.917      0.814\n",
      "\n",
      "      Epoch    GPU_mem       Loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      3/100      2.02G     0.4869     0.5065     0.8533        414        320: 100%|██████████| 3750/3750 [29:30<00:00,  2.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 938/938 [16:43<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all     120000     729317      0.936      0.821      0.922      0.823\n",
      "\n",
      "      Epoch    GPU_mem       Loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      4/100      2.02G     0.4758     0.4839     0.8501        424        320:  65%|██████▌   | 2450/3750 [19:32<10:22,  2.09it/s]Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x14b49f5c4790>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/helios/home/ploter/.conda/envs/sensor_dropout/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "Exception ignored in:     <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x14b49f5c4790>>\n",
      "Traceback (most recent call last):\n",
      "def _clean_thread_parent_frames(  File \"/gpfs/helios/home/ploter/.conda/envs/sensor_dropout/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "def _clean_thread_parent_frames(\n",
      "    \n",
      "\n",
      "\n",
      "KeyboardInterruptKeyboardInterrupt: \n",
      ": \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 15\u001b[0m\n\u001b[1;32m      7\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomDetectionTrainer(\n\u001b[1;32m      8\u001b[0m     overrides\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mvars\u001b[39m(config_args), \u001b[38;5;66;03m# Contains standard args + HF ID in 'data'\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Pass ONLY custom args directly\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mCFG_TRUST_REMOTE_CODE,\n\u001b[1;32m     11\u001b[0m     debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🚀 Starting trainer.train()...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✅ Training finished successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# print(f\"   Best model weights: {trainer.best}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/sensor_dropout/lib/python3.11/site-packages/ultralytics/engine/trainer.py:211\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/sensor_dropout/lib/python3.11/site-packages/ultralytics/engine/trainer.py:385\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp):\n\u001b[1;32m    384\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_batch(batch)\n\u001b[0;32m--> 385\u001b[0m     loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/sensor_dropout/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/sensor_dropout/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/sensor_dropout/lib/python3.11/site-packages/ultralytics/nn/tasks.py:119\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03mPerform forward pass of the model for either training or inference.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    (torch.Tensor): Loss if x is a dict (training), or network predictions (inference).\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/sensor_dropout/lib/python3.11/site-packages/ultralytics/nn/tasks.py:299\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[0;34m(self, batch, preds)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_criterion()\n\u001b[1;32m    298\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[0;32m--> 299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/sensor_dropout/lib/python3.11/site-packages/ultralytics/utils/loss.py:228\u001b[0m, in \u001b[0;36mv8DetectionLoss.__call__\u001b[0;34m(self, preds, batch)\u001b[0m\n\u001b[1;32m    224\u001b[0m pred_bboxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbbox_decode(anchor_points, pred_distri)  \u001b[38;5;66;03m# xyxy, (b, h*w, 4)\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# dfl_conf = pred_distri.view(batch_size, -1, 4, self.reg_max).detach().softmax(-1)\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# dfl_conf = (dfl_conf.amax(-1).mean(-1) + dfl_conf.amax(-1).amin(-1)) / 2\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m _, target_bboxes, target_scores, fg_mask, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massigner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# pred_scores.detach().sigmoid() * 0.8 + dfl_conf.unsqueeze(-1) * 0.2,\u001b[39;49;00m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_bboxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstride_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43manchor_points\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstride_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_gt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m target_scores_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(target_scores\u001b[38;5;241m.\u001b[39msum(), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# Cls loss\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/sensor_dropout/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/sensor_dropout/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/sensor_dropout/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/sensor_dropout/lib/python3.11/site-packages/ultralytics/utils/tal.py:77\u001b[0m, in \u001b[0;36mTaskAlignedAssigner.forward\u001b[0;34m(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     69\u001b[0m         torch\u001b[38;5;241m.\u001b[39mfull_like(pd_scores[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbg_idx),\n\u001b[1;32m     70\u001b[0m         torch\u001b[38;5;241m.\u001b[39mzeros_like(pd_bboxes),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m         torch\u001b[38;5;241m.\u001b[39mzeros_like(pd_scores[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m     74\u001b[0m     )\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manc_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_gt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mOutOfMemoryError:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Move tensors to CPU, compute, then move back to original device\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING: CUDA OutOfMemoryError in TaskAlignedAssigner, using CPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/sensor_dropout/lib/python3.11/site-packages/ultralytics/utils/tal.py:104\u001b[0m, in \u001b[0;36mTaskAlignedAssigner._forward\u001b[0;34m(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt):\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m    Compute the task-aligned assignment.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m        target_gt_idx (torch.Tensor): Target ground truth indices with shape (bs, num_total_anchors).\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     mask_pos, align_metric, overlaps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pos_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpd_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manc_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_gt\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     target_gt_idx, fg_mask, mask_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_highest_overlaps(mask_pos, overlaps, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_max_boxes)\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# Assigned target\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/sensor_dropout/lib/python3.11/site-packages/ultralytics/utils/tal.py:143\u001b[0m, in \u001b[0;36mTaskAlignedAssigner.get_pos_mask\u001b[0;34m(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt)\u001b[0m\n\u001b[1;32m    141\u001b[0m align_metric, overlaps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_box_metrics(pd_scores, pd_bboxes, gt_labels, gt_bboxes, mask_in_gts \u001b[38;5;241m*\u001b[39m mask_gt)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Get topk_metric mask, (b, max_num_obj, h*w)\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m mask_topk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_topk_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43malign_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopk_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_gt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Merge all mask to a final mask, (b, max_num_obj, h*w)\u001b[39;00m\n\u001b[1;32m    145\u001b[0m mask_pos \u001b[38;5;241m=\u001b[39m mask_topk \u001b[38;5;241m*\u001b[39m mask_in_gts \u001b[38;5;241m*\u001b[39m mask_gt\n",
      "File \u001b[0;32m~/.conda/envs/sensor_dropout/lib/python3.11/site-packages/ultralytics/utils/tal.py:217\u001b[0m, in \u001b[0;36mTaskAlignedAssigner.select_topk_candidates\u001b[0;34m(self, metrics, largest, topk_mask)\u001b[0m\n\u001b[1;32m    215\u001b[0m     topk_mask \u001b[38;5;241m=\u001b[39m (topk_metrics\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\u001b[38;5;241m.\u001b[39mexpand_as(topk_idxs)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# (b, max_num_obj, topk)\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m \u001b[43mtopk_idxs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mtopk_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# (b, max_num_obj, topk, h*w) -> (b, max_num_obj, h*w)\u001b[39;00m\n\u001b[1;32m    220\u001b[0m count_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(metrics\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint8, device\u001b[38;5;241m=\u001b[39mtopk_idxs\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# === Execution Cell in Jupyter Notebook ===\n",
    "# Ensure CustomDetectionTrainer etc are defined\n",
    "\n",
    "if config_args.data != \"your_huggingface_dataset_identifier\": # Or similar check\n",
    "    print(\"🚀 Instantiating CustomDetectionTrainer manually...\")\n",
    "    try:\n",
    "        trainer = CustomDetectionTrainer(\n",
    "            overrides=vars(config_args), # Contains standard args + HF ID in 'data'\n",
    "            # Pass ONLY custom args directly\n",
    "            trust_remote_code=CFG_TRUST_REMOTE_CODE,\n",
    "            debug=False\n",
    "        )\n",
    "\n",
    "        print(\"\\n🚀 Starting trainer.train()...\")\n",
    "        trainer.train()\n",
    "        print(\"\\n✅ Training finished successfully!\")\n",
    "        # print(f\"   Best model weights: {trainer.best}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Training failed with an error:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"⚠️ Training not started. Please set HF dataset identifier.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMAQgMs4ItDi"
   },
   "source": [
    "## EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ekKbyq_tXfiE",
    "outputId": "8d576dd9-095c-4bcb-d682-06f17db7014c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'Max-Ploter/detection-moving-mnist-medium' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 'best.pt', evaluating it: ../not_tracked_dir/output_yolo_v8_2025-07-182/weights/best.pt\n",
      "Selected checkpoint for evaluation: ../not_tracked_dir/output_yolo_v8_2025-07-182/weights/best.pt\n",
      "Loading trained model checkpoint...\n",
      "Loading 'test' split from Hugging Face...\n",
      "Loaded test split: 6000 samples.\n",
      "Reducing test split sizes for debugging...\n",
      "Reduced test split to 5000 samples.\n",
      "Initializing HFYOLODataset (Using Ultralytics Transforms)...\n",
      "Frames/Video: 20, Dim: (128, 128)\n",
      "Total train frames: 100000\n",
      "Discovered nc=10, names={0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
      "Augmentation enabled, but using simple LetterBox transform.\n",
      "Transforms created: Compose(<ultralytics.data.augment.LetterBox object at 0x14ec39e3b550>, <ultralytics.data.augment.Format object at 0x14ec38112d90>)\n",
      "Test dataset created.\n",
      "Creating test dataloader...\n",
      "Test dataloader created.\n",
      "Preparing validator...\n",
      "Test evaluation results will be saved in: ../not_tracked_dir/output_yolo_v8_2025-07-182/test_eval_best\n",
      "Test data info: nc=10, names={0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
      "Validator instance created and configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from types import SimpleNamespace\n",
    "from ultralytics import YOLO  # Import the YOLO class\n",
    "from ultralytics.data.dataset import YOLODataset # For collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset # Assuming this is used in HFYOLODataset\n",
    "\n",
    "# --- 1. Define the Checkpoint to Evaluate ---\n",
    "# Usually, you evaluate 'best.pt', but after 1 epoch, 'last.pt' is more likely\n",
    "checkpoint_dir = os.path.join(GDRIVE_BASE_PATH, CFG_EXPERIMENT_NAME, 'weights')\n",
    "checkpoint_to_eval = os.path.join(checkpoint_dir, 'best.pt') # Standard choice\n",
    "\n",
    "# Check if the chosen checkpoint exists\n",
    "best_checkpoint_path = os.path.join(checkpoint_dir, 'best.pt')\n",
    "if os.path.exists(best_checkpoint_path):\n",
    "    checkpoint_to_eval = best_checkpoint_path\n",
    "    print(f\"Found 'best.pt', evaluating it: {checkpoint_to_eval}\")\n",
    "elif os.path.exists(os.path.join(checkpoint_dir, 'last.pt')):\n",
    "    checkpoint_to_eval = os.path.join(checkpoint_dir, 'last.pt')\n",
    "    print(f\"Found 'last.pt', evaluating it: {checkpoint_to_eval}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Neither 'best.pt' nor 'last.pt' found in {checkpoint_dir}\")\n",
    "\n",
    "if not os.path.exists(checkpoint_to_eval):\n",
    "     raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_to_eval}\")\n",
    "print(f\"Selected checkpoint for evaluation: {checkpoint_to_eval}\")\n",
    "\n",
    "# --- 2. Load the Trained Model ---\n",
    "print(\"Loading trained model checkpoint...\")\n",
    "# This loads the model weights into the YOLO architecture\n",
    "#model = YOLO(checkpoint_to_eval)\n",
    "#print(\"Model loaded.\")\n",
    "\n",
    "# --- 3. Load the Test Dataset ---\n",
    "print(\"Loading 'test' split from Hugging Face...\")\n",
    "try:\n",
    "    hf_test_split = load_dataset(CFG_HF_DATASET_IDENTIFIER, split='test', trust_remote_code=CFG_TRUST_REMOTE_CODE)\n",
    "    print(f\"Loaded test split: {len(hf_test_split)} samples.\")\n",
    "\n",
    "    if True: # Debug\n",
    "      print(\"Reducing test split sizes for debugging...\")\n",
    "      hf_test_split = hf_test_split.select(range(5000))\n",
    "      print(f\"Reduced test split to {len(hf_test_split)} samples.\")\n",
    "\n",
    "    # Create the HFYOLODataset instance for the test split\n",
    "    # Ensure HFYOLODataset class is defined/imported\n",
    "    test_dataset = HFYOLODataset(hf_test_split, imgsz=CFG_IMG_SIZE, trust_remote_code=CFG_TRUST_REMOTE_CODE)\n",
    "    print(\"Test dataset created.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to load or process test split: {e}\") from e\n",
    "\n",
    "# --- 4. Create the Test DataLoader ---\n",
    "print(\"Creating test dataloader...\")\n",
    "# Use a larger batch size for validation/testing is common\n",
    "test_batch_size = CFG_BATCH_SIZE * 2\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=test_batch_size,\n",
    "    shuffle=False, # No need to shuffle for evaluation\n",
    "    num_workers=CFG_WORKERS,\n",
    "    pin_memory=True,\n",
    "    collate_fn=YOLODataset.collate_fn # Use the standard collate_fn\n",
    ")\n",
    "print(\"Test dataloader created.\")\n",
    "\n",
    "# --- 5. Prepare Validator Arguments and Instantiate Validator ---\n",
    "print(\"Preparing validator...\")\n",
    "# Define a directory to save test evaluation results/plots\n",
    "test_eval_save_dir = os.path.join(GDRIVE_BASE_PATH, CFG_EXPERIMENT_NAME, f'test_eval_{os.path.basename(checkpoint_to_eval).split(\".\")[0]}')\n",
    "os.makedirs(test_eval_save_dir, exist_ok=True)\n",
    "print(f\"Test evaluation results will be saved in: {test_eval_save_dir}\")\n",
    "\n",
    "# Get nc and names from the test dataset (should match training)\n",
    "test_nc = getattr(test_dataset, 'num_classes', 0)\n",
    "test_names = getattr(test_dataset, 'names', {})\n",
    "if test_nc <= 0:\n",
    "    raise ValueError(\"Could not determine number of classes from test dataset.\")\n",
    "test_data_dict = {'nc': test_nc, 'names': test_names}\n",
    "print(f\"Test data info: nc={test_nc}, names={test_names}\")\n",
    "\n",
    "# Prepare minimal arguments needed by the Validator\n",
    "# Check your CustomDetectionValidator's __init__ and methods if it needs more args\n",
    "validator_args = SimpleNamespace(\n",
    "    data = FAKE_YAML_FILENAME,\n",
    "    save_dir=test_eval_save_dir,\n",
    "    device=None, # Use the device determined during training setup\n",
    "    batch=test_batch_size,\n",
    "    imgsz=CFG_IMG_SIZE,\n",
    "    split='test',    # Specify the split being evaluated\n",
    "    task='detect',   # Specify the task\n",
    "    plots=True,      # Enable saving plots (e.g., confusion matrix, PR curves)\n",
    "    save_json=False, # Set True if you need COCO format JSON output\n",
    "    # save_hybrid=False,\n",
    "    conf=0.001,      # Confidence threshold (adjust if needed)\n",
    "    iou=0.6,         # IoU threshold for NMS (adjust if needed)\n",
    "    max_det=10,\n",
    "    # data=FAKE_YAML_FILENAME, # Maybe needed if validator parses it, but we set validator.data directly\n",
    "    project=GDRIVE_BASE_PATH, # Not directly used by validator usually, but good practice\n",
    "    name=os.path.basename(test_eval_save_dir), # Logical name for this eval run\n",
    ")\n",
    "\n",
    "# Instantiate your CustomDetectionValidator\n",
    "# Ensure the CustomDetectionValidator class is defined/imported\n",
    "validator = CustomDetectionValidator(\n",
    "    dataloader=test_loader,\n",
    "    save_dir=Path(test_eval_save_dir),\n",
    "    args=validator_args,\n",
    "    _callbacks={} # Pass empty callbacks if not needed for pure evaluation\n",
    ")\n",
    "\n",
    "# Link the loaded model (usually the internal torch model) and data info\n",
    "# The YOLO object holds the model in '.model'\n",
    "# validator.model = model\n",
    "validator.data = test_data_dict # Provide nc/names directly\n",
    "\n",
    "print(\"Validator instance created and configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-8DmR7SJIn6",
    "outputId": "b578ed15-0d4d-4785-9c29-27e260dea796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on the test set...\n",
      "Ultralytics 8.3.109 🚀 Python-3.11.11 torch-2.5.1 CUDA:0 (Tesla V100-PCIE-16GB, 16144MiB)\n",
      "Model summary (fused): 72 layers, 3,007,598 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 782/782 [15:28<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all     100000     606046      0.962      0.852      0.925      0.916\n",
      "                     0      46315      66476      0.959      0.893      0.948      0.941\n",
      "                     1      43976      63643      0.934      0.831      0.909      0.893\n",
      "                     2      43471      60886      0.971      0.858      0.929       0.92\n",
      "                     3      44223      62093      0.959      0.856      0.927      0.918\n",
      "                     4      41116      55906      0.973      0.848      0.923      0.911\n",
      "                     5      31423      39226      0.966      0.849      0.923      0.912\n",
      "                     6      44563      63031      0.974      0.855      0.932      0.922\n",
      "                     7      45390      65104      0.952      0.834      0.916      0.906\n",
      "                     8      46906      66950      0.972      0.864      0.934      0.926\n",
      "                     9      44837      62731       0.96      0.828      0.914      0.905\n",
      "Speed: 0.0ms preprocess, 0.3ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1m../not_tracked_dir/output_yolo_v8_2025-07-182/test_eval_best\u001b[0m\n",
      "Evaluation finished.\n",
      "\n",
      "--- Test Set Evaluation Metrics ---\n",
      "\n",
      "❌ Evaluation failed with an error:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1456124/2331695043.py\", line 13, in <module>\n",
      "    map50_95 = results.maps[0] # mAP50-95 for class 0 (or overall if only 1 class reported directly)\n",
      "               ^^^^^^^^^^^^\n",
      "AttributeError: 'dict' object has no attribute 'maps'\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Run Evaluation ---\n",
    "print(\"Starting evaluation on the test set...\")\n",
    "try:\n",
    "    # The validator instance is callable and runs the evaluation loop\n",
    "    results = validator(model = checkpoint_to_eval)\n",
    "    print(\"Evaluation finished.\")\n",
    "\n",
    "    # --- 7. Print Results ---\n",
    "    # The 'results' object (often a dictionary) contains the metrics\n",
    "    # Refer to ultralytics documentation or inspect the 'results' keys for specifics\n",
    "    print(\"\\n--- Test Set Evaluation Metrics ---\")\n",
    "    # Common metrics for detection:\n",
    "    map50_95 = results.maps[0] # mAP50-95 for class 0 (or overall if only 1 class reported directly)\n",
    "    map50 = results.maps[50] # mAP50 for class 0 (or overall)\n",
    "    print(f\"mAP50-95: {map50_95:.4f}\")\n",
    "    print(f\"mAP50:    {map50:.4f}\")\n",
    "    # Print all metrics found\n",
    "    print(\"\\nFull metrics dictionary:\")\n",
    "    print(results.metrics_data) # Or just print(results) depending on object type\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Evaluation failed with an error:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found 'best.pt', evaluating it: ../not_tracked_dir/output_yolo_v8_2025-04-11/weights/best.pt\n",
    "Selected checkpoint for evaluation: ../not_tracked_dir/output_yolo_v8_2025-04-11/weights/best.pt\n",
    "Loading trained model checkpoint...\n",
    "Loading 'test' split from Hugging Face...\n",
    "Generating train split: 24000 examples [00:06, 3754.70 examples/s]\n",
    "Generating test split: 10000 examples [00:02, 4042.25 examples/s]\n",
    "Loaded test split: 10000 samples.\n",
    "Initializing HFYOLODataset (Using Ultralytics Transforms)...\n",
    "Frames/Video: 20, Dim: (128, 128)\n",
    "Total train frames: 200000\n",
    "Discovered nc=10, names={0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
    "Augmentation enabled, but using simple LetterBox transform.\n",
    "\n",
    "Transforms created: Compose(<ultralytics.data.augment.LetterBox object at 0x154bae4eda30>, <ultralytics.data.augment.Format object at 0x154bae48f580>)\n",
    "Test dataset created.\n",
    "Creating test dataloader...\n",
    "Test dataloader created.\n",
    "Preparing validator...\n",
    "Test evaluation results will be saved in: ../not_tracked_dir/output_yolo_v8_2025-04-11/test_eval_best\n",
    "Test data info: nc=10, names={0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
    "Validator instance created and configured.\n",
    "Starting evaluation on the test set...\n",
    "Ultralytics 8.3.107 🚀 Python-3.9.12 torch-2.5.1+cu124 CUDA:0 (Tesla V100-PCIE-16GB, 16144MiB)\n",
    "Model summary (fused): 72 layers, 3,007,598 parameters, 0 gradients, 8.1 GFLOPs\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 2/1563 [00:07<1:25:49,  3.30s/it]\n",
    "Downloading https://ultralytics.com/assets/Arial.ttf to '/gpfs/helios/home/ploter/.config/Ultralytics/Arial.ttf'...\n",
    "\n",
    "100%|██████████| 755k/755k [00:00<00:00, 14.3MB/s]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1563/1563 [28:50<00:00,  1.11s/it]\n",
    "                   all     200000     851740      0.968      0.919      0.971      0.939\n",
    "                     0      65391      82934      0.966      0.944       0.98      0.959\n",
    "                     1      73283      95499      0.962      0.909      0.966      0.922\n",
    "                     2      70732      91277      0.981      0.921      0.972      0.928\n",
    "                     3      67769      86001      0.964       0.92      0.972      0.938\n",
    "                     4      66242      82726      0.967      0.926      0.972       0.94\n",
    "                     5      61559      75659      0.968      0.921      0.973      0.938\n",
    "                     6      63775      79171      0.975       0.92      0.974       0.95\n",
    "                     7      69464      87833      0.959      0.908      0.967      0.936\n",
    "                     8      67065      85789      0.973      0.923      0.974      0.943\n",
    "                     9      66988      84851      0.967      0.898      0.964       0.94\n",
    "Speed: 0.0ms preprocess, 0.3ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
    "Results saved to ../not_tracked_dir/output_yolo_v8_2025-04-11/test_eval_best\n",
    "Evaluation finished.\n",
    "\n",
    "--- Test Set Evaluation Metrics ---\n",
    "\n",
    "❌ Evaluation failed with an error:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fB8Slc3-XBy5"
   },
   "source": [
    "## EVAL_OWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3qQrYoXGXBy6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "\n",
    "# Assuming necessary imports from your environment are available:\n",
    "# HFYOLODataset class (your original version)\n",
    "# yolo_hf_collate_fn (or YOLODataset.collate_fn if compatible)\n",
    "# YOLO class from ultralytics\n",
    "# load_dataset from datasets\n",
    "# ops from ultralytics.utils\n",
    "# YOLODataset from ultralytics.data.dataset (for collate_fn reference)\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils import ops\n",
    "from ultralytics.data.dataset import YOLODataset # For collate_fn if using standard\n",
    "from datasets import load_dataset # Assuming you use this\n",
    "\n",
    "N = 50 # Collect garbage every 50 batches (adjust as needed)\n",
    "\n",
    "\n",
    "# Make sure your original HFYOLODataset class definition is available\n",
    "# Example placeholder - replace with your actual class definition if needed\n",
    "# from your_dataset_module import HFYOLODataset, yolo_hf_collate_fn\n",
    "\n",
    "# --- Custom Evaluation Loop for YOLO ---\n",
    "def evaluate_yolo_with_torchmetrics(model, dataloader, device, conf_thres=0.001, iou_thres=0.5):\n",
    "    \"\"\"\n",
    "    Evaluates a YOLO model using torchmetrics mAP with a custom loop.\n",
    "\n",
    "    Args:\n",
    "        model: The loaded YOLO model object.\n",
    "        dataloader: DataLoader for the test set (using original HFYOLODataset).\n",
    "        device: The torch device ('cuda' or 'cpu').\n",
    "        conf_thres (float): Confidence threshold for predictions.\n",
    "        iou_thres (float): IoU threshold for NMS used in model.predict.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the computed mAP metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"conf_thres: {conf_thres}\")\n",
    "    print(f\"iou_thres: {iou_thres}\")\n",
    "\n",
    "    # Ensure model is on the correct device and in evaluation mode\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize the mAP metric\n",
    "    # Ensure box_format matches the format of boxes you provide ('xyxy')\n",
    "    map_metric = torchmetrics.detection.MeanAveragePrecision(box_format='xyxy', iou_type='bbox').to(device)\n",
    "\n",
    "    print(f\"Starting custom evaluation loop on device: {device}\")\n",
    "    progress_bar = tqdm(enumerate(dataloader), desc=\"Evaluating\")\n",
    "\n",
    "    for batcch_index_i, batch in progress_bar:\n",
    "        # Move batch data to the device\n",
    "        # Assumes collate_fn provides 'img', 'bboxes' (norm xywh), 'cls', 'batch_idx', 'ori_shape'\n",
    "        # Use try-except for robustness against missing keys if collate fn varies\n",
    "        try:\n",
    "            samples = batch['img'].to(device).float() / 255.0 # Normalize images\n",
    "            gt_bboxes_norm = batch['bboxes'].to(device) # Normalized xywh\n",
    "            # Ensure gt_cls is 1D\n",
    "            gt_cls = batch['cls'].to(device).squeeze() # Add squeeze()\n",
    "            batch_idx = batch['batch_idx'].to(device)\n",
    "            original_shapes = batch['ori_shape'] # List of tuples [(h, w), ...]\n",
    "        except KeyError as e:\n",
    "            print(f\"\\nError: Missing key {e} in batch. Check dataset and collate_fn.\")\n",
    "            print(f\"Batch keys: {batch.keys()}\")\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing batch data: {e}\")\n",
    "            raise e\n",
    "\n",
    "\n",
    "        if samples.shape[0] == 0: # Handle empty batches if they occur\n",
    "             print(\"Warning: Skipping empty batch.\")\n",
    "             continue\n",
    "\n",
    "        batch_size = samples.shape[0]\n",
    "        resized_shape = samples.shape[2:] # Shape after transforms (e.g., (320, 320))\n",
    "\n",
    "        # --- Run Inference ---\n",
    "        with torch.no_grad():\n",
    "            # Use model.predict for easier handling of results and NMS\n",
    "            preds = model.predict(samples, conf=conf_thres, iou=iou_thres, verbose=False)\n",
    "            # 'preds' is typically a list of Results objects, one per image\n",
    "\n",
    "        # --- Prepare Predictions and Targets for Metric ---\n",
    "        preds_for_metric = []\n",
    "        targets_for_metric = []\n",
    "\n",
    "        # Process Predictions\n",
    "        for i in range(batch_size):\n",
    "            result = preds[i] # Ultralytics Results object for image i\n",
    "            original_shape_i = original_shapes[i] # Original (h, w)\n",
    "\n",
    "            # Clone the tensor to prevent in-place modification error\n",
    "            pred_boxes_resized_xyxy = result.boxes.xyxy.clone() # Clone the tensor\n",
    "            pred_scores = result.boxes.conf             # Tensor [N]\n",
    "             # Ensure pred_labels is 1D and Long type\n",
    "            pred_labels = result.boxes.cls.long().squeeze() # <<< FIX: Use .long() and ensure squeeze\n",
    "\n",
    "            # Ensure pred_labels is 1D, even if squeeze resulted in a scalar\n",
    "            if pred_labels.ndim == 0:\n",
    "                # If prediction is scalar, it means single prediction. Unsqueeze.\n",
    "                # Also handle case where result might be truly empty\n",
    "                if result.boxes.cls.numel() > 0: # Check original number before squeeze\n",
    "                     pred_labels = pred_labels.unsqueeze(0) # Convert scalar tensor to 1D tensor [1]\n",
    "                else: # If original was empty, ensure label is empty 1D\n",
    "                     pred_labels = torch.empty(0, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "            # Scale boxes from resized_shape to original_shape\n",
    "            if pred_boxes_resized_xyxy.numel() > 0:\n",
    "                 # ops.scale_boxes expects shape (h, w)\n",
    "                 scaled_pred_boxes = ops.scale_boxes(resized_shape, pred_boxes_resized_xyxy, original_shape_i)\n",
    "                 # Ensure labels/scores match boxes after potential scaling/filtering if needed\n",
    "                 if scaled_pred_boxes.shape[0] == 0 and pred_labels.shape[0] > 0:\n",
    "                     print(f\"Warning: Scaled boxes became empty but labels exist for pred item {i}. Forcing labels/scores empty.\")\n",
    "                     pred_labels = torch.empty(0, dtype=torch.long, device=device) # Ensure it's size [0] and long\n",
    "                     pred_scores = torch.empty(0, dtype=torch.float32, device=device)\n",
    "                 elif scaled_pred_boxes.shape[0] != pred_labels.shape[0]:\n",
    "                      print(f\"Warning: Mismatch after scaling boxes ({scaled_pred_boxes.shape[0]}) vs labels ({pred_labels.shape[0]}) for pred item {i}. Check scaling logic.\")\n",
    "                      # Attempt to keep only labels/scores corresponding to potentially remaining boxes if possible, otherwise clear?\n",
    "                      # For safety, maybe clear this prediction if inconsistent? Or trust result.boxes structure?\n",
    "                      # Assuming result.boxes components are consistent in length N initially.\n",
    "                      # If scaling removes boxes, it implies coords were invalid. Let's keep labels/scores for now.\n",
    "                      pass # Let metric handle potential issues if shapes mismatch internally? Risky.\n",
    "\n",
    "            else:\n",
    "                 # If no boxes initially, ensure labels/scores are also empty and 1D ([0])\n",
    "                 scaled_pred_boxes = torch.empty((0, 4), device=device)\n",
    "                 pred_labels = torch.empty(0, dtype=torch.long, device=device) # Ensure it's size [0] and long\n",
    "                 pred_scores = torch.empty(0, dtype=torch.float32, device=device) # Ensure scores are also size [0]\n",
    "\n",
    "\n",
    "            preds_for_metric.append({\n",
    "                'boxes': scaled_pred_boxes, # Absolute xyxy\n",
    "                'scores': pred_scores,\n",
    "                'labels': pred_labels, # Already ensured to be 1D Long\n",
    "            })\n",
    "\n",
    "        # Process Ground Truth Targets\n",
    "        for i in range(batch_size):\n",
    "            mask = (batch_idx == i)\n",
    "            img_boxes_norm = gt_bboxes_norm[mask] # Normalized xywh\n",
    "            img_labels = gt_cls[mask] # gt_cls is already squeezed.\n",
    "\n",
    "            # Ensure img_labels is 1D, even if mask selects only one item\n",
    "            if img_labels.ndim == 0:\n",
    "                 # Check if original gt_cls[mask] had elements before making it 1D\n",
    "                 if gt_cls[mask].numel() > 0:\n",
    "                     img_labels = img_labels.unsqueeze(0) # Convert scalar tensor to 1D tensor [1]\n",
    "                 else: # If original was empty, ensure label is empty 1D\n",
    "                     img_labels = torch.empty(0, dtype=torch.long, device=device)\n",
    "\n",
    "            img_h, img_w = original_shapes[i] # Original H, W\n",
    "\n",
    "            if img_boxes_norm.numel() > 0:\n",
    "                # Denormalize from xywh [0,1] to absolute xywh\n",
    "                img_boxes_abs_xywh = img_boxes_norm * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32, device=device)\n",
    "                # Convert absolute xywh to absolute xyxy\n",
    "                # Assumes input 'bboxes' from dataset/collate are center_x, center_y, width, height\n",
    "                gt_boxes_abs_xyxy = ops.xywh2xyxy(img_boxes_abs_xywh)\n",
    "\n",
    "                # Clamp boxes to image dimensions\n",
    "                gt_boxes_abs_xyxy = ops.clip_boxes(gt_boxes_abs_xyxy, (img_h, img_w))\n",
    "            else:\n",
    "                # If no boxes, ensure labels are also empty and 1D ([0])\n",
    "                gt_boxes_abs_xyxy = torch.empty((0, 4), device=device)\n",
    "                img_labels = torch.empty(0, dtype=torch.long, device=device) # Ensure it's size [0]\n",
    "\n",
    "\n",
    "            targets_for_metric.append({\n",
    "                'boxes': gt_boxes_abs_xyxy, # Absolute xyxy\n",
    "                'labels': img_labels.long(), # <<< FIX: Ensure labels are torch.long >>>\n",
    "            })\n",
    "\n",
    "        # --- Update Metric ---\n",
    "        try:\n",
    "            # Ensure consistency in case of empty predictions/targets for an image\n",
    "            if len(preds_for_metric) != len(targets_for_metric):\n",
    "                 print(f\"\\nError: Mismatch between processed predictions ({len(preds_for_metric)}) and targets ({len(targets_for_metric)}) count in batch.\")\n",
    "                 # Skip update for this batch might be safest\n",
    "                 continue\n",
    "\n",
    "            map_metric.update(preds_for_metric, targets_for_metric)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError updating map_metric in batch: {e}\")\n",
    "            # Provide more context on error\n",
    "            print(f\"Batch Index: {progress_bar.n}\")\n",
    "            print(f\"Number of preds: {len(preds_for_metric)}, Number of targets: {len(targets_for_metric)}\")\n",
    "            if preds_for_metric:\n",
    "                 print(f\"Preds[0] keys: {preds_for_metric[0].keys()}\")\n",
    "                 print(f\"  boxes shape: {preds_for_metric[0]['boxes'].shape}, dtype: {preds_for_metric[0]['boxes'].dtype}\")\n",
    "                 print(f\"  scores shape: {preds_for_metric[0]['scores'].shape}, dtype: {preds_for_metric[0]['scores'].dtype}\")\n",
    "                 print(f\"  labels shape: {preds_for_metric[0]['labels'].shape}, dtype: {preds_for_metric[0]['labels'].dtype}\")\n",
    "            if targets_for_metric:\n",
    "                 print(f\"Targets[0] keys: {targets_for_metric[0].keys()}\")\n",
    "                 print(f\"  boxes shape: {targets_for_metric[0]['boxes'].shape}, dtype: {targets_for_metric[0]['boxes'].dtype}\")\n",
    "                 print(f\"  labels shape: {targets_for_metric[0]['labels'].shape}, dtype: {targets_for_metric[0]['labels'].dtype}\")\n",
    "            raise e # Stop evaluation on error\n",
    "\n",
    "        # --- Explicit Garbage Collection ---\n",
    "        # Check if it's time to collect garbage\n",
    "        if (batcch_index_i + 1) % N == 0:\n",
    "            # print(f\"Batch {batcch_index_i + 1}: Triggering gc.collect()\")\n",
    "            # Optional: Explicitly delete large variables from this iteration if possible\n",
    "            try:\n",
    "                del preds # Example: delete model output tensor\n",
    "                del batch   # Example: delete batch data\n",
    "                del samples\n",
    "                del gt_bboxes_norm\n",
    "                del gt_cls\n",
    "                del batch_idx\n",
    "                del original_shapes\n",
    "            except NameError:\n",
    "                pass # In case they weren't created or already deleted\n",
    "\n",
    "            gc.collect() # Force garbage collection\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        del preds # Example: delete model output tensor\n",
    "        del batch   # Example: delete batch data\n",
    "        del samples\n",
    "    except NameError:\n",
    "        pass # In case they weren't created or already deleted\n",
    "\n",
    "    gc.collect() # Force garbage collection\n",
    "\n",
    "\n",
    "    # --- Compute Final Metrics ---\n",
    "    print(\"Computing final metrics...\")\n",
    "    processed_results = {} # Initialize before try block\n",
    "    try:\n",
    "        map_results = map_metric.compute()\n",
    "        # Process results into a flat dictionary for easier logging\n",
    "        for k, v in map_results.items():\n",
    "             if isinstance(v, torch.Tensor):\n",
    "                 processed_results[k] = v.item() if v.numel() == 1 else v.tolist()\n",
    "             else:\n",
    "                 processed_results[k] = v\n",
    "        print(f\"Computed Metrics: {processed_results}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing final metrics: {e}\")\n",
    "        # Assign error message to the dict if compute fails\n",
    "        processed_results = {\"error\": str(e)}\n",
    "\n",
    "\n",
    "    return processed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gjHrMM8xXBy6"
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Main EVAL_OWN Function (using custom loop) ---\n",
    "def main_fn(checkpoint_path, ds_loader):\n",
    "    \"\"\"\n",
    "    Main function to evaluate a YOLO model using the custom evaluation loop.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path (str): Path to the trained YOLO model checkpoint (.pt file).\n",
    "        hf_dataset_id (str): Hugging Face dataset identifier.\n",
    "        dataset_split (str): Split to evaluate ('test', 'validation').\n",
    "        imgsz (int): Image size used for evaluation.\n",
    "        batch_size (int): Batch size for the dataloader.\n",
    "        workers (int): Number of workers for the dataloader.\n",
    "        device (str, optional): Device ('cuda', 'cpu'). Auto-detects if None.\n",
    "        trust_remote_code (bool): Whether to trust remote code for HF dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with evaluation metrics (e.g., mAP).\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting EVAL_OWN (custom loop) ---\")\n",
    "    # --- 1. Setup Device ---\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- 2. Load Model ---\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "    print(f\"Loading model from: {checkpoint_path}\")\n",
    "    model = YOLO(checkpoint_path)\n",
    "    print(\"Model loaded.\")\n",
    "\n",
    "    # --- 4. Run Custom Evaluation ---\n",
    "    print(\"Calling custom evaluation function...\")\n",
    "    results = evaluate_yolo_with_torchmetrics(\n",
    "        model=model,\n",
    "        dataloader=ds_loader,\n",
    "        device=device\n",
    "        # Pass conf/iou thresholds if you want to override defaults\n",
    "    )\n",
    "\n",
    "    # --- 5. Return Results ---\n",
    "    print(f\"--- EVAL_OWN Finished ---\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ok-G_R0EXBy7",
    "outputId": "cb85a1fb-9ecd-42f7-ee27-0ac047bc71d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using best checkpoint: ../not_tracked_dir/output_yolo_v8_2025-04-11/weights/best.pt\n",
      "--- Starting EVAL_OWN (custom loop) ---\n",
      "Using device: cuda\n",
      "Loading model from: ../not_tracked_dir/output_yolo_v8_2025-04-11/weights/best.pt\n",
      "Model loaded.\n",
      "Calling custom evaluation function...\n",
      "conf_thres: 0.001\n",
      "iou_thres: 0.5\n",
      "Starting custom evaluation loop on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 782it [13:05,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final metrics...\n",
      "Computed Metrics: {'map': 0.9237134456634521, 'map_50': 0.9629759788513184, 'map_75': 0.9419248104095459, 'map_small': 0.9237366318702698, 'map_medium': -1.0, 'map_large': -1.0, 'mar_1': 0.7469073534011841, 'mar_10': 0.9397873878479004, 'mar_100': 0.9397911429405212, 'mar_small': 0.9397911429405212, 'mar_medium': -1.0, 'mar_large': -1.0, 'map_per_class': -1.0, 'mar_100_per_class': -1.0, 'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}\n",
      "--- EVAL_OWN Finished ---\n",
      "\n",
      "Final Evaluation Metrics:\n",
      "{'map': 0.9237134456634521, 'map_50': 0.9629759788513184, 'map_75': 0.9419248104095459, 'map_small': 0.9237366318702698, 'map_medium': -1.0, 'map_large': -1.0, 'mar_1': 0.7469073534011841, 'mar_10': 0.9397873878479004, 'mar_100': 0.9397911429405212, 'mar_small': 0.9397911429405212, 'mar_medium': -1.0, 'mar_large': -1.0, 'map_per_class': -1.0, 'mar_100_per_class': -1.0, 'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Find Checkpoint ---\n",
    "\n",
    "print(f\"Using best checkpoint: {checkpoint_to_eval}\")\n",
    "\n",
    "# --- Run Evaluation ---\n",
    "try:\n",
    "    # Make sure load_dataset is defined (even if dummy)\n",
    "    # if 'load_dataset' not in globals():\n",
    "        #  def load_dataset(*args, **kwargs): return list(range(100))\n",
    "    eval_metrics = main_fn(\n",
    "        checkpoint_path=checkpoint_to_eval,\n",
    "        ds_loader=test_loader\n",
    "    )\n",
    "    print(\"\\nFinal Evaluation Metrics:\")\n",
    "    print(eval_metrics)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Evaluation failed: Checkpoint not found - {e}\")\n",
    "except NameError as e:\n",
    "     print(f\"Evaluation failed: A required class or function is not defined - {e}\")\n",
    "     print(\"Ensure HFYOLODataset and its collate function are available.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during evaluation:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vgt5bf5cBGAx"
   },
   "source": [
    "On half dataset\n",
    "\n",
    "Final Evaluation Metrics:\n",
    "{'map': 0.9237134456634521, 'map_50': 0.9629759788513184, 'map_75': 0.9419248104095459, 'map_small': 0.9237366318702698, 'map_medium': -1.0, 'map_large': -1.0, 'mar_1': 0.7469073534011841, 'mar_10': 0.9397873878479004, 'mar_100': 0.9397911429405212, 'mar_small': 0.9397911429405212, 'mar_medium': -1.0, 'mar_large': -1.0, 'map_per_class': -1.0, 'mar_100_per_class': -1.0, 'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "sensor_dropout",
   "language": "python",
   "name": "sensor_dropout"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
