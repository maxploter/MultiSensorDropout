{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the base directory ON YOUR GOOGLE DRIVE where projects should be saved\n",
        "# IMPORTANT: Make sure this path exists in your Google Drive or create it.\n",
        "# Example: Create a folder named 'YOLOv8_Training' in the root of your 'MyDrive'\n",
        "GDRIVE_BASE_PATH = '/content/drive/MyDrive/YOLOv8_Training' # ADJUST THIS PATH AS NEEDED\n",
        "\n",
        "# Create the base directory on Drive if it doesn't exist\n",
        "os.makedirs(GDRIVE_BASE_PATH, exist_ok=True)\n",
        "print(f\"Google Drive mounted. Using base path: {GDRIVE_BASE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pwGMFuzJhAp",
        "outputId": "02d8c670-eb15-4ac6-9e37-2827d5e191e3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted. Using base path: /content/drive/MyDrive/YOLOv8_Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8_zKZGe6q1a",
        "outputId": "00351b5a-1585-4b95-959f-4e9bd9ff50e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.107-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.14.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2024.12.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.107-py3-none-any.whl (974 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.5/974.5 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.107 ultralytics-thop-2.0.14\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard imports\n",
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms.functional import resize, to_pil_image, to_tensor\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset\n",
        "from types import SimpleNamespace # For config object\n",
        "\n",
        "# Ultralytics imports\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.models.yolo.detect.train import DetectionTrainer # To subclass\n",
        "from ultralytics.utils import ops, checks\n",
        "\n",
        "\n",
        "# --- NEW: Import Ultralytics LetterBox ---\n",
        "try:\n",
        "    # Common location for augmentations\n",
        "    from ultralytics.data.augment import LetterBox\n",
        "except ImportError:\n",
        "    try:\n",
        "        # Alternative location in some versions might be utils\n",
        "        from ultralytics.utils.ops import LetterBox\n",
        "        print(\"Note: Imported LetterBox from ultralytics.utils.ops\")\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Could not import LetterBox from 'ultralytics.data.augment' or 'ultralytics.utils.ops'. \"\n",
        "                          \"Check Ultralytics version or installation.\")\n",
        "# ---\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCRptqO7HUjG",
        "outputId": "a57efa7e-34e9-462c-8806-f1104c154b2f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DS"
      ],
      "metadata": {
        "id": "yRKezob-4En2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === In Cell 2: HFYOLODataset Class Definition ===\n",
        "\n",
        "# === Cell 2: HFYOLODataset Class Definition ===\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "from pathlib import Path\n",
        "from copy import deepcopy\n",
        "\n",
        "# --- Ultralytics Imports ---\n",
        "# Need YOLODataset only for accessing its collate_fn later\n",
        "from ultralytics.data import YOLODataset\n",
        "# Import necessary transform components used in build_transforms\n",
        "from ultralytics.data.augment import LetterBox, Format, Compose # Standard components\n",
        "# v8_transforms might require many more internal imports if used for augmentation\n",
        "# from ultralytics.data.augment import v8_transforms\n",
        "from ultralytics.utils import DEFAULT_CFG, LOGGER # Used in build_transforms logic\n",
        "from ultralytics.utils.instance import Instances # Make sure this is imported\n",
        "\n",
        "\n",
        "class HFYOLODataset(Dataset):\n",
        "\n",
        "    # ... (Keep __init__ and _build_transforms_internal as before) ...\n",
        "    def __init__(self, hf_dataset_split, imgsz=640, stride=32, augment=True, hyp=None, rect=False, task='detect', trust_remote_code=False, prefix=\"\"):\n",
        "        # ... (Same init as previous step) ...\n",
        "        super().__init__() # Base Dataset init\n",
        "        self.imgsz = imgsz; self.augment = augment; self.hyp = hyp if hyp is not None else DEFAULT_CFG; self.rect = rect; self.stride = stride; self.task = task; self.prefix = prefix; self.hf_dataset = hf_dataset_split; self.trust_remote_code = trust_remote_code\n",
        "        self.use_segments = self.task == \"segment\"; self.use_keypoints = self.task == \"pose\"; self.use_obb = self.task == \"obb\"\n",
        "        print(f\"Initializing HFYOLODataset (Using Ultralytics Transforms)...\")\n",
        "        # Determine frame info & nc/names\n",
        "        self.num_videos = len(self.hf_dataset); # ... (rest of frame count/dim logic) ...\n",
        "        if self.num_videos == 0: raise ValueError(\"Empty dataset.\")\n",
        "        try: # Determine frame info\n",
        "            first_vid = self.hf_dataset[0]; first_data = first_vid['video']; # ... get first_frames_shape_approx ...\n",
        "            if not isinstance(first_data, np.ndarray):\n",
        "                 if isinstance(first_data, list) and first_data: first_frame_np = np.array(first_data[0]); first_video_frames_shape_approx = (len(first_data), *first_frame_np.shape)\n",
        "                 else: first_video_frames = np.array(first_data); first_video_frames_shape_approx = first_video_frames.shape\n",
        "            else: first_video_frames_shape_approx = first_data.shape\n",
        "            if len(first_video_frames_shape_approx) == 4: self.num_frames_per_video, self.frame_height, self.frame_width, _ = first_video_frames_shape_approx\n",
        "            elif len(first_video_frames_shape_approx) == 3: self.num_frames_per_video, self.frame_height, self.frame_width = first_video_frames_shape_approx\n",
        "            else: raise ValueError(f\"Unexpected video dimensions: {first_video_frames_shape_approx}\")\n",
        "            if self.num_frames_per_video <= 0: raise ValueError(f\"Non-positive frame count: {self.num_frames_per_video}\")\n",
        "            print(f\"Frames/Video: {self.num_frames_per_video}, Dim: ({self.frame_height}, {self.frame_width})\")\n",
        "        except Exception as e: raise RuntimeError(f\"Failed getting frame info: {e}\") from e\n",
        "        self.total_frames = self.num_videos * self.num_frames_per_video; print(f\"Total train frames: {self.total_frames}\")\n",
        "\n",
        "        self.nc = 10\n",
        "        self.num_classes = self.nc\n",
        "        self.names = {i:str(i) for i in range(self.nc)}\n",
        "        print(f\"Discovered nc={self.nc}, names={self.names}\")\n",
        "        self.data = {'names': self.names, 'nc': self.nc}\n",
        "        # Build Transforms\n",
        "        self.transforms = self._build_transforms_internal(hyp=self.hyp)\n",
        "        print(f\"Transforms created: {self.transforms}\")\n",
        "\n",
        "    def _build_transforms_internal(self, hyp):\n",
        "        # ... (Same as before, includes LetterBox and Format) ...\n",
        "         if self.augment:\n",
        "             LOGGER.warning(\"Augmentation enabled, but using simple LetterBox transform.\")\n",
        "             transforms = Compose([LetterBox(new_shape=(self.imgsz, self.imgsz), scaleup=getattr(hyp, 'scaleup', True), stride=self.stride)])\n",
        "         else:\n",
        "             transforms = Compose([LetterBox(new_shape=(self.imgsz, self.imgsz), scaleup=False, stride=self.stride)])\n",
        "         transforms.append(\n",
        "             Format(bbox_format=\"xywh\",\n",
        "                    normalize=True, return_mask=self.use_segments, return_keypoint=self.use_keypoints, return_obb=self.use_obb, batch_idx=True, mask_ratio=getattr(hyp, 'mask_ratio', 4), mask_overlap=getattr(hyp, 'overlap_mask', True), bgr=getattr(hyp, 'bgr', 0.0) if self.augment else 0.0))\n",
        "         return transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_frames\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if index >= self.total_frames: raise IndexError(...)\n",
        "        video_idx = index // self.num_frames_per_video\n",
        "        frame_idx = index % self.num_frames_per_video\n",
        "        try:\n",
        "            video_example = self.hf_dataset[video_idx]\n",
        "            # --- Load raw image (uint8 HWC NumPy) ---\n",
        "            # ... (same logic) ...\n",
        "            video_data = video_example['video']; img_np = np.array(video_data[frame_idx], dtype=np.uint8) if isinstance(video_data, list) else video_data[frame_idx].astype(np.uint8)\n",
        "            if img_np.ndim == 2: img_np = np.stack([img_np]*3, axis=-1)\n",
        "            elif img_np.shape[-1] == 1: img_np = np.concatenate([img_np]*3, axis=-1)\n",
        "            original_h, original_w = img_np.shape[:2]\n",
        "\n",
        "            # --- Load annotations -> ABSOLUTE PIXEL COORDS (xyxy) ---\n",
        "            bboxes = video_example['bboxes'][frame_idx]\n",
        "            labels = video_example['bboxes_labels'][frame_idx]\n",
        "            bboxes_abs_xyxy_list = []\n",
        "            cls_list_of_lists = [] # <<< Change variable name for clarity\n",
        "            if bboxes and labels:\n",
        "                 for bbox, label in zip(bboxes, labels):\n",
        "                    x_min, y_min, w, h = map(float, bbox) # Ensure float\n",
        "                    # Convert xywh to xyxy, clamp to image bounds\n",
        "                    x1 = max(0.0, x_min)\n",
        "                    y1 = max(0.0, y_min)\n",
        "                    x2 = min(float(original_w), x_min + w)\n",
        "                    y2 = min(float(original_h), y_min + h)\n",
        "                    if x2 > x1 and y2 > y1: # Check for valid box area\n",
        "                         bboxes_abs_xyxy_list.append([x1, y1, x2, y2])\n",
        "\n",
        "                          # --- CHANGE 1: Append label as a list ---\n",
        "                         cls_list_of_lists.append([int(label)])\n",
        "                         # --- End Change 1 ---\n",
        "\n",
        "            # === FIX: Ensure bboxes_np has shape [N, 4] even if N=0 ===\n",
        "            if not bboxes_abs_xyxy_list:\n",
        "                # If list is empty, create array with shape (0, 4)\n",
        "                bboxes_np = np.zeros((0, 4), dtype=np.float32)\n",
        "            else:\n",
        "                # If list has items, convert normally\n",
        "                bboxes_np = np.array(bboxes_abs_xyxy_list, dtype=np.float32)\n",
        "            # cls_np is okay as shape (0,) if cls_list is empty\n",
        "            # --- CHANGE 2: Create cls_np with shape (N, 1) or (0, 1) ---\n",
        "            if not cls_list_of_lists:\n",
        "                # Explicitly create shape (0, 1) for empty case\n",
        "                cls_np = np.array([], dtype=np.int64).reshape(0, 1)\n",
        "            else:\n",
        "                # np.array([[0], [2], [0]]) directly creates shape (N, 1)\n",
        "                cls_np = np.array(cls_list_of_lists, dtype=np.int64)\n",
        "            # --- End Change 2 ---\n",
        "\n",
        "            # --- Create Instances object ---\n",
        "            # Instances expects bboxes in xyxy format by default if normalized=False\n",
        "            segments = np.zeros((0, 1000, 2), dtype=np.float32)\n",
        "            instances = Instances(bboxes=bboxes_np, segments=segments, bbox_format='xyxy', normalized=False)\n",
        "\n",
        "            # === START MINIMAL CHANGE ===\n",
        "            # Calculate simple ratio (height_ratio, width_ratio) based on target imgsz and original shape.\n",
        "            # This mimics the structure added by YOLODataset.get_image_and_label before transforms.\n",
        "            # Use float division.\n",
        "            ratio_h = float(self.imgsz) / original_h\n",
        "            ratio_w = float(self.imgsz) / original_w\n",
        "            # Create the simple tuple (rh, rw)\n",
        "            simple_ratio_pad = (ratio_h, ratio_w)\n",
        "            # === END MINIMAL CHANGE ===\n",
        "\n",
        "            # Format expects 'img', 'cls', 'instances'\n",
        "            sample = {\n",
        "                'img': img_np,           # uint8 HWC NumPy\n",
        "                'instances': instances,  # Instances obj with abs pixel xyxy boxes\n",
        "                'cls': cls_np,           # int64 [N] NumPy\n",
        "                'ori_shape': (original_h, original_w), # Add original shape if needed by transforms\n",
        "                'ratio_pad': simple_ratio_pad,\n",
        "            }\n",
        "            # ---------------------------------------------\n",
        "\n",
        "            # --- Apply transforms ---\n",
        "            # This pipeline includes LetterBox and Format\n",
        "            transformed_sample = self.transforms(sample)\n",
        "            # Output should have 'img' (CHW Tensor, uint8), 'cls', 'bboxes' (normalized xywh Tensor), 'batch_idx'\n",
        "            # ------------------------\n",
        "\n",
        "            # --- Add metadata needed by plotting ---\n",
        "            transformed_sample['im_file'] = f\"video_{video_idx}_frame_{frame_idx}.jpg\"\n",
        "            transformed_sample['ori_shape'] = (original_h, original_w) # Ensure ori_shape is present\n",
        "\n",
        "\n",
        "\n",
        "            # --------------------------------------------\n",
        "\n",
        "            return transformed_sample\n",
        "\n",
        "        except Exception as e:\n",
        "             # ... (error handling) ...\n",
        "             print(f\"Error in __getitem__ for index {index}: {e}\")\n",
        "             import traceback; traceback.print_exc()\n",
        "             raise e\n",
        "\n",
        "\n",
        "    # --- Keep compatibility properties ---\n",
        "    @property\n",
        "    def build_type(self): return 'build_detection_dataset'\n",
        "    ## @property\n",
        "    # def data(self): return {'names': self.names, 'nc': self.num_classes} # Trainer handles this now"
      ],
      "metadata": {
        "id": "LyHdRjMPNuCU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Revised Collate Function ---\n",
        "def yolo_hf_collate_fn(batch):\n",
        "    \"\"\"Collates list of dicts from HFYOLODataset into a batch dict matching Ultralytics internal format.\"\"\"\n",
        "    batch = [b for b in batch if b is not None]\n",
        "    if not batch:\n",
        "        # Return an empty batch structure if collate gets an empty list\n",
        "        return {\n",
        "            'img': torch.empty(0, 3, 640, 640), # Adjust size if needed\n",
        "            'batch_idx': torch.empty(0),\n",
        "            'cls': torch.empty(0),\n",
        "            'bboxes': torch.empty(0, 4)\n",
        "        }\n",
        "\n",
        "    collated = {}\n",
        "    keys = batch[0].keys()\n",
        "\n",
        "    for key in keys:\n",
        "        items = [d[key] for d in batch]\n",
        "        if key == 'img':\n",
        "            # Pad images if they have slightly different sizes after transforms (unlikely with letterbox)\n",
        "            # For simplicity, assume letterbox produces consistent sizes for now.\n",
        "            try:\n",
        "                 collated[key] = torch.stack(items, 0)\n",
        "            except RuntimeError as e:\n",
        "                 print(f\"Error stacking images (check sizes are consistent): {e}\")\n",
        "                 # Fallback or error handling needed here if sizes can vary\n",
        "                 # Find max H, W and pad? For now, raise error.\n",
        "                 raise e\n",
        "        elif key == 'cls' or key == 'bboxes':\n",
        "            collated[key] = items # Keep as list for now\n",
        "        else: # Handle metadata like ratio_pad, shape\n",
        "            collated[key] = items\n",
        "\n",
        "    batch_idx = []\n",
        "    cls_list = []\n",
        "    bboxes_list = []\n",
        "    for i in range(len(batch)):\n",
        "        n_labels = len(collated['cls'][i])\n",
        "        if n_labels > 0:\n",
        "            batch_idx.append(torch.full((n_labels,), i, dtype=torch.int64)) # Use int64 for indices\n",
        "            cls_list.append(collated['cls'][i]) # Already tensors\n",
        "            bboxes_list.append(collated['bboxes'][i]) # Already tensors\n",
        "\n",
        "    # Ensure data types are correct before concatenation\n",
        "    # Use long for batch_idx and cls, float for bboxes\n",
        "    collated['batch_idx'] = torch.cat(batch_idx, 0) if batch_idx else torch.empty(0, dtype=torch.int64)\n",
        "    collated['cls'] = torch.cat(cls_list, 0) if cls_list else torch.empty(0, dtype=torch.long)\n",
        "    collated['bboxes'] = torch.cat(bboxes_list, 0) if bboxes_list else torch.empty((0, 4), dtype=torch.float32)\n",
        "\n",
        "    # Image should already be FloatTensor [0,1] from ToTensor\n",
        "    # The trainer usually handles final device placement and normalization factor (e.g. / 255.0) if needed\n",
        "\n",
        "    return collated\n",
        "\n"
      ],
      "metadata": {
        "id": "jyf6xavPHeiS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VALIDATOR"
      ],
      "metadata": {
        "id": "QieZuznNCUwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Add Custom Validator Class (e.g., Cell 4a) ===\n",
        "from ultralytics.models.yolo.detect import DetectionValidator\n",
        "from ultralytics.utils import LOGGER, emojis # For logging/errors if needed\n",
        "from copy import copy\n",
        "import torch\n",
        "\n",
        "from ultralytics.cfg import get_cfg, get_save_dir\n",
        "from ultralytics.data.utils import check_cls_dataset, check_det_dataset\n",
        "from ultralytics.nn.autobackend import AutoBackend\n",
        "from ultralytics.utils import LOGGER, TQDM, callbacks, colorstr, emojis\n",
        "from ultralytics.utils.checks import check_imgsz\n",
        "from ultralytics.utils.ops import Profile\n",
        "from ultralytics.utils.torch_utils import de_parallel, select_device, smart_inference_mode\n",
        "\n",
        "class CustomDetectionValidator(DetectionValidator):\n",
        "    @smart_inference_mode()\n",
        "    def __call__(self, trainer=None, model=None):\n",
        "        \"\"\"\n",
        "        Execute validation process, running inference on dataloader and computing performance metrics.\n",
        "\n",
        "        Args:\n",
        "            trainer (object, optional): Trainer object that contains the model to validate.\n",
        "            model (nn.Module, optional): Model to validate if not using a trainer.\n",
        "\n",
        "        Returns:\n",
        "            stats (dict): Dictionary containing validation statistics.\n",
        "        \"\"\"\n",
        "        self.training = trainer is not None\n",
        "        augment = self.args.augment and (not self.training)\n",
        "        if self.training:\n",
        "            self.device = trainer.device\n",
        "            self.data = trainer.data\n",
        "            # Force FP16 val during training\n",
        "            self.args.half = self.device.type != \"cpu\" and trainer.amp\n",
        "            model = trainer.ema.ema or trainer.model\n",
        "            model = model.half() if self.args.half else model.float()\n",
        "            # self.model = model\n",
        "            self.loss = torch.zeros_like(trainer.loss_items, device=trainer.device)\n",
        "            self.args.plots &= trainer.stopper.possible_stop or (trainer.epoch == trainer.epochs - 1)\n",
        "            model.eval()\n",
        "        else:\n",
        "            if str(self.args.model).endswith(\".yaml\") and model is None:\n",
        "                LOGGER.warning(\"WARNING ⚠️ validating an untrained model YAML will result in 0 mAP.\")\n",
        "            callbacks.add_integration_callbacks(self)\n",
        "            model = AutoBackend(\n",
        "                weights=model or self.args.model,\n",
        "                device=select_device(self.args.device, self.args.batch),\n",
        "                dnn=self.args.dnn,\n",
        "                data=self.args.data,\n",
        "                fp16=self.args.half,\n",
        "            )\n",
        "            # self.model = model\n",
        "            self.device = model.device  # update device\n",
        "            self.args.half = model.fp16  # update half\n",
        "            stride, pt, jit, engine = model.stride, model.pt, model.jit, model.engine\n",
        "            imgsz = check_imgsz(self.args.imgsz, stride=stride)\n",
        "            if engine:\n",
        "                self.args.batch = model.batch_size\n",
        "            elif not pt and not jit:\n",
        "                self.args.batch = model.metadata.get(\"batch\", 1)  # export.py models default to batch-size 1\n",
        "                LOGGER.info(f\"Setting batch={self.args.batch} input of shape ({self.args.batch}, 3, {imgsz}, {imgsz})\")\n",
        "\n",
        "            if str(self.args.data).split(\".\")[-1] in {\"yaml\", \"yml\"}:\n",
        "                self.data = {} # check_det_dataset(self.args.data)\n",
        "            elif self.args.task == \"classify\":\n",
        "                self.data = check_cls_dataset(self.args.data, split=self.args.split)\n",
        "            else:\n",
        "                raise FileNotFoundError(emojis(f\"Dataset '{self.args.data}' for task={self.args.task} not found ❌\"))\n",
        "\n",
        "            if self.device.type in {\"cpu\", \"mps\"}:\n",
        "                self.args.workers = 0  # faster CPU val as time dominated by inference, not dataloading\n",
        "            if not pt:\n",
        "                self.args.rect = False\n",
        "            self.stride = model.stride  # used in get_dataloader() for padding\n",
        "            self.dataloader = self.dataloader or self.get_dataloader(self.data.get(self.args.split), self.args.batch)\n",
        "\n",
        "            model.eval()\n",
        "            model.warmup(imgsz=(1 if pt else self.args.batch, 3, imgsz, imgsz))  # warmup\n",
        "\n",
        "        self.run_callbacks(\"on_val_start\")\n",
        "        dt = (\n",
        "            Profile(device=self.device),\n",
        "            Profile(device=self.device),\n",
        "            Profile(device=self.device),\n",
        "            Profile(device=self.device),\n",
        "        )\n",
        "        bar = TQDM(self.dataloader, desc=self.get_desc(), total=len(self.dataloader))\n",
        "        self.init_metrics(de_parallel(model))\n",
        "        self.jdict = []  # empty before each val\n",
        "        for batch_i, batch in enumerate(bar):\n",
        "            self.run_callbacks(\"on_val_batch_start\")\n",
        "            self.batch_i = batch_i\n",
        "            # Preprocess\n",
        "            with dt[0]:\n",
        "                batch = self.preprocess(batch)\n",
        "\n",
        "            # Inference\n",
        "            with dt[1]:\n",
        "                preds = model(batch[\"img\"], augment=augment)\n",
        "\n",
        "            # Loss\n",
        "            with dt[2]:\n",
        "                if self.training:\n",
        "                    self.loss += model.loss(batch, preds)[1]\n",
        "\n",
        "            # Postprocess\n",
        "            with dt[3]:\n",
        "                preds = self.postprocess(preds)\n",
        "\n",
        "            self.update_metrics(preds, batch)\n",
        "            if self.args.plots and batch_i < 3:\n",
        "                self.plot_val_samples(batch, batch_i)\n",
        "                self.plot_predictions(batch, preds, batch_i)\n",
        "\n",
        "            self.run_callbacks(\"on_val_batch_end\")\n",
        "        stats = self.get_stats()\n",
        "        self.check_stats(stats)\n",
        "        self.speed = dict(zip(self.speed.keys(), (x.t / len(self.dataloader.dataset) * 1e3 for x in dt)))\n",
        "        self.finalize_metrics()\n",
        "        self.print_results()\n",
        "        self.run_callbacks(\"on_val_end\")\n",
        "        if self.training:\n",
        "            model.float()\n",
        "            results = {**stats, **trainer.label_loss_items(self.loss.cpu() / len(self.dataloader), prefix=\"val\")}\n",
        "            return {k: round(float(v), 5) for k, v in results.items()}  # return results as 5 decimal place floats\n",
        "        else:\n",
        "            LOGGER.info(\n",
        "                \"Speed: {:.1f}ms preprocess, {:.1f}ms inference, {:.1f}ms loss, {:.1f}ms postprocess per image\".format(\n",
        "                    *tuple(self.speed.values())\n",
        "                )\n",
        "            )\n",
        "            if self.args.save_json and self.jdict:\n",
        "                with open(str(self.save_dir / \"predictions.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "                    LOGGER.info(f\"Saving {f.name}...\")\n",
        "                    json.dump(self.jdict, f)  # flatten and save\n",
        "                stats = self.eval_json(stats)  # update stats\n",
        "            if self.args.plots or self.args.save_json:\n",
        "                LOGGER.info(f\"Results saved to {colorstr('bold', self.save_dir)}\")\n",
        "            return stats\n",
        "\n",
        "    # --- Keep the _prepare_batch override if it was needed for the 0-D tensor error ---\n",
        "    # def _prepare_batch(self, si, batch):\n",
        "    #      ... (implementation that handles 0-D cls tensor) ..."
      ],
      "metadata": {
        "id": "p853PubBCUZa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAINER"
      ],
      "metadata": {
        "id": "-biaMFvu4IES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from ultralytics.utils.plotting import plot_images, plot_labels, plot_results\n",
        "# Import YOLODataset\n",
        "from ultralytics.data.dataset import YOLODataset\n",
        "\n",
        "class CustomDetectionTrainer(DetectionTrainer):\n",
        "    def __init__(self, trust_remote_code=False, debug=False, *args, **kwargs): # Removed hf_dataset_identifier from signature\n",
        "        print(\"CustomDetectionTrainer __init__ started...\")\n",
        "        self.custom_trust_code = trust_remote_code\n",
        "        self.hf_train_split = None; self.hf_val_split = None\n",
        "        self._debug = debug\n",
        "        # We rely on args.data being the HF identifier now\n",
        "        super().__init__(*args, **kwargs) # Calls overridden get_dataset\n",
        "        print(\"CustomDetectionTrainer __init__ finished.\")\n",
        "        # Verification AFTER get_dataset has run (called by super init)\n",
        "        # if not hasattr(self.args, 'nc') or not isinstance(self.args.nc, int) or self.args.nc <= 0:\n",
        "        #      raise RuntimeError(f\"Trainer self.args.nc not set correctly after get_dataset. Check get_dataset override.\")\n",
        "        # if not hasattr(self.args, 'names') or not isinstance(self.args.names, dict):\n",
        "        #      raise RuntimeError(f\"Trainer self.args.names not set correctly after get_dataset.\")\n",
        "        # print(f\"Trainer initialized: nc={self.args.nc}, names={self.args.names}\")\n",
        "\n",
        "    # --- Override get_dataset ---\n",
        "    def get_dataset(self):\n",
        "        \"\"\"\n",
        "        Overrides the base method. Loads HF dataset using args.data.\n",
        "        Instantiates HFYOLODataset (which discovers nc/names).\n",
        "        Sets args.nc and args.names based on the loaded dataset info.\n",
        "        Sets self.trainset and self.testset.\n",
        "        \"\"\"\n",
        "        print(\"****** Custom get_dataset called ******\")\n",
        "        if not hasattr(self, 'args'): raise RuntimeError(\"Trainer arguments missing.\")\n",
        "\n",
        "        hf_dataset_identifier = 'Max-Ploter/detection-moving-mnist-easy' #self.args.data # Get HF ID from data arg\n",
        "        trust_remote_code = self.custom_trust_code # Get from instance attribute\n",
        "\n",
        "        if not hf_dataset_identifier or not isinstance(hf_dataset_identifier, str):\n",
        "             raise ValueError(f\"HF dataset identifier '{hf_dataset_identifier}' (from args.data) is invalid.\")\n",
        "\n",
        "        # --- Load HF Splits ---\n",
        "        # ... (Load self.hf_train_split / self.hf_val_split using hf_dataset_identifier) ...\n",
        "        if self.hf_train_split is None:\n",
        "            try: self.hf_train_split = load_dataset(hf_dataset_identifier, split='train', trust_remote_code=trust_remote_code); print(f\"Loaded train split: {len(self.hf_train_split)} samples.\")\n",
        "            except Exception as e: raise RuntimeError(f\"Failed load HF train '{hf_dataset_identifier}': {e}\") from e\n",
        "        if self.hf_val_split is None:\n",
        "            try: # Try val then test\n",
        "                try: self.hf_val_split = load_dataset(hf_dataset_identifier, split='validation', trust_remote_code=trust_remote_code); print(\"Using 'validation' split.\")\n",
        "                except ValueError: self.hf_val_split = load_dataset(hf_dataset_identifier, split='test', trust_remote_code=trust_remote_code); print(\"Using 'test' split.\")\n",
        "                if not self.hf_val_split: raise ValueError(\"No val/test split found.\")\n",
        "                print(f\"Loaded val/test split: {len(self.hf_val_split)} samples.\")\n",
        "            except Exception as e: # Fallback split train\n",
        "                 print(f\"Warning: Failed loading val/test: {e}. Splitting train.\")\n",
        "                 # ... (Split train logic) ...\n",
        "                 if self.hf_train_split is None: raise RuntimeError(\"Train split not available.\")\n",
        "                 if len(self.hf_train_split) < 2: raise RuntimeError(\"Train split too small.\")\n",
        "                 splits = self.hf_train_split.train_test_split(test_size=0.2, seed=getattr(self.args, 'seed', 42))\n",
        "                 self.hf_train_split, self.hf_val_split = splits['train'], splits['test']\n",
        "                 print(f\"Used 80/20 split of 'train' for train ({len(self.hf_train_split)})/validation ({len(self.hf_val_split)}).\")\n",
        "\n",
        "        if self._debug:\n",
        "          # reduce train and val split sizes\n",
        "          print(\"Reducing train/val split sizes for debugging...\")\n",
        "          self.hf_train_split = self.hf_train_split.select(range(1))\n",
        "          self.hf_val_split = self.hf_val_split.select(range(1))\n",
        "\n",
        "        # --- Instantiate HFYOLODatasets (which find nc/names) ---\n",
        "        print(\"Instantiating HFYOLODataset for trainset...\")\n",
        "        self.trainset = HFYOLODataset(self.hf_train_split, imgsz=self.args.imgsz, trust_remote_code=trust_remote_code)\n",
        "        print(\"Instantiating HFYOLODataset for testset (validation)...\")\n",
        "        self.testset = HFYOLODataset(self.hf_val_split, imgsz=self.args.imgsz, trust_remote_code=trust_remote_code)\n",
        "\n",
        "        # --- Get nc/names FROM the instantiated dataset ---\n",
        "        known_nc = getattr(self.trainset, 'num_classes', 0)\n",
        "        known_names = getattr(self.trainset, 'names', {})\n",
        "        if known_nc <= 0: # Fallback to testset if trainset failed\n",
        "            known_nc = getattr(self.testset, 'num_classes', 0)\n",
        "            known_names = getattr(self.testset, 'names', {})\n",
        "\n",
        "        if known_nc <= 0:\n",
        "            raise ValueError(\"Could not determine number of classes from loaded HFYOLODataset instances.\")\n",
        "        # --------------------------------------------------\n",
        "\n",
        "        # --- Set nc and names on self.args ---\n",
        "        # This is the CRUCIAL step - fulfilling the presumed responsibility\n",
        "        print(f\"Setting trainer args: nc={known_nc}, names={known_names}\")\n",
        "        # self.args.nc = known_nc\n",
        "        # self.args.names = known_names\n",
        "\n",
        "        # --- <<< NEW: Explicitly set self.data attribute >>> ---\n",
        "        # This dictionary is expected by the original get_model method\n",
        "        self.data = {'nc': known_nc, 'names': known_names}\n",
        "        # Add other keys if get_model relies on them, e.g. 'path' (can be dummy)\n",
        "        # self.data['path'] = '.' # Example if path is needed\n",
        "        print(f\"Set self.data attribute: {self.data}\")\n",
        "        # ----------------------------------------------------\n",
        "\n",
        "        # Optional: Update dataset.data for consistency\n",
        "        if hasattr(self.trainset, 'data'): self.trainset.data = self.data\n",
        "        if hasattr(self.testset, 'data'): self.testset.data = self.data\n",
        "\n",
        "\n",
        "        # ------------------------------------\n",
        "\n",
        "        print(f\"****** Custom get_dataset finished. Set args.nc={known_nc}. ******\")\n",
        "        return self.trainset, self.testset\n",
        "\n",
        "\n",
        "    # --- get_dataloader and plot_training_labels overrides remain the same ---\n",
        "    def get_dataloader(self, dataset=None, batch_size=16, rank=0, mode='train'):\n",
        "        # ... (implementation is unchanged) ...\n",
        "        print(f\"****** Custom get_dataloader called for mode: {mode} ******\")\n",
        "        if dataset is None: dataset = self.trainset if mode == 'train' else self.testset\n",
        "        if not isinstance(dataset, HFYOLODataset): print(f\"Warning: Dataset type {type(dataset)}.\")\n",
        "        batch_size_arg = getattr(self.args, 'batch', batch_size); batch_size_to_use = batch_size_arg if isinstance(batch_size_arg, int) and batch_size_arg > 0 else batch_size\n",
        "        if mode != 'train': batch_size_to_use *= 2\n",
        "        workers = getattr(self.args, 'workers', 0); shuffle = (mode == 'train')\n",
        "        print(f\"Creating DataLoader for mode '{mode}' with batch_size={batch_size_to_use}, workers={workers}...\")\n",
        "        loader = DataLoader(dataset, batch_size=batch_size_to_use, shuffle=shuffle, num_workers=workers, pin_memory=True,\n",
        "                            collate_fn=YOLODataset.collate_fn\n",
        "                            )\n",
        "        print(f\"****** Custom DataLoader created for {mode} ******\")\n",
        "        return loader\n",
        "\n",
        "    def plot_training_labels(self): print(\"Skipping plot_training_labels in Custom Trainer.\"); pass\n",
        "\n",
        "    def get_validator(self):\n",
        "        \"\"\"Returns a CustomDetectionValidator instance.\"\"\"\n",
        "        print(\"****** Custom get_validator called (Returning CustomDetectionValidator) ******\")\n",
        "        # Ensure validation dataloader exists\n",
        "        if not hasattr(self, 'test_loader') or self.test_loader is None:\n",
        "             print(\"Creating validation dataloader within get_validator...\")\n",
        "             if not hasattr(self, 'testset') or self.testset is None: raise RuntimeError(\"Validation dataset missing.\")\n",
        "             val_batch_size = getattr(self.args, 'batch', 16) * 2\n",
        "             self.test_loader = self.get_dataloader(self.testset, batch_size=val_batch_size, mode='val')\n",
        "\n",
        "        validator_args = copy(self.args) # Pass copy of trainer args\n",
        "\n",
        "        # Instantiate OUR custom validator\n",
        "        validator = CustomDetectionValidator( # Use the custom class\n",
        "            dataloader=self.test_loader,\n",
        "            save_dir=self.save_dir,\n",
        "            args=validator_args,\n",
        "            # --- FIX: Pass the main self.callbacks dict directly ---\n",
        "            _callbacks=self.callbacks\n",
        "            # --- End Fix ---\n",
        "        )\n",
        "        # Link model and data dict\n",
        "        validator.model = self.model\n",
        "        validator.data = self.data\n",
        "        print(f\"****** Custom get_validator finished. Validator created. Using data: {validator.data} ******\")\n",
        "        return validator\n"
      ],
      "metadata": {
        "id": "BXIB2EjUHjlS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Configuration Cell in Jupyter Notebook ===\n",
        "from types import SimpleNamespace\n",
        "import os\n",
        "\n",
        "# --- Define YAML Content and Filename ---\n",
        "FAKE_YAML_FILENAME = \"fake.yaml\" # Name of the file to create\n",
        "YAML_SHIM_CONTENT = \"\"\"\n",
        "# Minimal YAML to satisfy Ultralytics checks during validation\n",
        "path: ./ignored_path # Ignored, but path key might be checked\n",
        "train: images/train   # Ignored\n",
        "val: images/val       # Ignored\n",
        "\n",
        "# --- Important Part ---\n",
        "nc: 10 # Your known number of classes\n",
        "names:\n",
        "  0: '0'\n",
        "  1: '1'\n",
        "  2: '2'\n",
        "  3: '3'\n",
        "  4: '4'\n",
        "  5: '5'\n",
        "  6: '6'\n",
        "  7: '7'\n",
        "  8: '8'\n",
        "  9: '9'\n",
        "# --- End Important Part ---\n",
        "\"\"\"\n",
        "# ---------------------------------------\n",
        "\n",
        "# --- Configuration Variables ---\n",
        "CFG_HF_DATASET_IDENTIFIER = \"Max-Ploter/detection-moving-mnist-easy\" # Your HF dataset path/name\n",
        "CFG_MODEL_NAME = 'yolov8n.pt'\n",
        "# CFG_NUM_CLASSES = 10  # No longer needed here\n",
        "CFG_EPOCHS = 100\n",
        "CFG_BATCH_SIZE = 64\n",
        "CFG_IMG_SIZE = 320\n",
        "CFG_WORKERS = 2\n",
        "CFG_TRUST_REMOTE_CODE = True # Custom flag for trainer\n",
        "CFG_PROJECT_NAME = \"yolo_hf_custom_trainer\"\n",
        "CFG_EXPERIMENT_NAME = \"train_yolo_nano_24k_dataset3\"\n",
        "CFG_PLOTS = True\n",
        "CFG_SAVE_PERIOD = 1 # <<<<<<< ADDED: Set save_period to 1\n",
        "\n",
        "\n",
        "# --- Construct the path to the last checkpoint ---\n",
        "# !!! This path MUST exist from your previous training run !!!\n",
        "checkpoint_dir = os.path.join(GDRIVE_BASE_PATH, CFG_EXPERIMENT_NAME, 'weights')\n",
        "resume_checkpoint_path = os.path.join(checkpoint_dir, 'last.pt')\n",
        "\n",
        "print(f\"Attempting to resume training from: {resume_checkpoint_path}\")\n",
        "\n",
        "# --- Check if the checkpoint exists ---\n",
        "if not os.path.exists(resume_checkpoint_path):\n",
        "    raise FileNotFoundError(f\"Checkpoint file not found at: {resume_checkpoint_path}. Cannot resume.\")\n",
        "else:\n",
        "    print(\"Checkpoint file found.\")\n",
        "    # --- Set the model path to the checkpoint for resuming ---\n",
        "    CFG_MODEL_TO_LOAD = resume_checkpoint_path\n",
        "\n",
        "\n",
        "# --- Prepare config_args with standard args ---\n",
        "# 'data' now holds the HF identifier\n",
        "# 'nc' is NOT set here\n",
        "config_args = SimpleNamespace(\n",
        "    model = CFG_MODEL_NAME,\n",
        "    data = FAKE_YAML_FILENAME, #CFG_HF_DATASET_IDENTIFIER, # Pass HF ID as data arg\n",
        "    epochs = CFG_EPOCHS,\n",
        "    batch = CFG_BATCH_SIZE,\n",
        "    imgsz = CFG_IMG_SIZE,\n",
        "    project = GDRIVE_BASE_PATH,\n",
        "    name = CFG_EXPERIMENT_NAME,\n",
        "    workers = CFG_WORKERS,\n",
        "    device = None,\n",
        "    plots = CFG_PLOTS,\n",
        "    resume=CFG_MODEL_TO_LOAD,\n",
        "    # trust_remote_code is handled separately\n",
        "    # nc is handled by get_dataset\n",
        ")\n",
        "\n",
        "with open(FAKE_YAML_FILENAME, 'w') as f:\n",
        "    f.write(YAML_SHIM_CONTENT)\n",
        "print(f\"Created fake YAML file: {FAKE_YAML_FILENAME}\")\n",
        "\n",
        "# --- Sanity Check ---\n",
        "if config_args.data == \"your_huggingface_dataset_identifier\": # Or similar check\n",
        "     print(f\"🛑 Error: Please set HF dataset identifier in CFG_HF_DATASET_IDENTIFIER.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLJi3dhOHm6X",
        "outputId": "eaec80f6-5ede-444d-90eb-9f756b9984bb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to resume training from: /content/drive/MyDrive/YOLOv8_Training/train_yolo_nano_24k_dataset3/weights/last.pt\n",
            "Checkpoint file found.\n",
            "Created fake YAML file: fake.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAIN"
      ],
      "metadata": {
        "id": "vK-ZOOk53_Jd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTa5xlaG8XnR",
        "outputId": "cbd4a23e-a4c3-4d01-eba8-1332368641cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Instantiating CustomDetectionTrainer manually...\n",
            "CustomDetectionTrainer __init__ started...\n",
            "Ultralytics 8.3.107 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/content/drive/MyDrive/YOLOv8_Training/train_yolo_nano_24k_dataset3/weights/last.pt, data=fake.yaml, epochs=100, time=None, patience=100, batch=64, imgsz=320, save=True, save_period=-1, cache=False, device=None, workers=2, project=/content/drive/MyDrive/YOLOv8_Training, name=train_yolo_nano_24k_dataset3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=/content/drive/MyDrive/YOLOv8_Training/train_yolo_nano_24k_dataset3/weights/last.pt, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/content/drive/MyDrive/YOLOv8_Training/train_yolo_nano_24k_dataset3\n",
            "****** Custom get_dataset called ******\n",
            "Loaded train split: 24000 samples.\n",
            "Using 'test' split.\n",
            "Loaded val/test split: 10000 samples.\n",
            "Instantiating HFYOLODataset for trainset...\n",
            "Initializing HFYOLODataset (Using Ultralytics Transforms)...\n",
            "Frames/Video: 20, Dim: (128, 128)\n",
            "Total train frames: 480000\n",
            "Discovered nc=10, names={0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
            "Augmentation enabled, but using simple LetterBox transform.\n",
            "Transforms created: Compose(<ultralytics.data.augment.LetterBox object at 0x7ae064002d50>, <ultralytics.data.augment.Format object at 0x7adbca2b9cd0>)\n",
            "Instantiating HFYOLODataset for testset (validation)...\n",
            "Initializing HFYOLODataset (Using Ultralytics Transforms)...\n",
            "Frames/Video: 20, Dim: (128, 128)\n",
            "Total train frames: 200000\n",
            "Discovered nc=10, names={0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
            "Augmentation enabled, but using simple LetterBox transform.\n",
            "Transforms created: Compose(<ultralytics.data.augment.LetterBox object at 0x7adbca364090>, <ultralytics.data.augment.Format object at 0x7adf1ec52650>)\n",
            "Setting trainer args: nc=10, names={0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
            "Set self.data attribute: {'nc': 10, 'names': {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}}\n",
            "****** Custom get_dataset finished. Set args.nc=10. ******\n",
            "CustomDetectionTrainer __init__ finished.\n",
            "\n",
            "🚀 Starting trainer.train()...\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /content/drive/MyDrive/YOLOv8_Training/train_yolo_nano_24k_dataset3', view at http://localhost:6006/\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    753262  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \n",
            "Model summary: 129 layers, 3,012,798 parameters, 3,012,782 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "****** Custom get_dataloader called for mode: train ******\n",
            "Creating DataLoader for mode 'train' with batch_size=64, workers=2...\n",
            "****** Custom DataLoader created for train ******\n",
            "****** Custom get_dataloader called for mode: val ******\n",
            "Creating DataLoader for mode 'val' with batch_size=128, workers=2...\n",
            "****** Custom DataLoader created for val ******\n",
            "****** Custom get_validator called (Returning CustomDetectionValidator) ******\n",
            "****** Custom get_validator finished. Validator created. Using data: {'nc': 10, 'names': {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}} ******\n",
            "Skipping plot_training_labels in Custom Trainer.\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Resuming training /content/drive/MyDrive/YOLOv8_Training/train_yolo_nano_24k_dataset3/weights/last.pt from epoch 2 to 100 total epochs\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 320 train, 320 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/YOLOv8_Training/train_yolo_nano_24k_dataset3\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       Loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      2/100      3.76G     0.2501     0.3321     0.7938        243        320:  61%|██████▏   | 4606/7500 [51:57<31:37,  1.52it/s]"
          ]
        }
      ],
      "source": [
        "# === Execution Cell in Jupyter Notebook ===\n",
        "# Ensure CustomDetectionTrainer etc are defined\n",
        "\n",
        "if config_args.data != \"your_huggingface_dataset_identifier\": # Or similar check\n",
        "    print(\"🚀 Instantiating CustomDetectionTrainer manually...\")\n",
        "    try:\n",
        "        trainer = CustomDetectionTrainer(\n",
        "            overrides=vars(config_args), # Contains standard args + HF ID in 'data'\n",
        "            # Pass ONLY custom args directly\n",
        "            trust_remote_code=CFG_TRUST_REMOTE_CODE,\n",
        "            debug=False\n",
        "        )\n",
        "\n",
        "        print(\"\\n🚀 Starting trainer.train()...\")\n",
        "        trainer.train()\n",
        "        print(\"\\n✅ Training finished successfully!\")\n",
        "        # print(f\"   Best model weights: {trainer.best}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Training failed with an error:\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"⚠️ Training not started. Please set HF dataset identifier.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EVAL"
      ],
      "metadata": {
        "id": "FMAQgMs4ItDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from types import SimpleNamespace\n",
        "from ultralytics import YOLO  # Import the YOLO class\n",
        "from ultralytics.data.dataset import YOLODataset # For collate_fn\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset # Assuming this is used in HFYOLODataset\n",
        "\n",
        "# --- Configuration (Should match your training setup) ---\n",
        "# Ensure these variables are set from your previous training config cells\n",
        "# GDRIVE_BASE_PATH = '/content/drive/MyDrive/YOLOv8_Training' # Example\n",
        "# CFG_EXPERIMENT_NAME = \"train_yolo_nano_24k_dataset\" # Example run name\n",
        "# CFG_HF_DATASET_IDENTIFIER = \"Max-Ploter/detection-moving-mnist-easy\" # Example HF dataset\n",
        "# CFG_IMG_SIZE = 320\n",
        "# CFG_BATCH_SIZE = 64 # Training batch size\n",
        "# CFG_WORKERS = 2\n",
        "# CFG_TRUST_REMOTE_CODE = True\n",
        "# device = None # Or explicitly 'cuda:0', 'cpu' etc. based on training\n",
        "\n",
        "# --- 1. Define the Checkpoint to Evaluate ---\n",
        "# Usually, you evaluate 'best.pt', but after 1 epoch, 'last.pt' is more likely\n",
        "checkpoint_dir = os.path.join(GDRIVE_BASE_PATH, CFG_EXPERIMENT_NAME, 'weights')\n",
        "checkpoint_to_eval = os.path.join(checkpoint_dir, 'best.pt') # Standard choice\n",
        "# checkpoint_to_eval = os.path.join(checkpoint_dir, 'last.pt')   # Choice after 1 epoch\n",
        "\n",
        "# Check if the chosen checkpoint exists\n",
        "best_checkpoint_path = os.path.join(checkpoint_dir, 'best.pt')\n",
        "if os.path.exists(best_checkpoint_path):\n",
        "    checkpoint_to_eval = best_checkpoint_path\n",
        "    print(f\"Found 'best.pt', evaluating it: {checkpoint_to_eval}\")\n",
        "elif os.path.exists(os.path.join(checkpoint_dir, 'last.pt')):\n",
        "    checkpoint_to_eval = os.path.join(checkpoint_dir, 'last.pt')\n",
        "    print(f\"Found 'last.pt', evaluating it: {checkpoint_to_eval}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Neither 'best.pt' nor 'last.pt' found in {checkpoint_dir}\")\n",
        "\n",
        "if not os.path.exists(checkpoint_to_eval):\n",
        "     raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_to_eval}\")\n",
        "print(f\"Selected checkpoint for evaluation: {checkpoint_to_eval}\")\n",
        "\n",
        "# --- 2. Load the Trained Model ---\n",
        "print(\"Loading trained model checkpoint...\")\n",
        "# This loads the model weights into the YOLO architecture\n",
        "model = YOLO(checkpoint_to_eval)\n",
        "print(\"Model loaded.\")\n",
        "\n",
        "# --- 3. Load the Test Dataset ---\n",
        "print(\"Loading 'test' split from Hugging Face...\")\n",
        "try:\n",
        "    hf_test_split = load_dataset(CFG_HF_DATASET_IDENTIFIER, split='test', trust_remote_code=CFG_TRUST_REMOTE_CODE)\n",
        "    print(f\"Loaded test split: {len(hf_test_split)} samples.\")\n",
        "    # Create the HFYOLODataset instance for the test split\n",
        "    # Ensure HFYOLODataset class is defined/imported\n",
        "    test_dataset = HFYOLODataset(hf_test_split, imgsz=CFG_IMG_SIZE, trust_remote_code=CFG_TRUST_REMOTE_CODE)\n",
        "    print(\"Test dataset created.\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Failed to load or process test split: {e}\") from e\n",
        "\n",
        "# --- 4. Create the Test DataLoader ---\n",
        "print(\"Creating test dataloader...\")\n",
        "# Use a larger batch size for validation/testing is common\n",
        "test_batch_size = CFG_BATCH_SIZE * 2\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=test_batch_size,\n",
        "    shuffle=False, # No need to shuffle for evaluation\n",
        "    num_workers=CFG_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=YOLODataset.collate_fn # Use the standard collate_fn\n",
        ")\n",
        "print(\"Test dataloader created.\")\n",
        "\n",
        "# --- 5. Prepare Validator Arguments and Instantiate Validator ---\n",
        "print(\"Preparing validator...\")\n",
        "# Define a directory to save test evaluation results/plots\n",
        "test_eval_save_dir = os.path.join(GDRIVE_BASE_PATH, CFG_EXPERIMENT_NAME, f'test_eval_{os.path.basename(checkpoint_to_eval).split(\".\")[0]}')\n",
        "os.makedirs(test_eval_save_dir, exist_ok=True)\n",
        "print(f\"Test evaluation results will be saved in: {test_eval_save_dir}\")\n",
        "\n",
        "# Get nc and names from the test dataset (should match training)\n",
        "test_nc = getattr(test_dataset, 'num_classes', 0)\n",
        "test_names = getattr(test_dataset, 'names', {})\n",
        "if test_nc <= 0:\n",
        "    raise ValueError(\"Could not determine number of classes from test dataset.\")\n",
        "test_data_dict = {'nc': test_nc, 'names': test_names}\n",
        "print(f\"Test data info: nc={test_nc}, names={test_names}\")\n",
        "\n",
        "# Prepare minimal arguments needed by the Validator\n",
        "# Check your CustomDetectionValidator's __init__ and methods if it needs more args\n",
        "validator_args = SimpleNamespace(\n",
        "    data = FAKE_YAML_FILENAME,\n",
        "    save_dir=test_eval_save_dir,\n",
        "    device=None, # Use the device determined during training setup\n",
        "    batch=test_batch_size,\n",
        "    imgsz=CFG_IMG_SIZE,\n",
        "    split='test',    # Specify the split being evaluated\n",
        "    task='detect',   # Specify the task\n",
        "    plots=True,      # Enable saving plots (e.g., confusion matrix, PR curves)\n",
        "    save_json=False, # Set True if you need COCO format JSON output\n",
        "    # save_hybrid=False,\n",
        "    conf=0.001,      # Confidence threshold (adjust if needed)\n",
        "    iou=0.6,         # IoU threshold for NMS (adjust if needed)\n",
        "    max_det=10,\n",
        "    # data=FAKE_YAML_FILENAME, # Maybe needed if validator parses it, but we set validator.data directly\n",
        "    project=GDRIVE_BASE_PATH, # Not directly used by validator usually, but good practice\n",
        "    name=os.path.basename(test_eval_save_dir), # Logical name for this eval run\n",
        ")\n",
        "\n",
        "# Instantiate your CustomDetectionValidator\n",
        "# Ensure the CustomDetectionValidator class is defined/imported\n",
        "validator = CustomDetectionValidator(\n",
        "    dataloader=test_loader,\n",
        "    save_dir=Path(test_eval_save_dir),\n",
        "    args=validator_args,\n",
        "    _callbacks={} # Pass empty callbacks if not needed for pure evaluation\n",
        ")\n",
        "\n",
        "# Link the loaded model (usually the internal torch model) and data info\n",
        "# The YOLO object holds the model in '.model'\n",
        "# validator.model = model\n",
        "validator.data = test_data_dict # Provide nc/names directly\n",
        "\n",
        "print(\"Validator instance created and configured.\")\n",
        "\n",
        "# --- 6. Run Evaluation ---\n",
        "print(\"Starting evaluation on the test set...\")\n",
        "try:\n",
        "    # The validator instance is callable and runs the evaluation loop\n",
        "    results = validator(model = checkpoint_to_eval)\n",
        "    print(\"Evaluation finished.\")\n",
        "\n",
        "    # --- 7. Print Results ---\n",
        "    # The 'results' object (often a dictionary) contains the metrics\n",
        "    # Refer to ultralytics documentation or inspect the 'results' keys for specifics\n",
        "    print(\"\\n--- Test Set Evaluation Metrics ---\")\n",
        "    # Common metrics for detection:\n",
        "    map50_95 = results.maps[0] # mAP50-95 for class 0 (or overall if only 1 class reported directly)\n",
        "    map50 = results.maps[50] # mAP50 for class 0 (or overall)\n",
        "    print(f\"mAP50-95: {map50_95:.4f}\")\n",
        "    print(f\"mAP50:    {map50:.4f}\")\n",
        "    # Print all metrics found\n",
        "    print(\"\\nFull metrics dictionary:\")\n",
        "    print(results.metrics_data) # Or just print(results) depending on object type\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Evaluation failed with an error:\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-8DmR7SJIn6",
        "outputId": "b578ed15-0d4d-4785-9c29-27e260dea796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 'best.pt', evaluating it: /content/drive/MyDrive/YOLOv8_Training/train_yolo_nano_24k_dataset3/weights/best.pt\n",
            "Selected checkpoint for evaluation: /content/drive/MyDrive/YOLOv8_Training/train_yolo_nano_24k_dataset3/weights/best.pt\n",
            "Loading trained model checkpoint...\n",
            "Model loaded.\n",
            "Loading 'test' split from Hugging Face...\n",
            "Loaded test split: 10000 samples.\n",
            "Initializing HFYOLODataset (Using Ultralytics Transforms)...\n",
            "Frames/Video: 20, Dim: (128, 128)\n",
            "Total train frames: 200000\n",
            "Discovered nc=10, names={0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
            "Augmentation enabled, but using simple LetterBox transform.\n",
            "Transforms created: Compose(<ultralytics.data.augment.LetterBox object at 0x7b2078234a50>, <ultralytics.data.augment.Format object at 0x7b2078e36e10>)\n",
            "Test dataset created.\n",
            "Creating test dataloader...\n",
            "Test dataloader created.\n",
            "Preparing validator...\n",
            "Test evaluation results will be saved in: /content/drive/MyDrive/YOLOv8_Training/train_yolo_nano_24k_dataset3/test_eval_best\n",
            "Test data info: nc=10, names={0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
            "Validator instance created and configured.\n",
            "Starting evaluation on the test set...\n",
            "Ultralytics 8.3.107 🚀 Python-3.11.12 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n",
            "Model summary (fused): 72 layers, 3,007,598 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 1/1563 [00:20<8:47:14, 20.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 755k/755k [00:00<00:00, 12.5MB/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   4%|▎         | 57/1563 [11:59<5:10:12, 12.36s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FGd85qc-Dj-D"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}